{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "\n",
    "CONFIG = u.load_config('../config/config.yaml')\n",
    "METRICS_CONFIG = u.load_config('../config/metrics_config.yaml')\n",
    "MODELING_CONFIG = u.load_config('../config/modeling_config.yaml')\n",
    "EXPERIMENTS_CONFIG = u.load_config('../config/experiments_config.yaml')\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']\n",
    "modeling_folder = MODELING_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve profits data\n",
    "profits_df = td.retrieve_profits_data(start_date, end_date)\n",
    "profits_df, _ = cwm.split_dataframe_by_coverage(profits_df, start_date, end_date, id_column='coin_id')\n",
    "profits_df, _ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "\n",
    "# remove records from market_data_df that don't have transfers if configured to do so\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_full = profits_df.copy(deep=True)\n",
    "market_data_df_full = market_data_df.copy(deep=True)\n",
    "prices_df_full = prices_df.copy(deep=True)\n",
    "\n",
    "# profits_df = profits_df_full.copy(deep=True)\n",
    "# market_data_df = market_data_df_full.copy(deep=True)\n",
    "# prices_df = prices_df_full.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# figure out why the prices joins remove rows, shouldn't prices be filled all the way?\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "profits_df = profits_df_full.copy(deep=True)\n",
    "# market_data_df = market_data_df_full.copy(deep=True)\n",
    "prices_df = prices_df_full.copy(deep=True)\n",
    "\n",
    "# variables\n",
    "# prices_df = prices_df_full.copy(deep=True)\n",
    "# profits_df = partitions[7].copy(deep=True)\n",
    "target_date = '2024-08-31'\n",
    "\n",
    "\n",
    "def impute_profits_df_rows(profits_df, prices_df, target_date):\n",
    "    \"\"\"\n",
    "    Impute rows for all coin-wallet pairs in profits_df on the target date using only\n",
    "    vectorized functions, i.e. there are no groupby statements or for loops/lambda\n",
    "    functions that iterate over each row. This is necessary due to the size and memory\n",
    "    requirements of the input df.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Splits profits_df into records before and after the target date\n",
    "    2. Filters for pairs needing new rows\n",
    "    3. Identifies the last date for each coin-wallet pair\n",
    "    4. Appends price columns for the last date and target date\n",
    "    5. Calculates new values for pairs needing rows\n",
    "    6. Concatenates the new rows with the original dataframe\n",
    "\n",
    "    Args:\n",
    "        profits_df (pd.DataFrame): DataFrame containing profit information\n",
    "        prices_df (pd.DataFrame): DataFrame containing price information\n",
    "        target_date (str or datetime): The date for which to impute rows\n",
    "\n",
    "    Returns:\n",
    "        profits_df_filled (pd.DataFrame): Updated profits DataFrame with imputed rows\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If joining prices_df removes rows from profits_df\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info('%s Imputing rows for all coin-wallet pairs in profits_df on %s...',\n",
    "                profits_df.shape,\n",
    "                target_date)\n",
    "\n",
    "\n",
    "    # Convert date to datetime\n",
    "    target_date = pd.to_datetime(target_date)\n",
    "\n",
    "    # Store shape for logging purposes\n",
    "    start_shape = profits_df.shape\n",
    "\n",
    "    # Create indices so we can use vectorized operations\n",
    "    profits_df = profits_df.set_index(['coin_id', 'wallet_address', 'date'])\n",
    "    prices_df = prices_df.set_index(['coin_id', 'date'])\n",
    "\n",
    "    # Step 1: Split profits_df records before and after the target_date\n",
    "    # -----------------------------------------------------------------\n",
    "    profits_df_after_target = profits_df.xs(slice(target_date + pd.Timedelta('1 day'), None), level=2, drop_level=False)\n",
    "    profits_df = profits_df.xs(slice(None, target_date), level=2, drop_level=False)\n",
    "\n",
    "    logger.debug(\"%s <Step 1> Split profits_df into %s rows through the target_date and %s after target_date: %.2f seconds\",\n",
    "                    profits_df.shape,\n",
    "                    len(profits_df),\n",
    "                    len(profits_df_after_target),\n",
    "                    time.time() - start_time)\n",
    "    step_time = time.time()\n",
    "\n",
    "\n",
    "    # Step 2: Filter profits_df to only pairs that need new rows\n",
    "    # ----------------------------------------------------------\n",
    "    # Create a boolean mask for rows at the target_date\n",
    "    target_date_mask = profits_df.index.get_level_values('date') == target_date\n",
    "\n",
    "    # Create a boolean mask for pairs that don't have a row at the target_date\n",
    "    pairs_mask = ~profits_df.index.droplevel('date').isin(\n",
    "        profits_df[target_date_mask].index.droplevel('date')\n",
    "    )\n",
    "    profits_df = profits_df[pairs_mask].sort_index()\n",
    "\n",
    "    logger.debug(\"%s <Step 2> Identified %s coin-wallet pairs that need imputed rows: %.2f seconds\",\n",
    "                    profits_df.shape,\n",
    "                    len(profits_df),\n",
    "                    time.time() - step_time)\n",
    "    step_time = time.time()\n",
    "\n",
    "\n",
    "    # Step 3: Identify the last date for each coin-wallet pair\n",
    "    # ----------------------------------------------\n",
    "    # The logic here is that every row that doesn't have the same coin_id-wallet_address\n",
    "    # combination as the previous row must indicate that the previous coin-wallet pair\n",
    "    # just had its last date.\n",
    "\n",
    "    # Create shifted index\n",
    "    shifted_index = profits_df.index.to_frame().shift(-1)\n",
    "\n",
    "    # Create boolean mask for last dates\n",
    "    is_last_date = (profits_df.index.get_level_values('coin_id') != shifted_index['coin_id']) | \\\n",
    "                (profits_df.index.get_level_values('wallet_address') != shifted_index['wallet_address'])\n",
    "\n",
    "    # Filter for last dates\n",
    "    profits_df = profits_df[is_last_date]\n",
    "\n",
    "    logger.debug(\"%s <Step 3> Filtered profits_df to only the last dates for each coin-wallet pair: %.2f seconds\",\n",
    "                    profits_df.shape,\n",
    "                    time.time() - step_time)\n",
    "    step_time = time.time()\n",
    "\n",
    "\n",
    "    # Step 4: Append columns for previous_price (as of the last date) and price (as of the target_date)\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "    # Add price_previous by joining the price as of the last date for each coin-wallet pair\n",
    "    prejoin_size = len(profits_df)\n",
    "    profits_df = profits_df.join(prices_df['price'], on=['coin_id', 'date'], how='inner')\n",
    "    profits_df = profits_df.rename(columns={'price': 'price_previous'})\n",
    "\n",
    "    # Add price by joining the price as of the target_date\n",
    "    prices_target_date = prices_df.xs(target_date, level='date')\n",
    "    profits_df = profits_df.join(prices_target_date['price'], on='coin_id', how='inner')\n",
    "\n",
    "    if len(profits_df) != prejoin_size:\n",
    "        raise ValueError(\"Inner join to prices_df on coin_id-date removed %s rows from profits_df with \"\n",
    "                        \"original length %s. There should be complete coverage for all rows in profits_df.\",\n",
    "                        prejoin_size-len(profits_df),\n",
    "                        len(profits_df))\n",
    "\n",
    "    logger.debug(\"%s <Step 4> Joined prices_df and added price and previous_price helper columns: %.2f seconds\",\n",
    "                    profits_df.shape,\n",
    "                    time.time() - step_time)\n",
    "    step_time = time.time()\n",
    "\n",
    "\n",
    "    # Step 5: Calculate new values for pairs needing rows\n",
    "    # ---------------------------------------------------\n",
    "    new_rows_df = pd.DataFrame(index=profits_df.index)\n",
    "    new_rows_df['date'] = target_date\n",
    "    new_rows_df['profits_change'] = (profits_df['price'] / profits_df['price_previous'] - 1) * profits_df['usd_balance']\n",
    "    new_rows_df['profits_cumulative'] = new_rows_df['profits_change'] + profits_df['profits_cumulative']\n",
    "    new_rows_df['usd_balance'] = (profits_df['price'] / profits_df['price_previous']) * profits_df['usd_balance']\n",
    "    new_rows_df['usd_net_transfers'] = 0\n",
    "    new_rows_df['usd_inflows'] = 0\n",
    "    new_rows_df['usd_inflows_cumulative'] = profits_df['usd_inflows_cumulative']\n",
    "    new_rows_df['total_return'] = new_rows_df['profits_cumulative'] / new_rows_df['usd_inflows_cumulative']\n",
    "    new_rows_df['price_previous'] = profits_df['price_previous']\n",
    "    new_rows_df['price'] = profits_df['price']\n",
    "\n",
    "    logger.debug(\"%s <Step 5> Calculated %s new rows: %.2f seconds\",\n",
    "                    profits_df.shape,\n",
    "                    len(new_rows_df),\n",
    "                    time.time() - step_time)\n",
    "    step_time = time.time()\n",
    "\n",
    "\n",
    "    # Step 6: Reset MultiIndex and concatenate dfs\n",
    "    # --------------------------------------------\n",
    "    new_rows_df = new_rows_df.reset_index(level='date', drop=True)\n",
    "    new_rows_df = new_rows_df.reset_index().set_index(['coin_id', 'wallet_address', 'date'])\n",
    "\n",
    "    profits_df_filled = pd.concat([profits_df, new_rows_df])\n",
    "\n",
    "    logger.debug(\"%s <Step 6> Reset indices and added new rows to profits_df: %.2f seconds\",\n",
    "                    profits_df.shape,\n",
    "                    time.time() - step_time)\n",
    "    logger.info(\"%s Successfully merged profits_df %s with new_rows_df %s to get profits_df_filled %s after %.2f total seconds.\",\n",
    "                profits_df_filled.shape,\n",
    "                start_shape,\n",
    "                new_rows_df.shape,\n",
    "                profits_df_filled.shape,\n",
    "                time.time() - start_time)\n",
    "\n",
    "    return profits_df_filled\n",
    "\n",
    "\n",
    "df = impute_profits_df_rows(profits_df,prices_df,target_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_3 = profits_df.copy(deep=True)\n",
    "profits_df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4: Append columns for previous_price (as of the last date) and price (as of the target_date)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Add price_previous by joining the price as of the last date for each coin-wallet pair\n",
    "profits_df_4 = profits_df.join(prices_df['price'], on=['coin_id', 'date'], how='inner')\n",
    "profits_df_4 = profits_df_4.rename(columns={'price': 'price_previous'})\n",
    "\n",
    "# # Add price by joining the price as of the target_date\n",
    "# prices_target_date = prices_df.xs(target_date, level='date')\n",
    "# profits_df = profits_df.join(prices_target_date['price'], on='coin_id', how='inner')\n",
    "\n",
    "# logger.debug(\"%s <Step 4> Joined prices_df and added price and previous_price helper columns: %.2f seconds\",\n",
    "#                 profits_df.shape,\n",
    "#                 time.time() - step_time)\n",
    "# step_time = time.time()\n",
    "\n",
    "profits_df_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming profits_df_3 and profits_df_4 are your DataFrames with MultiIndex\n",
    "\n",
    "# Step 1: Perform a left join of profits_df_3 with profits_df_4\n",
    "joined_df = profits_df_3.join(profits_df_4, how='left', lsuffix='_3', rsuffix='_4')\n",
    "\n",
    "# Step 2: Create a boolean mask for rows in profits_df_3 but not in profits_df_4\n",
    "# We'll use a column that should always be present in profits_df_4, let's say 'usd_balance'\n",
    "missing_mask = joined_df['usd_balance_4'].isna()\n",
    "\n",
    "# Step 3: Isolate the missing records\n",
    "missing_records = joined_df[missing_mask]\n",
    "\n",
    "# Step 4: Clean up the result to keep only profits_df_3 columns\n",
    "columns_to_keep = [col for col in missing_records.columns if not col.endswith('_4')]\n",
    "missing_records = missing_records[columns_to_keep]\n",
    "\n",
    "# Step 5: Remove the '_3' suffix from column names\n",
    "missing_records.columns = [col[:-2] if col.endswith('_3') else col for col in missing_records.columns]\n",
    "\n",
    "print(\"Records in profits_df_3 but not in profits_df_4:\")\n",
    "print(missing_records.head())\n",
    "print(f\"Total missing records: {len(missing_records)}\")\n",
    "\n",
    "# Optional: If you want to reset the index to match the original structure\n",
    "# missing_records = missing_records.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_partitions(profits_df, n_partitions):\n",
    "#     \"\"\"\n",
    "#     Partition a DataFrame into multiple subsets based on unique coin_ids.\n",
    "\n",
    "#     Parameters:\n",
    "#     - profits_df (pd.DataFrame): The input DataFrame to be partitioned. Must contain\n",
    "#         a 'coin_id' column.\n",
    "#     - n_partitions (int): The number of partitions to create.\n",
    "\n",
    "#     Returns:\n",
    "#     - partition_dfs (List[pd.DataFrame]): A list of DataFrames, each representing\n",
    "#         a partition of the original data.\n",
    "#     \"\"\"\n",
    "#     # Get unique coin_ids and convert to a regular list\n",
    "#     unique_coin_ids = profits_df['coin_id'].unique().tolist()\n",
    "\n",
    "#     # Shuffle the list of coin_ids\n",
    "#     np.random.shuffle(unique_coin_ids)\n",
    "\n",
    "#     # Calculate the number of coin_ids per partition\n",
    "#     coins_per_partition = len(unique_coin_ids) // n_partitions\n",
    "\n",
    "#     # Create partitions\n",
    "#     partition_dfs = []\n",
    "#     for i in range(n_partitions):\n",
    "#         start_idx = i * coins_per_partition\n",
    "#         end_idx = start_idx + coins_per_partition if i < n_partitions - 1 else None\n",
    "#         partition_coin_ids = unique_coin_ids[start_idx:end_idx]\n",
    "\n",
    "#         # Create a boolean mask for the current partition\n",
    "#         mask = profits_df['coin_id'].isin(partition_coin_ids)\n",
    "\n",
    "#         # Add the partition to the list\n",
    "#         partition_dfs.append(profits_df[mask])\n",
    "\n",
    "#     return partition_dfs\n",
    "\n",
    "\n",
    "# n_partitions = 8\n",
    "# partitions = create_partitions(profits_df, n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_prices_df = missing_records.reset_index()[['coin_id','date']].drop_duplicates()\n",
    "missing_prices_df['coin_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_id = '02785a31-24b2-403c-82d7-d3cb8783e1e6'\n",
    "# prices_target_date = prices_df.xs(target_date, level='date')\n",
    "\n",
    "prices_df.xs(coin_id, level='coin_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient columns\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "coin_id_mapping = dict(enumerate(profits_df['coin_id'].cat.categories))\n",
    "profits_df['coin_id'] = profits_df['coin_id'].cat.codes.astype('int16')\n",
    "\n",
    "# Convert date column to store the difference in days relative to target_date\n",
    "profits_df['date'] = (profits_df['date'] - target_date).dt.days.astype('int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vars\n",
    "# target_date = '2024-08-31'\n",
    "# # new_rows_df = generate_new_row(profits_df, prices_df, target_date)\n",
    "\n",
    "# target_date = pd.to_datetime(target_date)\n",
    "\n",
    "# # # Create efficient indexes\n",
    "# # profits_df = profits_df.set_index(['coin_id', 'wallet_address', 'date']).copy(deep=True)\n",
    "# # prices_df = prices_df.set_index(['coin_id', 'date']).copy(deep=True)\n",
    "\n",
    "# # # Identify pairs needing new rows\n",
    "# # logger.debug('Identifying pairs that need a row for %s...', target_date)\n",
    "# # all_pairs = profits_df.index.droplevel('date').unique()\n",
    "# # existing_pairs = profits_df.loc(axis=0)[:, :, target_date].index.droplevel('date')\n",
    "# # pairs_needing_rows = all_pairs.difference(existing_pairs)\n",
    "# # logger.debug('Identified %s pairs that will need rows imputed.', len(pairs_needing_rows))\n",
    "\n",
    "\n",
    "# new_rows = []\n",
    "\n",
    "# logger.debug('Imputing new rows...')\n",
    "# for coin_id, wallet_address in pairs_needing_rows:\n",
    "#     # Get most recent record\n",
    "#     recent_record = profits_df.loc[coin_id, wallet_address].loc[:target_date].iloc[-1]\n",
    "\n",
    "#     # Get prices\n",
    "#     price_previous = prices_df.loc[(coin_id, recent_record.name), 'price']\n",
    "#     price_current = prices_df.loc[(coin_id, target_date), 'price']\n",
    "\n",
    "#     # Calculate new values\n",
    "#     price_ratio = price_current / price_previous\n",
    "#     new_usd_balance = recent_record['usd_balance'] * price_ratio\n",
    "#     profits_change = new_usd_balance - recent_record['usd_balance']\n",
    "#     profits_cumulative = recent_record['profits_cumulative'] + profits_change\n",
    "\n",
    "#     new_row = {\n",
    "#         'coin_id': coin_id,\n",
    "#         'wallet_address': wallet_address,\n",
    "#         'date': target_date,\n",
    "#         'profits_change': profits_change,\n",
    "#         'profits_cumulative': profits_cumulative,\n",
    "#         'usd_balance': new_usd_balance,\n",
    "#         'usd_net_transfers': 0,\n",
    "#         'usd_inflows': 0,\n",
    "#         'usd_inflows_cumulative': recent_record['usd_inflows_cumulative'],\n",
    "#         'total_return': profits_cumulative / max(recent_record['usd_inflows_cumulative'], 0.01)\n",
    "#     }\n",
    "\n",
    "#     new_rows.append(new_row)\n",
    "\n",
    "# new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# logger.debug('Generated new_rows_df with shape %s.', new_rows_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent data for pairs needing rows\n",
    "most_recent_data = profits_df.loc[pairs_needing_rows]\n",
    "most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "\n",
    "# Reset index of prices_df for the merge operation\n",
    "prices_df_reset = prices_df.reset_index()\n",
    "\n",
    "# Perform asof merge to get the most recent price before or on the date of each record\n",
    "merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "                            prices_df_reset.sort_values('date'),\n",
    "                            on='date',\n",
    "                            by='coin_id',\n",
    "                            direction='backward')\n",
    "\n",
    "# Now get the price at the target date\n",
    "target_prices = prices_df.loc(axis=0)[:, target_date].reset_index()\n",
    "target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# Merge the target prices\n",
    "merged_data = pd.merge(merged_data, target_prices[['coin_id', 'target_price']], on='coin_id', how='left')\n",
    "\n",
    "# Calculate price ratio\n",
    "merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n",
    "\n",
    "logger.debug('Merged data shape: %s', merged_data.shape)\n",
    "logger.debug('Merged data columns: %s', merged_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent row for each pair needing a new row\n",
    "most_recent_data = profits_df.loc[profits_df.index.isin(pairs_needing_rows, level=['coin_id', 'wallet_address'])]\n",
    "# most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# # Ensure the date column is properly formatted\n",
    "# most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "# prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "\n",
    "# # Perform asof merge to get the most recent price before or on the date of each record\n",
    "# merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "#                             prices_df[['date', 'coin_id', 'price']].sort_values('date'),\n",
    "#                             on='date',\n",
    "#                             by='coin_id',\n",
    "#                             direction='backward')\n",
    "\n",
    "# # Now get the price at the target date\n",
    "# target_prices = prices_df[prices_df['date'] == target_date][['coin_id', 'price']]\n",
    "# target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# # Merge the target prices\n",
    "# merged_data = pd.merge(merged_data, target_prices, on='coin_id', how='left')\n",
    "\n",
    "# # Calculate price ratio\n",
    "# merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
