{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "\n",
    "CONFIG = u.load_config('../config/config.yaml')\n",
    "METRICS_CONFIG = u.load_config('../config/metrics_config.yaml')\n",
    "MODELING_CONFIG = u.load_config('../config/modeling_config.yaml')\n",
    "EXPERIMENTS_CONFIG = u.load_config('../config/experiments_config.yaml')\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']\n",
    "modeling_folder = MODELING_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve profits data\n",
    "profits_df = td.retrieve_profits_data(start_date, end_date)\n",
    "profits_df, _ = cwm.split_dataframe_by_coverage(profits_df, start_date, end_date, id_column='coin_id')\n",
    "profits_df, _ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "# # impute period boundary dates\n",
    "# dates_to_impute = [\n",
    "#     config['training_data']['training_period_end'],\n",
    "#     config['training_data']['modeling_period_start'],\n",
    "#     config['training_data']['modeling_period_end'],\n",
    "# ]\n",
    "# profits_df_merged = td.impute_profits_for_multiple_dates(profits_df, prices_df, dates_to_impute, n_threads=24)\n",
    "\n",
    "# # remove records from market_data_df that don't have transfers if configured to do so\n",
    "# if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "#     market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_full = profits_df.copy(deep=True)\n",
    "prices_df_full = prices_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = config['training_data']['training_period_end']\n",
    "\n",
    "new_rows_df = td.multithreaded_impute_profits_rows(profits_df, prices_df, target_date, 24)\n",
    "new_rows_df_full = new_rows_df.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_jr = profits_df_merged.sample(frac=0.0001).copy(deep=True)\n",
    "profits_df_jr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERIOD_START = config['training_data']['training_period_start']\n",
    "TRAINING_PERIOD_END = config['training_data']['training_period_end']\n",
    "MODELING_PERIOD_START = config['training_data']['modeling_period_start']\n",
    "MODELING_PERIOD_END = config['training_data']['modeling_period_end']\n",
    "\n",
    "\n",
    "class NotebookTestProfitsDataQuality:\n",
    "    def __init__(self, profits_df):\n",
    "        self.profits_df = profits_df\n",
    "\n",
    "    def run_all_tests(self):\n",
    "        test_methods = [method for method in dir(self) if method.startswith('test_')]\n",
    "        for method in test_methods:\n",
    "            try:\n",
    "                getattr(self, method)()\n",
    "                print(f\"{method} passed.\")\n",
    "            except AssertionError as e:\n",
    "                print(f\"{method} failed: {str(e)}\")\n",
    "\n",
    "    def test_no_duplicate_records(self):\n",
    "        deduped_df = self.profits_df[['coin_id', 'wallet_address', 'date']].drop_duplicates()\n",
    "        logger.info(f\"Original profits_df length: {len(self.profits_df)}, Deduplicated: {len(deduped_df)}\")\n",
    "        assert len(self.profits_df) == len(deduped_df), \"There are duplicate rows based on coin_id, wallet_address, and date\"\n",
    "\n",
    "    def test_records_at_training_period_end(self):\n",
    "        profits_df_filtered = self.profits_df[self.profits_df['date'] < MODELING_PERIOD_START]\n",
    "        pairs_in_training_period = profits_df_filtered[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "        period_end_df = self.profits_df[self.profits_df['date'] == TRAINING_PERIOD_END]\n",
    "\n",
    "        logger.info(f\"Found {len(pairs_in_training_period)} total pairs in training period with {len(period_end_df)} having data at period end.\")\n",
    "        assert len(pairs_in_training_period) == len(period_end_df), \"Not all training data coin-wallet pairs have a record at the end of the training period\"\n",
    "\n",
    "    def test_no_negative_usd_balances(self):\n",
    "        negative_balances = self.profits_df[self.profits_df['usd_balance'] < -0.1]\n",
    "        logger.info(f\"Found {len(negative_balances)} records with negative USD balances.\")\n",
    "        assert len(negative_balances) == 0, \"There are negative USD balances in the dataset\"\n",
    "\n",
    "    def test_date_range(self):\n",
    "        min_date = self.profits_df['date'].min()\n",
    "        max_date = self.profits_df['date'].max()\n",
    "        logger.info(f\"profits_df date range: {min_date} to {max_date}\")\n",
    "        assert max_date == pd.to_datetime(MODELING_PERIOD_END), f\"The last date in the dataset should be {MODELING_PERIOD_END}\"\n",
    "\n",
    "    def test_no_missing_values(self):\n",
    "        missing_values = self.profits_df.isna().sum()\n",
    "        assert missing_values.sum() == 0, f\"There are missing values in the dataset: {missing_values[missing_values > 0]}\"\n",
    "\n",
    "    def test_profits_consistency(self):\n",
    "        df = self.profits_df.copy().sort_values('date')\n",
    "        df['profits_change_check'] = df.groupby(['coin_id', 'wallet_address'], observed=True)['profits_cumulative'].diff()\n",
    "\n",
    "        # check for difference with the proftis_change and what we would expect\n",
    "        df['diff'] = df['profits_change'] - df['profits_change_check']\n",
    "\n",
    "        # we will also check the % difference relative to usd inflows to handle floating point rounding\n",
    "        df['diff_to_inflows'] = df['diff']/df['usd_inflows_cumulative']\n",
    "\n",
    "        threshold = 0.02\n",
    "        inconsistent_profits = df[\n",
    "            (~df['profits_change_check'].isna()) &\n",
    "            (df['diff'].abs() > threshold) &\n",
    "\n",
    "            # differences that are a tiny % of total inflows are likely due to floating point rounding\n",
    "            (df['diff_to_inflows'].abs() > 0.0001)\n",
    "        ]\n",
    "\n",
    "        if len(inconsistent_profits) > 0:\n",
    "            logger.warning(f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes.\")\n",
    "            logger.warning(\"Sample of inconsistent profits:\")\n",
    "            logger.warning(inconsistent_profits.head().to_string())\n",
    "\n",
    "        assert len(inconsistent_profits) == 0, f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes. Check logs for details.\"\n",
    "\n",
    "    def test_records_at_training_period_end_all_wallets(self):\n",
    "        training_profits_df = self.profits_df[self.profits_df['date'] <= TRAINING_PERIOD_END]\n",
    "        training_wallets_df = training_profits_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        training_end_df = self.profits_df[self.profits_df['date'] == TRAINING_PERIOD_END]\n",
    "        training_end_df = training_end_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        assert len(training_wallets_df) == len(training_end_df), \"Some wallets are missing a record as of the training_period_end\"\n",
    "\n",
    "    def test_records_at_modeling_period_start(self):\n",
    "        modeling_profits_df = self.profits_df[self.profits_df['date'] <= MODELING_PERIOD_START]\n",
    "        modeling_wallets_df = modeling_profits_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        modeling_start_df = self.profits_df[self.profits_df['date'] == MODELING_PERIOD_START]\n",
    "        modeling_start_df = modeling_start_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        assert len(modeling_wallets_df) == len(modeling_start_df), \"Some wallets are missing a record as of the modeling_period_start\"\n",
    "\n",
    "    def test_records_at_modeling_period_end(self):\n",
    "        modeling_profits_df = self.profits_df[self.profits_df['date'] <= MODELING_PERIOD_END]\n",
    "        modeling_wallets_df = modeling_profits_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        modeling_end_df = self.profits_df[self.profits_df['date'] == MODELING_PERIOD_END]\n",
    "        modeling_end_df = modeling_end_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        assert len(modeling_wallets_df) == len(modeling_end_df), \"Some wallets are missing a record as of the modeling_period_end\"\n",
    "\n",
    "    def test_no_records_before_training_period_start(self):\n",
    "        early_records = self.profits_df[self.profits_df['date'] < TRAINING_PERIOD_START]\n",
    "        assert len(early_records) == 0, f\"Found {len(early_records)} records prior to training_period_start\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = NotebookTestProfitsDataQuality(profits_df)\n",
    "tester.run_all_tests()  # Run all tests\n",
    "# tester.test_profits_consistency()  # Run a specific test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 'd4cdfafb-3175-4ce9-9b0f-e404e0f3f4da'\n",
    "w = 3975371\n",
    "df = u.cw_filter_df(profits_df,c,w).sort_values('date')\n",
    "\n",
    "df['profits_change_check'] = df.groupby(['coin_id', 'wallet_address'], observed=True)['profits_cumulative'].diff()\n",
    "df['diff'] = df['profits_change'] - df['profits_change_check']\n",
    "# we will also check the % difference relative to usd inflows to handle floating point rounding\n",
    "df['diff_to_inflows'] = df['diff']/df['usd_inflows_cumulative']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = u.cw_filter_df(profits_df,c,w).sort_values('date').copy()\n",
    "df['profits_change_check'] = df.groupby(['coin_id', 'wallet_address'], observed=True)['profits_cumulative'].diff()\n",
    "df['diff'] = df['profits_change'] - df['profits_change_check']\n",
    "\n",
    "threshold = 0.02\n",
    "inconsistent_profits = df[\n",
    "    (~df['profits_change_check'].isna()) &\n",
    "    (df['diff'].abs() > threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['profits_change_check'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profits_df.sample(frac=0.001).copy(deep=True)\n",
    "df['profits_change_check'] = df.groupby(['coin_id', 'wallet_address'], observed=True)['profits_cumulative'].diff()\n",
    "df['diff'] = df['profits_change'] - df['profits_change_check']\n",
    "\n",
    "# threshold = 0.02\n",
    "# inconsistent_profits = df[\n",
    "#     (~df['profits_change_check'].isna()) &\n",
    "#     (df['diff'].abs() > threshold)\n",
    "# ]\n",
    "\n",
    "# if len(inconsistent_profits) > 0:\n",
    "#     logger.warning(f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes.\")\n",
    "#     logger.warning(\"Sample of inconsistent profits:\")\n",
    "#     logger.warning(inconsistent_profits.head().to_string())\n",
    "\n",
    "# assert len(inconsistent_profits) == 0, f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes. Check logs for details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.02\n",
    "\n",
    "inconsistent_profits = df[\n",
    "    (~df['profits_change_check'].isna()) &\n",
    "    (df['diff'].abs() > threshold)\n",
    "]\n",
    "inconsistent_profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_address = 12147564\n",
    "coin_id = '4b833d21-0490-4256-8fd6-0e384f7d3b16'\n",
    "\n",
    "test_df = profits_df[(profits_df['coin_id']==coin_id) & (profits_df['wallet_address']==wallet_address)]\n",
    "test_df = test_df.sort_values('date',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 3.3 Build the configured model input data (train/test data)\n",
    "X_train, X_test, y_train, y_test = i.build_configured_model_input(profits_df, market_data_df, config, metrics_config, modeling_config)\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate and save the model's performance on the test set to a CSV\n",
    "evals = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config['time_series']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_profits_df = profits_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "target_date = config['training_data']['training_period_end']\n",
    "new_rows_df = td.impute_profits_df_rows(profits_df, prices_df, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df = full_profits_df.copy(deep=True)\n",
    "profits_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "dates_to_impute = [\n",
    "    config['training_data']['training_period_end'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end'],\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def impute_profits_for_multiple_dates(profits_df, prices_df, dates, n_threads):\n",
    "    \"\"\"\n",
    "    Wrapper function to impute profits for multiple dates using multithreaded processing.\n",
    "\n",
    "    Args:\n",
    "        profits_df (pd.DataFrame): DataFrame containing dated profits data for coin-wallet pairs\n",
    "        prices_df (pd.DataFrame): DataFrame containing price information\n",
    "        dates (list): List of dates (str or datetime) for which to impute rows\n",
    "        n_threads (int): The number of threads to use for imputation\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated profits_df with imputed rows for all specified dates\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting profits_df imputation for %s dates...\", len(dates))\n",
    "\n",
    "    new_rows_list = []\n",
    "\n",
    "    for date in dates:\n",
    "        new_rows_df = td.multithreaded_impute_profits_rows(profits_df, prices_df, date, n_threads)\n",
    "        new_rows_list.append(new_rows_df)\n",
    "\n",
    "    # Concatenate all new rows at once\n",
    "    all_new_rows = pd.concat(new_rows_list, ignore_index=True)\n",
    "\n",
    "    # Append all new rows to profits_df\n",
    "    updated_profits_df = pd.concat([profits_df, all_new_rows], ignore_index=True)\n",
    "\n",
    "    logger.info(\"Completed new row generation after %.2f seconds. Total rows after imputation: %s\",\n",
    "                time.time() - start_time,\n",
    "                updated_profits_df.shape[0])\n",
    "\n",
    "    return updated_profits_df\n",
    "\n",
    "update_profits_df = impute_profits_for_multiple_dates(profits_df, prices_df, dates_to_impute, n_threads=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "optimal_thread_count = multiprocessing.cpu_count()\n",
    "optimal_thread_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient columns\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "coin_id_mapping = dict(enumerate(profits_df['coin_id'].cat.categories))\n",
    "profits_df['coin_id'] = profits_df['coin_id'].cat.codes.astype('int16')\n",
    "\n",
    "# Convert date column to store the difference in days relative to target_date\n",
    "profits_df['date'] = (profits_df['date'] - target_date).dt.days.astype('int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vars\n",
    "# target_date = '2024-08-31'\n",
    "# # new_rows_df = generate_new_row(profits_df, prices_df, target_date)\n",
    "\n",
    "# target_date = pd.to_datetime(target_date)\n",
    "\n",
    "# # # Create efficient indexes\n",
    "# # profits_df = profits_df.set_index(['coin_id', 'wallet_address', 'date']).copy(deep=True)\n",
    "# # prices_df = prices_df.set_index(['coin_id', 'date']).copy(deep=True)\n",
    "\n",
    "# # # Identify pairs needing new rows\n",
    "# # logger.debug('Identifying pairs that need a row for %s...', target_date)\n",
    "# # all_pairs = profits_df.index.droplevel('date').unique()\n",
    "# # existing_pairs = profits_df.loc(axis=0)[:, :, target_date].index.droplevel('date')\n",
    "# # pairs_needing_rows = all_pairs.difference(existing_pairs)\n",
    "# # logger.debug('Identified %s pairs that will need rows imputed.', len(pairs_needing_rows))\n",
    "\n",
    "\n",
    "# new_rows = []\n",
    "\n",
    "# logger.debug('Imputing new rows...')\n",
    "# for coin_id, wallet_address in pairs_needing_rows:\n",
    "#     # Get most recent record\n",
    "#     recent_record = profits_df.loc[coin_id, wallet_address].loc[:target_date].iloc[-1]\n",
    "\n",
    "#     # Get prices\n",
    "#     price_previous = prices_df.loc[(coin_id, recent_record.name), 'price']\n",
    "#     price_current = prices_df.loc[(coin_id, target_date), 'price']\n",
    "\n",
    "#     # Calculate new values\n",
    "#     price_ratio = price_current / price_previous\n",
    "#     new_usd_balance = recent_record['usd_balance'] * price_ratio\n",
    "#     profits_change = new_usd_balance - recent_record['usd_balance']\n",
    "#     profits_cumulative = recent_record['profits_cumulative'] + profits_change\n",
    "\n",
    "#     new_row = {\n",
    "#         'coin_id': coin_id,\n",
    "#         'wallet_address': wallet_address,\n",
    "#         'date': target_date,\n",
    "#         'profits_change': profits_change,\n",
    "#         'profits_cumulative': profits_cumulative,\n",
    "#         'usd_balance': new_usd_balance,\n",
    "#         'usd_net_transfers': 0,\n",
    "#         'usd_inflows': 0,\n",
    "#         'usd_inflows_cumulative': recent_record['usd_inflows_cumulative'],\n",
    "#         'total_return': profits_cumulative / max(recent_record['usd_inflows_cumulative'], 0.01)\n",
    "#     }\n",
    "\n",
    "#     new_rows.append(new_row)\n",
    "\n",
    "# new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# logger.debug('Generated new_rows_df with shape %s.', new_rows_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent data for pairs needing rows\n",
    "most_recent_data = profits_df.loc[pairs_needing_rows]\n",
    "most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "\n",
    "# Reset index of prices_df for the merge operation\n",
    "prices_df_reset = prices_df.reset_index()\n",
    "\n",
    "# Perform asof merge to get the most recent price before or on the date of each record\n",
    "merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "                            prices_df_reset.sort_values('date'),\n",
    "                            on='date',\n",
    "                            by='coin_id',\n",
    "                            direction='backward')\n",
    "\n",
    "# Now get the price at the target date\n",
    "target_prices = prices_df.loc(axis=0)[:, target_date].reset_index()\n",
    "target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# Merge the target prices\n",
    "merged_data = pd.merge(merged_data, target_prices[['coin_id', 'target_price']], on='coin_id', how='left')\n",
    "\n",
    "# Calculate price ratio\n",
    "merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n",
    "\n",
    "logger.debug('Merged data shape: %s', merged_data.shape)\n",
    "logger.debug('Merged data columns: %s', merged_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent row for each pair needing a new row\n",
    "most_recent_data = profits_df.loc[profits_df.index.isin(pairs_needing_rows, level=['coin_id', 'wallet_address'])]\n",
    "# most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# # Ensure the date column is properly formatted\n",
    "# most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "# prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "\n",
    "# # Perform asof merge to get the most recent price before or on the date of each record\n",
    "# merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "#                             prices_df[['date', 'coin_id', 'price']].sort_values('date'),\n",
    "#                             on='date',\n",
    "#                             by='coin_id',\n",
    "#                             direction='backward')\n",
    "\n",
    "# # Now get the price at the target date\n",
    "# target_prices = prices_df[prices_df['date'] == target_date][['coin_id', 'price']]\n",
    "# target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# # Merge the target prices\n",
    "# merged_data = pd.merge(merged_data, target_prices, on='coin_id', how='left')\n",
    "\n",
    "# # Calculate price ratio\n",
    "# merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
