{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "\n",
    "CONFIG = u.load_config('../config/config.yaml')\n",
    "METRICS_CONFIG = u.load_config('../config/metrics_config.yaml')\n",
    "MODELING_CONFIG = u.load_config('../config/modeling_config.yaml')\n",
    "EXPERIMENTS_CONFIG = u.load_config('../config/experiments_config.yaml')\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']\n",
    "modeling_folder = MODELING_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve profits data\n",
    "profits_df = td.retrieve_profits_data(start_date, end_date)\n",
    "profits_df, _ = cwm.split_dataframe_by_coverage(profits_df, start_date, end_date, id_column='coin_id')\n",
    "profits_df, _ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "\n",
    "# remove records from market_data_df that don't have transfers if configured to do so\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profits_df_full = profits_df.copy(deep=True)\n",
    "# market_data_df_full = market_data_df.copy(deep=True)\n",
    "# prices_df_full = prices_df.copy(deep=True)\n",
    "\n",
    "profits_df = profits_df_full.copy(deep=True)\n",
    "market_data_df = market_data_df_full.copy(deep=True)\n",
    "prices_df = prices_df_full.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "profits_df = profits_df_full.copy(deep=True)\n",
    "prices_df = prices_df_full.copy(deep=True)\n",
    "target_date = '2024-08-31'\n",
    "\n",
    "profits_df_filled = td.impute_profits_df_rows(profits_df,prices_df,target_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitions(profits_df, n_partitions):\n",
    "    \"\"\"\n",
    "    Partition a DataFrame into multiple subsets based on unique coin_ids.\n",
    "\n",
    "    Parameters:\n",
    "    - profits_df (pd.DataFrame): The input DataFrame to be partitioned. Must contain\n",
    "        a 'coin_id' column.\n",
    "    - n_partitions (int): The number of partitions to create.\n",
    "\n",
    "    Returns:\n",
    "    - partition_dfs (List[pd.DataFrame]): A list of DataFrames, each representing\n",
    "        a partition of the original data.\n",
    "    \"\"\"\n",
    "    # Get unique coin_ids and convert to a regular list\n",
    "    unique_coin_ids = profits_df['coin_id'].unique().tolist()\n",
    "\n",
    "    # Shuffle the list of coin_ids\n",
    "    np.random.seed(88)\n",
    "    np.random.shuffle(unique_coin_ids)\n",
    "\n",
    "    # Calculate the number of coin_ids per partition\n",
    "    coins_per_partition = len(unique_coin_ids) // n_partitions\n",
    "\n",
    "    # Create partitions\n",
    "    partition_dfs = []\n",
    "    for i in range(n_partitions):\n",
    "        start_idx = i * coins_per_partition\n",
    "        end_idx = start_idx + coins_per_partition if i < n_partitions - 1 else None\n",
    "        partition_coin_ids = unique_coin_ids[start_idx:end_idx]\n",
    "\n",
    "        # Create a boolean mask for the current partition\n",
    "        mask = profits_df['coin_id'].isin(partition_coin_ids)\n",
    "\n",
    "        # Add the partition to the list\n",
    "        partition_dfs.append(profits_df[mask])\n",
    "\n",
    "    return partition_dfs\n",
    "\n",
    "\n",
    "n_partitions = 6\n",
    "partitions = create_partitions(profits_df, n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def worker(partition, prices_df, target_date, result_queue):\n",
    "    \"\"\"\n",
    "    Worker function to process a partition and put the result in the queue.\n",
    "    \"\"\"\n",
    "    result = td.impute_profits_df_rows(partition, prices_df, target_date)\n",
    "    result_queue.put(result)\n",
    "\n",
    "def multithreaded_impute_profits(partitions, prices_df, target_date):\n",
    "    \"\"\"\n",
    "    Process partitions using multithreading and merge results.\n",
    "\n",
    "    Args:\n",
    "        partitions (list): List of DataFrame partitions\n",
    "        prices_df (pd.DataFrame): DataFrame containing price information\n",
    "        target_date (str or datetime): The date for which to impute rows\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged result of all processed partitions\n",
    "    \"\"\"\n",
    "    # Create a thread-safe queue to store results\n",
    "    result_queue = queue.Queue()\n",
    "\n",
    "    # Create a list to hold thread objects\n",
    "    threads = []\n",
    "\n",
    "    # Create and start a thread for each partition\n",
    "    for partition in partitions:\n",
    "        thread = threading.Thread(\n",
    "            target=worker,\n",
    "            args=(partition, prices_df, target_date, result_queue)\n",
    "        )\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    while not result_queue.empty():\n",
    "        results.append(result_queue.get())\n",
    "\n",
    "    # Merge results\n",
    "    merged_result = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return merged_result\n",
    "\n",
    "\n",
    "result = multithreaded_impute_profits(partitions, prices_df, target_date)\n",
    "\n",
    "# Any post-processing on the result can be done here\n",
    "print(f\"Processed DataFrame shape: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_partition_performance(profits_df, prices_df, target_date, partition_numbers):\n",
    "    \"\"\"\n",
    "    Test the performance of the multithreaded_impute_profits function with different numbers of partitions.\n",
    "\n",
    "    This function iterates through the provided partition numbers, running the multithreaded_impute_profits\n",
    "    function for each. It measures the execution time, logs the result size, and generates a performance plot.\n",
    "\n",
    "    Args:\n",
    "        profits_df (pd.DataFrame): The input DataFrame containing profit information.\n",
    "        prices_df (pd.DataFrame): The input DataFrame containing price information.\n",
    "        target_date (str or datetime): The target date for imputation.\n",
    "        partition_numbers (list of int): A list of partition numbers to test.\n",
    "\n",
    "    Returns:\n",
    "        list of tuple: A list of (partition_number, execution_time) tuples.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the size of any result DataFrame differs from the others.\n",
    "\n",
    "    Side effects:\n",
    "        - Prints the execution time for each partition number to the console.\n",
    "        - Logs the size of the result DataFrame for each partition number.\n",
    "        - Generates and displays a plot of execution time vs. number of partitions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    expected_size = None\n",
    "\n",
    "    for n_partitions in partition_numbers:\n",
    "        start_time = time.time()\n",
    "\n",
    "        partitions = create_partitions(profits_df, n_partitions)\n",
    "        result = multithreaded_impute_profits(partitions, prices_df, target_date)\n",
    "\n",
    "        # Check size consistency\n",
    "        current_size = result.shape[0]\n",
    "        if expected_size is None:\n",
    "            expected_size = current_size\n",
    "        elif current_size != expected_size:\n",
    "            raise ValueError(f\"Inconsistent result size detected. Expected {expected_size} rows, \"\n",
    "                             f\"but got {current_size} rows for {n_partitions} partitions.\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        results.append((n_partitions, execution_time))\n",
    "        logger.info(\"Partitions: %s, Result Shape: %s , Time: %.2f seconds\"\n",
    "                    ,n_partitions\n",
    "                    ,result.shape\n",
    "                    ,execution_time)\n",
    "\n",
    "    # Generate the plot\n",
    "    partitions, times = zip(*results)\n",
    "    plt.plot(partitions, times, marker='o')\n",
    "    plt.xlabel('Number of Partitions')\n",
    "    plt.ylabel('Execution Time (seconds)')\n",
    "    plt.title('Performance vs Number of Partitions')\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Assuming profits_df, prices_df, and target_date are already defined\n",
    "partition_numbers = [11,12,13,16,22,24]\n",
    "results = test_partition_performance(profits_df, prices_df, target_date, partition_numbers)\n",
    "\n",
    "# Optional: Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "partitions, times = zip(*results)\n",
    "plt.plot(partitions, times, marker='o')\n",
    "plt.xlabel('Number of Partitions')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Performance vs Number of Partitions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient columns\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "coin_id_mapping = dict(enumerate(profits_df['coin_id'].cat.categories))\n",
    "profits_df['coin_id'] = profits_df['coin_id'].cat.codes.astype('int16')\n",
    "\n",
    "# Convert date column to store the difference in days relative to target_date\n",
    "profits_df['date'] = (profits_df['date'] - target_date).dt.days.astype('int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vars\n",
    "# target_date = '2024-08-31'\n",
    "# # new_rows_df = generate_new_row(profits_df, prices_df, target_date)\n",
    "\n",
    "# target_date = pd.to_datetime(target_date)\n",
    "\n",
    "# # # Create efficient indexes\n",
    "# # profits_df = profits_df.set_index(['coin_id', 'wallet_address', 'date']).copy(deep=True)\n",
    "# # prices_df = prices_df.set_index(['coin_id', 'date']).copy(deep=True)\n",
    "\n",
    "# # # Identify pairs needing new rows\n",
    "# # logger.debug('Identifying pairs that need a row for %s...', target_date)\n",
    "# # all_pairs = profits_df.index.droplevel('date').unique()\n",
    "# # existing_pairs = profits_df.loc(axis=0)[:, :, target_date].index.droplevel('date')\n",
    "# # pairs_needing_rows = all_pairs.difference(existing_pairs)\n",
    "# # logger.debug('Identified %s pairs that will need rows imputed.', len(pairs_needing_rows))\n",
    "\n",
    "\n",
    "# new_rows = []\n",
    "\n",
    "# logger.debug('Imputing new rows...')\n",
    "# for coin_id, wallet_address in pairs_needing_rows:\n",
    "#     # Get most recent record\n",
    "#     recent_record = profits_df.loc[coin_id, wallet_address].loc[:target_date].iloc[-1]\n",
    "\n",
    "#     # Get prices\n",
    "#     price_previous = prices_df.loc[(coin_id, recent_record.name), 'price']\n",
    "#     price_current = prices_df.loc[(coin_id, target_date), 'price']\n",
    "\n",
    "#     # Calculate new values\n",
    "#     price_ratio = price_current / price_previous\n",
    "#     new_usd_balance = recent_record['usd_balance'] * price_ratio\n",
    "#     profits_change = new_usd_balance - recent_record['usd_balance']\n",
    "#     profits_cumulative = recent_record['profits_cumulative'] + profits_change\n",
    "\n",
    "#     new_row = {\n",
    "#         'coin_id': coin_id,\n",
    "#         'wallet_address': wallet_address,\n",
    "#         'date': target_date,\n",
    "#         'profits_change': profits_change,\n",
    "#         'profits_cumulative': profits_cumulative,\n",
    "#         'usd_balance': new_usd_balance,\n",
    "#         'usd_net_transfers': 0,\n",
    "#         'usd_inflows': 0,\n",
    "#         'usd_inflows_cumulative': recent_record['usd_inflows_cumulative'],\n",
    "#         'total_return': profits_cumulative / max(recent_record['usd_inflows_cumulative'], 0.01)\n",
    "#     }\n",
    "\n",
    "#     new_rows.append(new_row)\n",
    "\n",
    "# new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# logger.debug('Generated new_rows_df with shape %s.', new_rows_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent data for pairs needing rows\n",
    "most_recent_data = profits_df.loc[pairs_needing_rows]\n",
    "most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "\n",
    "# Reset index of prices_df for the merge operation\n",
    "prices_df_reset = prices_df.reset_index()\n",
    "\n",
    "# Perform asof merge to get the most recent price before or on the date of each record\n",
    "merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "                            prices_df_reset.sort_values('date'),\n",
    "                            on='date',\n",
    "                            by='coin_id',\n",
    "                            direction='backward')\n",
    "\n",
    "# Now get the price at the target date\n",
    "target_prices = prices_df.loc(axis=0)[:, target_date].reset_index()\n",
    "target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# Merge the target prices\n",
    "merged_data = pd.merge(merged_data, target_prices[['coin_id', 'target_price']], on='coin_id', how='left')\n",
    "\n",
    "# Calculate price ratio\n",
    "merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n",
    "\n",
    "logger.debug('Merged data shape: %s', merged_data.shape)\n",
    "logger.debug('Merged data columns: %s', merged_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent row for each pair needing a new row\n",
    "most_recent_data = profits_df.loc[profits_df.index.isin(pairs_needing_rows, level=['coin_id', 'wallet_address'])]\n",
    "# most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# # Ensure the date column is properly formatted\n",
    "# most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "# prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "\n",
    "# # Perform asof merge to get the most recent price before or on the date of each record\n",
    "# merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "#                             prices_df[['date', 'coin_id', 'price']].sort_values('date'),\n",
    "#                             on='date',\n",
    "#                             by='coin_id',\n",
    "#                             direction='backward')\n",
    "\n",
    "# # Now get the price at the target date\n",
    "# target_prices = prices_df[prices_df['date'] == target_date][['coin_id', 'price']]\n",
    "# target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# # Merge the target prices\n",
    "# merged_data = pd.merge(merged_data, target_prices, on='coin_id', how='left')\n",
    "\n",
    "# # Calculate price ratio\n",
    "# merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
