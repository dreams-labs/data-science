{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "\n",
    "CONFIG = u.load_config('../config/config.yaml')\n",
    "METRICS_CONFIG = u.load_config('../config/metrics_config.yaml')\n",
    "MODELING_CONFIG = u.load_config('../config/modeling_config.yaml')\n",
    "EXPERIMENTS_CONFIG = u.load_config('../config/experiments_config.yaml')\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']\n",
    "modeling_folder = MODELING_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve profits data\n",
    "profits_df = td.retrieve_profits_data(start_date, end_date)\n",
    "profits_df, _ = cwm.split_dataframe_by_coverage(profits_df, start_date, end_date, id_column='coin_id')\n",
    "profits_df, _ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "# # impute period boundary dates\n",
    "# dates_to_impute = [\n",
    "#     config['training_data']['training_period_end'],\n",
    "#     config['training_data']['modeling_period_start'],\n",
    "#     config['training_data']['modeling_period_end'],\n",
    "# ]\n",
    "# profits_df_merged = td.impute_profits_for_multiple_dates(profits_df, prices_df, dates_to_impute, n_threads=24)\n",
    "\n",
    "# # remove records from market_data_df that don't have transfers if configured to do so\n",
    "# if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "#     market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profits_df_full = profits_df.copy(deep=True)\n",
    "# prices_df_full = prices_df.copy(deep=True)\n",
    "# profits_df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = config['training_data']['training_period_end']\n",
    "\n",
    "new_rows_df = td.multithreaded_impute_profits_rows(profits_df, prices_df, target_date, 24)\n",
    "# new_rows_df2 = td.impute_profits_df_rows(profits_df, prices_df, target_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df = profits_df_full.copy(deep=True)\n",
    "prices_df = prices_df_full.copy(deep=True)\n",
    "\n",
    "start_time = time.time()\n",
    "logger.info('%s Imputing rows for all coin-wallet pairs in profits_df on %s...',\n",
    "            profits_df.shape,\n",
    "            target_date)\n",
    "\n",
    "# Convert date to datetime\n",
    "target_date = pd.to_datetime(target_date)\n",
    "\n",
    "# Create indices so we can use vectorized operations\n",
    "profits_df = profits_df.set_index(['coin_id', 'wallet_address', 'date'])\n",
    "prices_df = prices_df.set_index(['coin_id', 'date']).copy(deep=True)\n",
    "\n",
    "# Step 1: Split profits_df records before and after the target_date\n",
    "# -----------------------------------------------------------------\n",
    "profits_df_after_target = profits_df.xs(\n",
    "    slice(target_date + pd.Timedelta('1 day'), None),\n",
    "    level=2,\n",
    "    drop_level=False)\n",
    "profits_df = profits_df.xs(slice(None, target_date), level=2, drop_level=False)\n",
    "\n",
    "logger.debug(\"%s <Step 1> Split profits_df into %s rows through the target_date and %s after \"\n",
    "                \"target_date: %.2f seconds\",\n",
    "                profits_df.shape,\n",
    "                len(profits_df),\n",
    "                len(profits_df_after_target),\n",
    "                time.time() - start_time)\n",
    "step_time = time.time()\n",
    "\n",
    "\n",
    "# Step 2: Filter profits_df to only pairs that need new rows\n",
    "# ----------------------------------------------------------\n",
    "# Create a boolean mask for rows at the target_date\n",
    "target_date_mask = profits_df.index.get_level_values('date') == target_date\n",
    "\n",
    "# Create a boolean mask for pairs that don't have a row at the target_date\n",
    "pairs_mask = ~profits_df.index.droplevel('date').isin(\n",
    "    profits_df[target_date_mask].index.droplevel('date')\n",
    ")\n",
    "profits_df = profits_df[pairs_mask].sort_index()\n",
    "\n",
    "logger.debug(\"%s <Step 2> Identified %s coin-wallet pairs that need imputed rows: %.2f seconds\",\n",
    "                profits_df.shape,\n",
    "                len(profits_df),\n",
    "                time.time() - step_time)\n",
    "step_time = time.time()\n",
    "\n",
    "\n",
    "# Step 3: Identify the last date for each coin-wallet pair\n",
    "# ----------------------------------------------\n",
    "# The logic here is that every row that doesn't have the same coin_id-wallet_address\n",
    "# combination as the previous row must indicate that the previous coin-wallet pair\n",
    "# just had its last date.\n",
    "\n",
    "# Create shifted index\n",
    "shifted_index = profits_df.index.to_frame().shift(-1)\n",
    "\n",
    "# Create boolean mask for last dates\n",
    "is_last_date = (profits_df.index.get_level_values('coin_id') != shifted_index['coin_id']) | \\\n",
    "        (profits_df.index.get_level_values('wallet_address') != shifted_index['wallet_address'])\n",
    "\n",
    "# Filter for last dates\n",
    "profits_df = profits_df[is_last_date]\n",
    "\n",
    "logger.debug(\"%s <Step 3> Filtered profits_df to only the last dates for each coin-wallet \"\n",
    "                \"pair: %.2f seconds\",\n",
    "                profits_df.shape,\n",
    "                time.time() - step_time)\n",
    "step_time = time.time()\n",
    "\n",
    "\n",
    "# Step 4: Append columns for previous_price (on last date) and price (on the target_date)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# Add price_previous by joining the price as of the last date for each coin-wallet pair\n",
    "prejoin_size = len(profits_df)\n",
    "profits_df = profits_df.join(prices_df['price'], on=['coin_id', 'date'], how='inner')\n",
    "profits_df = profits_df.rename(columns={'price': 'price_previous'})\n",
    "\n",
    "# Add price by joining the price as of the target_date\n",
    "prices_target_date = prices_df.xs(target_date, level='date')\n",
    "profits_df = profits_df.join(prices_target_date['price'], on='coin_id', how='inner')\n",
    "\n",
    "if len(profits_df) != prejoin_size:\n",
    "    raise ValueError(str(\"Inner join to prices_df on coin_id-date removed %s rows from\"\n",
    "                            \"profits_df with original length %s. There should be complete\"\n",
    "                            \"coverage for all rows in profits_df.\",\n",
    "                            prejoin_size-len(profits_df),\n",
    "                            len(profits_df)))\n",
    "\n",
    "logger.debug(\"%s <Step 4> Joined prices_df and added price and previous_price helper \"\n",
    "                \"columns: %.2f seconds\",\n",
    "                profits_df.shape,\n",
    "                time.time() - step_time)\n",
    "step_time = time.time()\n",
    "\n",
    "\n",
    "# Step 5: Calculate new values for pairs needing rows\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Create a new df using the same coin-wallet pairs\n",
    "new_rows_df = pd.DataFrame(index=profits_df.index)\n",
    "\n",
    "# Calculate financial metrics\n",
    "new_rows_df['profits_change'] = ((profits_df['price'] / profits_df['price_previous'] - 1)\n",
    "                                    * profits_df['usd_balance'])\n",
    "new_rows_df['profits_cumulative'] = (new_rows_df['profits_change']\n",
    "                                        + profits_df['profits_cumulative'])\n",
    "new_rows_df['usd_balance'] = ((profits_df['price'] / profits_df['price_previous'])\n",
    "                                * profits_df['usd_balance'])\n",
    "new_rows_df['usd_net_transfers'] = 0\n",
    "new_rows_df['usd_inflows'] = 0\n",
    "new_rows_df['usd_inflows_cumulative'] = profits_df['usd_inflows_cumulative']\n",
    "new_rows_df['total_return'] = (new_rows_df['profits_cumulative']\n",
    "                                / new_rows_df['usd_inflows_cumulative'])\n",
    "\n",
    "# Override the profits_df date index to be the target_date\n",
    "new_rows_df = new_rows_df.reset_index()\n",
    "new_rows_df['date'] = target_date  # pylint: disable=E1137 # df does not support item assignment\n",
    "new_rows_df = new_rows_df.set_index(['coin_id', 'wallet_address', 'date'])\n",
    "\n",
    "logger.debug(\"%s <Step 5> Calculated %s new rows: %.2f seconds\",\n",
    "                profits_df.shape,\n",
    "                len(new_rows_df),\n",
    "                time.time() - step_time)\n",
    "step_time = time.time()\n",
    "\n",
    "# Step 6: Reset MultiIndex and concatenate dfs\n",
    "# --------------------------------------------\n",
    "new_rows_df = new_rows_df.reset_index()\n",
    "\n",
    "logger.debug(\"%s <Step 6> Reset indices and added new rows to profits_df: %.2f seconds\",\n",
    "                new_rows_df.shape,\n",
    "                time.time() - step_time)\n",
    "logger.info(\"%s Successfully generated new_rows_df with shape %s after %.2f total seconds.\",\n",
    "            new_rows_df.shape,\n",
    "            new_rows_df.shape,\n",
    "            time.time() - start_time)\n",
    "\n",
    "profits_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows_df_full = pd.DataFrame(index=profits_df.index)\n",
    "new_rows_df = new_rows_df_full.copy(deep=True)\n",
    "new_rows_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows_df = new_rows_df_full.copy(deep=True)\n",
    "\n",
    "\n",
    "# Step 5: Calculate new values for pairs needing rows\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Create a new df using the same coin-wallet pairs\n",
    "new_rows_df = pd.DataFrame(index=profits_df.index)\n",
    "\n",
    "# Override the profits_df date index to be the target_date\n",
    "new_rows_df = new_rows_df.reset_index()\n",
    "new_rows_df['date'] = target_date  # pylint: disable=E1137 # df does not support item assignment\n",
    "new_rows_df = new_rows_df.set_index(['coin_id', 'wallet_address', 'date'])\n",
    "\n",
    "# Calculate financial metrics\n",
    "new_rows_df['profits_change'] = ((profits_df['price'] / profits_df['price_previous'] - 1) * profits_df['usd_balance'])\n",
    "# new_rows_df['profits_cumulative'] = (new_rows_df['profits_change']\n",
    "#                                         + profits_df['profits_cumulative'])\n",
    "# new_rows_df['usd_balance'] = ((profits_df['price'] / profits_df['price_previous'])\n",
    "#                                 * profits_df['usd_balance'])\n",
    "# new_rows_df['usd_net_transfers'] = 0\n",
    "# new_rows_df['usd_inflows'] = 0\n",
    "# new_rows_df['usd_inflows_cumulative'] = profits_df['usd_inflows_cumulative']\n",
    "# new_rows_df['total_return'] = (new_rows_df['profits_cumulative']\n",
    "#                                 / new_rows_df['usd_inflows_cumulative'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_rows_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((profits_df['price'] / profits_df['price_previous'] - 1) * profits_df['usd_balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "profits_df_merged = td.impute_profits_for_multiple_dates(profits_df, prices_df, dates_to_impute, n_threads=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profits_df.shape)\n",
    "profits_df.sample(frac=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_profits_df_rows(profits_df, prices_df, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_jr = profits_df_merged.sample(frac=0.0001).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_jr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df_jr.head()\n",
    "profits_df_jr[profits_df_jr['date']=='2024-09-02'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = '2024-01-01'\n",
    "\n",
    "new_rows_df = pd.DataFrame(index=profits_df_jr.index)\n",
    "new_rows_df = new_rows_df.reset_index()\n",
    "new_rows_df['date'] = target_date\n",
    "new_rows_df = new_rows_df.set_index(['coin_id', 'wallet_address', 'date'])\n",
    "\n",
    "# new_rows_df['date'] = target_date\n",
    "# new_rows_df['profits_change'] = ((profits_df['price'] / profits_df['price_previous'] - 1)\n",
    "#                                     * profits_df['usd_balance'])\n",
    "# new_rows_df['profits_cumulative'] = (new_rows_df['profits_change']\n",
    "#                                         + profits_df['profits_cumulative'])\n",
    "# new_rows_df['usd_balance'] = ((profits_df['price'] / profits_df['price_previous'])\n",
    "#                                 * profits_df['usd_balance'])\n",
    "# new_rows_df['usd_net_transfers'] = 0\n",
    "# new_rows_df['usd_inflows'] = 0\n",
    "# new_rows_df['usd_inflows_cumulative'] = profits_df['usd_inflows_cumulative']\n",
    "# new_rows_df['total_return'] = (new_rows_df['profits_cumulative']\n",
    "#                                 / new_rows_df['usd_inflows_cumulative'])\n",
    "# new_rows_df['price_previous'] = profits_df['price_previous']\n",
    "# new_rows_df['price'] = profits_df['price']\n",
    "\n",
    "new_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(profits_df[profits_df['date']].isin(config['training_data']['training_period_start']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_df = profits_df[['coin_id', 'wallet_address', 'date']].drop_duplicates()\n",
    "logger.info(f\"Original profits_df length: {len(profits_df)}, Deduplicated: {len(deduped_df)}\")\n",
    "assert len(profits_df) == len(deduped_df), \"There are duplicate rows based on coin_id, wallet_address, and date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERIOD_START = config['training_data']['training_period_start']\n",
    "TRAINING_PERIOD_END = config['training_data']['training_period_end']\n",
    "MODELING_PERIOD_START = config['training_data']['modeling_period_start']\n",
    "MODELING_PERIOD_END = config['training_data']['modeling_period_end']\n",
    "\n",
    "\n",
    "class NotebookTestProfitsDataQuality:\n",
    "    def __init__(self, profits_df):\n",
    "        self.profits_df = profits_df\n",
    "\n",
    "    def run_all_tests(self):\n",
    "        test_methods = [method for method in dir(self) if method.startswith('test_')]\n",
    "        for method in test_methods:\n",
    "            try:\n",
    "                getattr(self, method)()\n",
    "                print(f\"{method} passed.\")\n",
    "            except AssertionError as e:\n",
    "                print(f\"{method} failed: {str(e)}\")\n",
    "\n",
    "    def test_no_duplicate_records(self):\n",
    "        deduped_df = self.profits_df[['coin_id', 'wallet_address', 'date']].drop_duplicates()\n",
    "        logger.info(f\"Original profits_df length: {len(self.profits_df)}, Deduplicated: {len(deduped_df)}\")\n",
    "        assert len(self.profits_df) == len(deduped_df), \"There are duplicate rows based on coin_id, wallet_address, and date\"\n",
    "\n",
    "    def test_records_at_training_period_end(self):\n",
    "        profits_df_filtered = self.profits_df[self.profits_df['date'] < MODELING_PERIOD_START]\n",
    "        pairs_in_training_period = profits_df_filtered[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "        period_end_df = self.profits_df[self.profits_df['date'] == TRAINING_PERIOD_END]\n",
    "\n",
    "        logger.info(f\"Found {len(pairs_in_training_period)} total pairs in training period with {len(period_end_df)} having data at period end.\")\n",
    "        assert len(pairs_in_training_period) == len(period_end_df), \"Not all training data coin-wallet pairs have a record at the end of the training period\"\n",
    "\n",
    "    def test_no_negative_usd_balances(self):\n",
    "        negative_balances = self.profits_df[self.profits_df['usd_balance'] < -0.1]\n",
    "        logger.info(f\"Found {len(negative_balances)} records with negative USD balances.\")\n",
    "        assert len(negative_balances) == 0, \"There are negative USD balances in the dataset\"\n",
    "\n",
    "    def test_date_range(self):\n",
    "        min_date = self.profits_df['date'].min()\n",
    "        max_date = self.profits_df['date'].max()\n",
    "        logger.info(f\"profits_df date range: {min_date} to {max_date}\")\n",
    "        assert max_date == pd.to_datetime(MODELING_PERIOD_END), f\"The last date in the dataset should be {MODELING_PERIOD_END}\"\n",
    "\n",
    "    def test_no_missing_values(self):\n",
    "        missing_values = self.profits_df.isna().sum()\n",
    "        assert missing_values.sum() == 0, f\"There are missing values in the dataset: {missing_values[missing_values > 0]}\"\n",
    "\n",
    "    def test_profits_consistency(self):\n",
    "        df = self.profits_df.copy()\n",
    "        df['profits_change_check'] = df.groupby(['coin_id', 'wallet_address'], observed=True)['profits_cumulative'].diff()\n",
    "        df['diff'] = df['profits_change'] - df['profits_change_check']\n",
    "\n",
    "        threshold = 0.02\n",
    "        inconsistent_profits = df[\n",
    "            (~df['profits_change_check'].isna()) &\n",
    "            (df['diff'].abs() > threshold)\n",
    "        ]\n",
    "\n",
    "        if len(inconsistent_profits) > 0:\n",
    "            logger.warning(f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes.\")\n",
    "            logger.warning(\"Sample of inconsistent profits:\")\n",
    "            logger.warning(inconsistent_profits.head().to_string())\n",
    "\n",
    "        assert len(inconsistent_profits) == 0, f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes. Check logs for details.\"\n",
    "\n",
    "    def test_records_at_training_period_end_all_wallets(self):\n",
    "        training_profits_df = self.profits_df[self.profits_df['date'] <= TRAINING_PERIOD_END]\n",
    "        training_wallets_df = training_profits_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        training_end_df = self.profits_df[self.profits_df['date'] == TRAINING_PERIOD_END]\n",
    "        training_end_df = training_end_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        assert len(training_wallets_df) == len(training_end_df), \"Some wallets are missing a record as of the training_period_end\"\n",
    "\n",
    "    def test_records_at_modeling_period_start(self):\n",
    "        modeling_profits_df = self.profits_df[self.profits_df['date'] <= MODELING_PERIOD_START]\n",
    "        modeling_wallets_df = modeling_profits_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        modeling_start_df = self.profits_df[self.profits_df['date'] == MODELING_PERIOD_START]\n",
    "        modeling_start_df = modeling_start_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        assert len(modeling_wallets_df) == len(modeling_start_df), \"Some wallets are missing a record as of the modeling_period_start\"\n",
    "\n",
    "    def test_records_at_modeling_period_end(self):\n",
    "        modeling_profits_df = self.profits_df[self.profits_df['date'] <= MODELING_PERIOD_END]\n",
    "        modeling_wallets_df = modeling_profits_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        modeling_end_df = self.profits_df[self.profits_df['date'] == MODELING_PERIOD_END]\n",
    "        modeling_end_df = modeling_end_df[['coin_id', 'wallet_address']].drop_duplicates()\n",
    "\n",
    "        assert len(modeling_wallets_df) == len(modeling_end_df), \"Some wallets are missing a record as of the modeling_period_end\"\n",
    "\n",
    "    def test_no_records_before_training_period_start(self):\n",
    "        early_records = self.profits_df[self.profits_df['date'] < TRAINING_PERIOD_START]\n",
    "        assert len(early_records) == 0, f\"Found {len(early_records)} records prior to training_period_start\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = NotebookTestProfitsDataQuality(profits_df_merged)\n",
    "# tester.run_all_tests()  # Run all tests\n",
    "tester.test_profits_consistency()  # Run a specific test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profits_df.sample(frac=0.001).copy(deep=True)\n",
    "df['profits_change_check'] = df.groupby(['coin_id', 'wallet_address'], observed=True)['profits_cumulative'].diff()\n",
    "df['diff'] = df['profits_change'] - df['profits_change_check']\n",
    "\n",
    "# threshold = 0.02\n",
    "# inconsistent_profits = df[\n",
    "#     (~df['profits_change_check'].isna()) &\n",
    "#     (df['diff'].abs() > threshold)\n",
    "# ]\n",
    "\n",
    "# if len(inconsistent_profits) > 0:\n",
    "#     logger.warning(f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes.\")\n",
    "#     logger.warning(\"Sample of inconsistent profits:\")\n",
    "#     logger.warning(inconsistent_profits.head().to_string())\n",
    "\n",
    "# assert len(inconsistent_profits) == 0, f\"Found {len(inconsistent_profits)} records with potentially inconsistent profit changes. Check logs for details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.02\n",
    "\n",
    "inconsistent_profits = df[\n",
    "    (~df['profits_change_check'].isna()) &\n",
    "    (df['diff'].abs() > threshold)\n",
    "]\n",
    "inconsistent_profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_address = 12147564\n",
    "coin_id = '4b833d21-0490-4256-8fd6-0e384f7d3b16'\n",
    "\n",
    "test_df = profits_df[(profits_df['coin_id']==coin_id) & (profits_df['wallet_address']==wallet_address)]\n",
    "test_df = test_df.sort_values('date',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 3.3 Build the configured model input data (train/test data)\n",
    "X_train, X_test, y_train, y_test = i.build_configured_model_input(profits_df, market_data_df, config, metrics_config, modeling_config)\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate and save the model's performance on the test set to a CSV\n",
    "evals = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config['time_series']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_profits_df = profits_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "target_date = config['training_data']['training_period_end']\n",
    "new_rows_df = td.impute_profits_df_rows(profits_df, prices_df, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df = full_profits_df.copy(deep=True)\n",
    "profits_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "dates_to_impute = [\n",
    "    config['training_data']['training_period_end'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end'],\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def impute_profits_for_multiple_dates(profits_df, prices_df, dates, n_threads):\n",
    "    \"\"\"\n",
    "    Wrapper function to impute profits for multiple dates using multithreaded processing.\n",
    "\n",
    "    Args:\n",
    "        profits_df (pd.DataFrame): DataFrame containing dated profits data for coin-wallet pairs\n",
    "        prices_df (pd.DataFrame): DataFrame containing price information\n",
    "        dates (list): List of dates (str or datetime) for which to impute rows\n",
    "        n_threads (int): The number of threads to use for imputation\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated profits_df with imputed rows for all specified dates\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting profits_df imputation for %s dates...\", len(dates))\n",
    "\n",
    "    new_rows_list = []\n",
    "\n",
    "    for date in dates:\n",
    "        new_rows_df = td.multithreaded_impute_profits_rows(profits_df, prices_df, date, n_threads)\n",
    "        new_rows_list.append(new_rows_df)\n",
    "\n",
    "    # Concatenate all new rows at once\n",
    "    all_new_rows = pd.concat(new_rows_list, ignore_index=True)\n",
    "\n",
    "    # Append all new rows to profits_df\n",
    "    updated_profits_df = pd.concat([profits_df, all_new_rows], ignore_index=True)\n",
    "\n",
    "    logger.info(\"Completed new row generation after %.2f seconds. Total rows after imputation: %s\",\n",
    "                time.time() - start_time,\n",
    "                updated_profits_df.shape[0])\n",
    "\n",
    "    return updated_profits_df\n",
    "\n",
    "update_profits_df = impute_profits_for_multiple_dates(profits_df, prices_df, dates_to_impute, n_threads=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "optimal_thread_count = multiprocessing.cpu_count()\n",
    "optimal_thread_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient columns\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "coin_id_mapping = dict(enumerate(profits_df['coin_id'].cat.categories))\n",
    "profits_df['coin_id'] = profits_df['coin_id'].cat.codes.astype('int16')\n",
    "\n",
    "# Convert date column to store the difference in days relative to target_date\n",
    "profits_df['date'] = (profits_df['date'] - target_date).dt.days.astype('int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vars\n",
    "# target_date = '2024-08-31'\n",
    "# # new_rows_df = generate_new_row(profits_df, prices_df, target_date)\n",
    "\n",
    "# target_date = pd.to_datetime(target_date)\n",
    "\n",
    "# # # Create efficient indexes\n",
    "# # profits_df = profits_df.set_index(['coin_id', 'wallet_address', 'date']).copy(deep=True)\n",
    "# # prices_df = prices_df.set_index(['coin_id', 'date']).copy(deep=True)\n",
    "\n",
    "# # # Identify pairs needing new rows\n",
    "# # logger.debug('Identifying pairs that need a row for %s...', target_date)\n",
    "# # all_pairs = profits_df.index.droplevel('date').unique()\n",
    "# # existing_pairs = profits_df.loc(axis=0)[:, :, target_date].index.droplevel('date')\n",
    "# # pairs_needing_rows = all_pairs.difference(existing_pairs)\n",
    "# # logger.debug('Identified %s pairs that will need rows imputed.', len(pairs_needing_rows))\n",
    "\n",
    "\n",
    "# new_rows = []\n",
    "\n",
    "# logger.debug('Imputing new rows...')\n",
    "# for coin_id, wallet_address in pairs_needing_rows:\n",
    "#     # Get most recent record\n",
    "#     recent_record = profits_df.loc[coin_id, wallet_address].loc[:target_date].iloc[-1]\n",
    "\n",
    "#     # Get prices\n",
    "#     price_previous = prices_df.loc[(coin_id, recent_record.name), 'price']\n",
    "#     price_current = prices_df.loc[(coin_id, target_date), 'price']\n",
    "\n",
    "#     # Calculate new values\n",
    "#     price_ratio = price_current / price_previous\n",
    "#     new_usd_balance = recent_record['usd_balance'] * price_ratio\n",
    "#     profits_change = new_usd_balance - recent_record['usd_balance']\n",
    "#     profits_cumulative = recent_record['profits_cumulative'] + profits_change\n",
    "\n",
    "#     new_row = {\n",
    "#         'coin_id': coin_id,\n",
    "#         'wallet_address': wallet_address,\n",
    "#         'date': target_date,\n",
    "#         'profits_change': profits_change,\n",
    "#         'profits_cumulative': profits_cumulative,\n",
    "#         'usd_balance': new_usd_balance,\n",
    "#         'usd_net_transfers': 0,\n",
    "#         'usd_inflows': 0,\n",
    "#         'usd_inflows_cumulative': recent_record['usd_inflows_cumulative'],\n",
    "#         'total_return': profits_cumulative / max(recent_record['usd_inflows_cumulative'], 0.01)\n",
    "#     }\n",
    "\n",
    "#     new_rows.append(new_row)\n",
    "\n",
    "# new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# logger.debug('Generated new_rows_df with shape %s.', new_rows_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent data for pairs needing rows\n",
    "most_recent_data = profits_df.loc[pairs_needing_rows]\n",
    "most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "\n",
    "# Reset index of prices_df for the merge operation\n",
    "prices_df_reset = prices_df.reset_index()\n",
    "\n",
    "# Perform asof merge to get the most recent price before or on the date of each record\n",
    "merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "                            prices_df_reset.sort_values('date'),\n",
    "                            on='date',\n",
    "                            by='coin_id',\n",
    "                            direction='backward')\n",
    "\n",
    "# Now get the price at the target date\n",
    "target_prices = prices_df.loc(axis=0)[:, target_date].reset_index()\n",
    "target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# Merge the target prices\n",
    "merged_data = pd.merge(merged_data, target_prices[['coin_id', 'target_price']], on='coin_id', how='left')\n",
    "\n",
    "# Calculate price ratio\n",
    "merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n",
    "\n",
    "logger.debug('Merged data shape: %s', merged_data.shape)\n",
    "logger.debug('Merged data columns: %s', merged_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent row for each pair needing a new row\n",
    "most_recent_data = profits_df.loc[profits_df.index.isin(pairs_needing_rows, level=['coin_id', 'wallet_address'])]\n",
    "# most_recent_data = most_recent_data.groupby(level=['coin_id', 'wallet_address']).last().reset_index()\n",
    "\n",
    "# # Ensure the date column is properly formatted\n",
    "# most_recent_data['date'] = pd.to_datetime(most_recent_data['date'])\n",
    "# prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "\n",
    "# # Perform asof merge to get the most recent price before or on the date of each record\n",
    "# merged_data = pd.merge_asof(most_recent_data.sort_values('date'),\n",
    "#                             prices_df[['date', 'coin_id', 'price']].sort_values('date'),\n",
    "#                             on='date',\n",
    "#                             by='coin_id',\n",
    "#                             direction='backward')\n",
    "\n",
    "# # Now get the price at the target date\n",
    "# target_prices = prices_df[prices_df['date'] == target_date][['coin_id', 'price']]\n",
    "# target_prices = target_prices.rename(columns={'price': 'target_price'})\n",
    "\n",
    "# # Merge the target prices\n",
    "# merged_data = pd.merge(merged_data, target_prices, on='coin_id', how='left')\n",
    "\n",
    "# # Calculate price ratio\n",
    "# merged_data['price_ratio'] = merged_data['target_price'] / merged_data['price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
