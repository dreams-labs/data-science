{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "CONFIG = config\n",
    "METRICS_CONFIG = metrics_config\n",
    "MODELING_CONFIG = modeling_config\n",
    "EXPERIMENTS_CONFIG = experiments_config\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']\n",
    "modeling_folder = MODELING_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve profits data if necessary\n",
    "if 'profits_df' not in globals():\n",
    "    profits_df = None\n",
    "profits_df = i.rebuild_profits_df_if_necessary(\n",
    "                config,\n",
    "                modeling_folder,\n",
    "                prices_df,\n",
    "                profits_df)\n",
    "\n",
    "\n",
    "# filter market_data rows without transfers if configured to do so\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "    prices_df = market_data_df[['coin_id','date','price']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, performance_df = i.build_configured_model_input(\n",
    "                                    profits_df,\n",
    "                                    market_data_df,\n",
    "                                    config,\n",
    "                                    metrics_config,\n",
    "                                    modeling_config)\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_folder,\n",
    "                    modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate and save the model's performance on the test set to a CSV\n",
    "metrics_df = m.evaluate_model(model, X_test, y_test, model_id, modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "\n",
    "# Construct the performance metrics folder path\n",
    "evaluation_folder = os.path.join(modeling_folder, \"outputs\", \"performance_metrics\")\n",
    "predictions_folder = os.path.join(modeling_folder, \"outputs\", \"predictions\")\n",
    "\n",
    "# Ensure the evaluation and predictions folders exist\n",
    "if not os.path.exists(evaluation_folder):\n",
    "    raise FileNotFoundError(f\"The evaluation folder '{evaluation_folder}' does not exist.\")\n",
    "if not os.path.exists(predictions_folder):\n",
    "    raise FileNotFoundError(f\"The predictions folder '{predictions_folder}' does not exist.\")\n",
    "\n",
    "# Predict the probabilities and the labels\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to CSV with 'coin_id' as the index\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"y_pred_prob\": y_pred_prob,\n",
    "    \"y_pred\": y_pred\n",
    "}, index=X_test.index)  # Use the index which includes 'coin_id'\n",
    "# predictions_filename = os.path.join(predictions_folder, f\"predictions_{model_id}.csv\")\n",
    "# predictions_df.to_csv(predictions_filename, index=True)\n",
    "\n",
    "# # Calculate metrics\n",
    "# metrics_dict = {\n",
    "#     \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "#     \"precision\": precision_score(y_test, y_pred),\n",
    "#     \"recall\": recall_score(y_test, y_pred),\n",
    "#     \"f1_score\": f1_score(y_test, y_pred),\n",
    "#     \"roc_auc\": roc_auc_score(y_test, y_pred_prob),\n",
    "#     \"log_loss\": log_loss(y_test, y_pred_prob),\n",
    "#     \"confusion_matrix\": confusion_matrix(y_test, y_pred).tolist()  # stored as list\n",
    "# }\n",
    "\n",
    "# # Save metrics to a CSV\n",
    "# metrics_df = pd.DataFrame([metrics_dict])\n",
    "# metrics_filename = os.path.join(evaluation_folder, f\"metrics_{model_id}.csv\")\n",
    "# metrics_df.to_csv(metrics_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.set_index('coin_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = performance_df.join(predictions_df, on='coin_id', how='inner')\n",
    "predictions = merged_df['y_pred_prob']\n",
    "performances = merged_df['performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_running_profitability_score(predictions, performances):\n",
    "    \"\"\"\n",
    "    Calculates the running profitability score for the entire series.\n",
    "\n",
    "    Args:\n",
    "    - predictions (numpy.array or pandas.Series): The model's predictions (probabilities or values).\n",
    "    - performances (numpy.array or pandas.Series): The actual performance values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Two numpy arrays - (x_values, y_values)\n",
    "        x_values: Percentage of total picks (0 to 1)\n",
    "        y_values: Running profitability scores\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If predictions and performances have different lengths.\n",
    "    \"\"\"\n",
    "    if len(predictions) != len(performances):\n",
    "        raise ValueError(\"Predictions and performances must have the same length\")\n",
    "\n",
    "    # Create a DataFrame with predictions and performances\n",
    "    df = pd.DataFrame({'predictions': predictions, 'performances': performances})\n",
    "\n",
    "    # Sort by predictions in descending order\n",
    "    df_sorted = df.sort_values('predictions', ascending=False)\n",
    "\n",
    "    total_picks = len(df_sorted)\n",
    "    cumulative_model_returns = np.cumsum(df_sorted['performances'])\n",
    "\n",
    "    # Calculate best possible returns for each number of picks\n",
    "    best_possible_returns = np.sort(performances)[::-1]  # Sort performances in descending order\n",
    "    cumulative_best_returns = np.cumsum(best_possible_returns)\n",
    "\n",
    "    # Calculate running profitability scores\n",
    "    running_scores = np.divide(\n",
    "        cumulative_model_returns,\n",
    "        cumulative_best_returns,\n",
    "        out=np.zeros_like(cumulative_model_returns),\n",
    "        where=cumulative_best_returns != 0\n",
    "    )\n",
    "\n",
    "    # Create x-values (percentage of total picks)\n",
    "    x_values = np.arange(1, total_picks + 1) / total_picks\n",
    "\n",
    "    return x_values, running_scores\n",
    "\n",
    "\n",
    "x_values,running_scores = calculate_running_profitability_score(predictions, performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and performances\n",
    "df = pd.DataFrame({'predictions': predictions, 'performances': performances})\n",
    "\n",
    "# Sort by predictions in descending order\n",
    "df_sorted = df.sort_values('predictions', ascending=False)\n",
    "\n",
    "# cumulative_model_returns = np.cumsum(df_sorted['performances'])\n",
    "\n",
    "# # Calculate best possible returns for each number of picks\n",
    "# best_possible_returns = np.sort(performances)[::-1]  # Sort performances in descending order\n",
    "# cumulative_best_returns = np.cumsum(best_possible_returns)\n",
    "\n",
    "# # Calculate running profitability scores\n",
    "# running_profitability_scores = np.divide(\n",
    "#     cumulative_model_returns,\n",
    "#     cumulative_best_returns,\n",
    "#     out=np.zeros_like(cumulative_model_returns),\n",
    "#     where=cumulative_best_returns != 0\n",
    "# )\n",
    "\n",
    "# return running_profitability_scoresx_values\n",
    "\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_scoresx_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_scores.plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def normal_input_data():\n",
    "    \"\"\"\n",
    "    Fixture to provide normal input data for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Containing predictions and performances as numpy arrays.\n",
    "    \"\"\"\n",
    "    predictions = np.array([0.6, 0.9, 0.2, 0.7, 0.3, 0.5, 0.1, 0.8, 0.4])\n",
    "    performances = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0])\n",
    "    return predictions, performances\n",
    "normal_input_data=normal_input_data()\n",
    "# @pytest.mark.unit\n",
    "# def test_calculate_running_profitability_score_normal_case(normal_input_data):\n",
    "\"\"\"\n",
    "Test the calculate_running_profitability_score function with normal input.\n",
    "\n",
    "This test verifies that the function correctly calculates running profitability\n",
    "scores for a typical set of predictions and performances where predictions\n",
    "are good but not perfect.\n",
    "\"\"\"\n",
    "predictions, performances = normal_input_data\n",
    "\n",
    "x_values, y_values = m.calculate_running_profitability_score(predictions, performances)\n",
    "\n",
    "expected_x = np.array([0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556,\n",
    "                        0.66666667, 0.77777778, 0.88888889, 1.        ])\n",
    "expected_y = np.array([0.        , 0.5       , 0.66666667, 0.75      , 0.6       ,\n",
    "                        0.66666667, 0.71428571, 0.75      , 0.66666667])\n",
    "\n",
    "assert np.allclose(x_values, expected_x, atol=1e-4)\n",
    "assert np.allclose(y_values, expected_y, atol=1e-4)\n",
    "\n",
    "# Additional assertions to check specific properties\n",
    "assert y_values[0] == 0.0  # First value should be 0.0 in this case\n",
    "assert np.all(y_values <= 1.0)  # All values should be <= 1.0\n",
    "assert np.all(y_values >= 0.0)  # All values should be >= 0.0\n",
    "assert np.all(np.diff(x_values) > 0)  # x_values should be strictly increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_order=0\n",
    "y_valuescorrect_order = 0\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(i+1, len(predictions)):\n",
    "        if (predictions[i] > predictions[j] and performances[i] >= performances[j]) or \\\n",
    "           (predictions[i] < predictions[j] and performances[i] <= performances[j]):\n",
    "            correct_order += 1\n",
    "\n",
    "total_comparisons = len(predictions) * (len(predictions) - 1) // 2\n",
    "correctness_percentage = correct_order / total_comparisons * 100\n",
    "\n",
    "print(f\"Percentage of correct orderings: {correctness_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([0.6, 0.9, 0.2, 0.7, 0.3, 0.5, 0.1, 0.8, 0.4])\n",
    "performances = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0])\n",
    "# Create a DataFrame with predictions and performances\n",
    "df = pd.DataFrame({'predictions': predictions, 'performances': performances})\n",
    "df.sort_values('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_mooncrater_targets(performance_df, modeling_config):\n",
    "    \"\"\"\n",
    "    Calculates 'is_moon' and 'is_crater' target variables based on performance.\n",
    "\n",
    "    Parameters:\n",
    "    - performance_df: DataFrame with columns 'coin_id' and 'performance'.\n",
    "    - modeling_config: Configuration for modeling with target variable thresholds.\n",
    "\n",
    "    Returns:\n",
    "    - target_variables_df: DataFrame with columns 'coin_id', 'is_moon', and 'is_crater'.\n",
    "    \"\"\"\n",
    "    moon_threshold = modeling_config['target_variables']['moon_threshold']\n",
    "    crater_threshold = modeling_config['target_variables']['crater_threshold']\n",
    "    moon_minimum_percent = modeling_config['target_variables']['moon_minimum_percent']\n",
    "    crater_minimum_percent = modeling_config['target_variables']['crater_minimum_percent']\n",
    "\n",
    "    target_variables_df = performance_df.copy()\n",
    "    target_variables_df['is_moon'] = (target_variables_df['performance'] >= moon_threshold).astype(int)\n",
    "    target_variables_df['is_crater'] = (target_variables_df['performance'] <= crater_threshold).astype(int)\n",
    "\n",
    "    total_coins = len(target_variables_df)\n",
    "    moons = target_variables_df['is_moon'].sum()\n",
    "    craters = target_variables_df['is_crater'].sum()\n",
    "\n",
    "    # Ensure minimum percentage for moons and craters\n",
    "    if moons / total_coins < moon_minimum_percent:\n",
    "        additional_moons_needed = int(total_coins * moon_minimum_percent) - moons\n",
    "        moon_candidates = target_variables_df[target_variables_df['is_moon'] == 0].nlargest(additional_moons_needed, 'performance')\n",
    "        target_variables_df.loc[moon_candidates.index, 'is_moon'] = 1\n",
    "\n",
    "    if craters / total_coins < crater_minimum_percent:\n",
    "        additional_craters_needed = int(total_coins * crater_minimum_percent) - craters\n",
    "        crater_candidates = target_variables_df[target_variables_df['is_crater'] == 0].nsmallest(additional_craters_needed, 'performance')\n",
    "        target_variables_df.loc[crater_candidates.index, 'is_crater'] = 1\n",
    "\n",
    "    return target_variables_df[['coin_id', 'is_moon', 'is_crater']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fe)\n",
    "target_variables_df, performance_df, outcomes_df = fe.create_target_variables(market_data_df, config['training_data'], modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def valid_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with valid price data for multiple coins.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 2,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01', '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5, 35000, 2500, 0.6]\n",
    "    })\n",
    "valid_prices_df=valid_prices_df()\n",
    "\n",
    "def valid_training_data_config():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample training data configuration.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'modeling_period_start': '2023-01-01',\n",
    "        'modeling_period_end': '2023-12-31'\n",
    "    }\n",
    "valid_training_data_config=valid_training_data_config()\n",
    "\n",
    "def no_change_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with no price change for some coins.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 2,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01', '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5, 30000, 2500, 0.5]\n",
    "    })\n",
    "no_change_prices_df=no_change_prices_df()\n",
    "\n",
    "def negative_performance_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with negative performance for some coins.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 2,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01', '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5, 25000, 2500, 0.4]\n",
    "    })\n",
    "negative_performance_prices_df=negative_performance_prices_df()\n",
    "\n",
    "def multiple_datapoints_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with multiple data points between start and end dates.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 4,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01',\n",
    "                 '2023-06-15', '2023-06-15', '2023-06-15',\n",
    "                 '2023-09-30', '2023-09-30', '2023-09-30',\n",
    "                 '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5,\n",
    "                  32000, 2200, 0.55,\n",
    "                  34000, 2400, 0.58,\n",
    "                  35000, 2500, 0.6]\n",
    "    })\n",
    "multiple_datapoints_prices_df=multiple_datapoints_prices_df()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test prepare_and_compute_performance function with multiple data points between start and end dates.\n",
    "\n",
    "This test ensures that the function correctly calculates performance using only start and end dates,\n",
    "ignoring intermediate data points.\n",
    "\"\"\"\n",
    "performance_df, outcomes_df = fe.prepare_and_compute_performance(multiple_datapoints_prices_df, valid_training_data_config)\n",
    "\n",
    "expected_performance = pd.DataFrame({\n",
    "    'coin_id': ['BTC', 'ETH', 'XRP'],\n",
    "    'performance': [0.1667, 0.25, 0.2]\n",
    "})\n",
    "\n",
    "assert (np.isclose(performance_df['performance'].values, expected_performance['performance'].values, rtol=1e-4, atol=1e-4)).all()\n",
    "\n",
    "expected_outcomes = pd.DataFrame({\n",
    "    'coin_id': ['BTC', 'ETH', 'XRP'],\n",
    "    'outcome': ['performance calculated'] * 3\n",
    "})\n",
    "\n",
    "assert np.array_equal(outcomes_df.values, expected_outcomes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.isclose(performance_df['performance'].values, expected_performance['performance'].values, rtol=1e-4, atol=1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def negative_performance_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with negative performance for some coins.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 2,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01', '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5, 25000, 2500, 0.4]\n",
    "    })\n",
    "negative_performance_prices_df=negative_performance_prices_df()\n",
    "\n",
    "def test_prepare_and_compute_performance_negative(negative_performance_prices_df, valid_training_data_config):\n",
    "    \"\"\"\n",
    "    Test prepare_and_compute_performance function with negative performance for some coins.\n",
    "\n",
    "    This test ensures that the function correctly calculates negative performance values\n",
    "    for coins with price decreases and correct performance for others.\n",
    "    \"\"\"\n",
    "    performance_df, outcomes_df = fe.prepare_and_compute_performance(negative_performance_prices_df, valid_training_data_config)\n",
    "\n",
    "    expected_performance = pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'],\n",
    "        'performance': [-0.1667, 0.25, -0.2]\n",
    "    })\n",
    "\n",
    "    assert np.array_equal(performance_df.values, expected_performance.values)\n",
    "\n",
    "    for actual, expected in zip(performance_df['performance'], expected_performance['performance']):\n",
    "        assert actual == pytest.approx(expected, abs=1e-4)\n",
    "\n",
    "@pytest.fixture\n",
    "def multiple_datapoints_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with multiple data points between start and end dates.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 4,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01',\n",
    "                 '2023-06-15', '2023-06-15', '2023-06-15',\n",
    "                 '2023-09-30', '2023-09-30', '2023-09-30',\n",
    "                 '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5,\n",
    "                  32000, 2200, 0.55,\n",
    "                  34000, 2400, 0.58,\n",
    "                  35000, 2500, 0.6]\n",
    "    })\n",
    "\n",
    "@pytest.mark.unit\n",
    "def test_prepare_and_compute_performance_multiple_datapoints(multiple_datapoints_prices_df, valid_training_data_config):\n",
    "    \"\"\"\n",
    "    Test prepare_and_compute_performance function with multiple data points between start and end dates.\n",
    "\n",
    "    This test ensures that the function correctly calculates performance using only start and end dates,\n",
    "    ignoring intermediate data points.\n",
    "    \"\"\"\n",
    "    performance_df, outcomes_df = fe.prepare_and_compute_performance(multiple_datapoints_prices_df, valid_training_data_config)\n",
    "\n",
    "    expected_performance = pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'],\n",
    "        'performance': [0.1667, 0.25, 0.2]\n",
    "    })\n",
    "\n",
    "    assert np.array_equal(performance_df.values, expected_performance.values)\n",
    "\n",
    "    for actual, expected in zip(performance_df['performance'], expected_performance['performance']):\n",
    "        assert actual == pytest.approx(expected, abs=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prepare_and_compute_performance_negative(negative_performance_prices_df, valid_training_data_config):\n",
    "    \"\"\"\n",
    "    Test prepare_and_compute_performance function with negative performance for some coins.\n",
    "\n",
    "    This test ensures that the function correctly calculates negative performance values\n",
    "    for coins with price decreases and correct performance for others.\n",
    "    \"\"\"\n",
    "    performance_df, outcomes_df = fe.prepare_and_compute_performance(negative_performance_prices_df, valid_training_data_config)\n",
    "\n",
    "    expected_performance = pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'],\n",
    "        'performance': [-0.1667, 0.25, -0.2]\n",
    "    })\n",
    "\n",
    "    assert np.array_equal(performance_df.values, expected_performance.values)\n",
    "\n",
    "    for actual, expected in zip(performance_df['performance'], expected_performance['performance']):\n",
    "        assert actual == pytest.approx(expected, abs=1e-4)\n",
    "\n",
    "@pytest.fixture\n",
    "def multiple_datapoints_prices_df():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame with multiple data points between start and end dates.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'] * 4,\n",
    "        'date': ['2023-01-01', '2023-01-01', '2023-01-01',\n",
    "                 '2023-06-15', '2023-06-15', '2023-06-15',\n",
    "                 '2023-09-30', '2023-09-30', '2023-09-30',\n",
    "                 '2023-12-31', '2023-12-31', '2023-12-31'],\n",
    "        'price': [30000, 2000, 0.5,\n",
    "                  32000, 2200, 0.55,\n",
    "                  34000, 2400, 0.58,\n",
    "                  35000, 2500, 0.6]\n",
    "    })\n",
    "\n",
    "@pytest.mark.unit\n",
    "def test_prepare_and_compute_performance_multiple_datapoints(multiple_datapoints_prices_df, valid_training_data_config):\n",
    "    \"\"\"\n",
    "    Test prepare_and_compute_performance function with multiple data points between start and end dates.\n",
    "\n",
    "    This test ensures that the function correctly calculates performance using only start and end dates,\n",
    "    ignoring intermediate data points.\n",
    "    \"\"\"\n",
    "    performance_df, outcomes_df = fe.prepare_and_compute_performance(multiple_datapoints_prices_df, valid_training_data_config)\n",
    "\n",
    "    expected_performance = pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'ETH', 'XRP'],\n",
    "        'performance': [0.1667, 0.25, 0.2]\n",
    "    })\n",
    "\n",
    "    assert np.array_equal(performance_df.values, expected_performance.values)\n",
    "\n",
    "    for actual, expected in zip(performance_df['performance'], expected_performance['performance']):\n",
    "        assert actual == pytest.approx(expected, abs=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
