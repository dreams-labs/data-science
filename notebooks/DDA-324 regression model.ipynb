{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "CONFIG = config\n",
    "METRICS_CONFIG = metrics_config\n",
    "MODELING_CONFIG = modeling_config\n",
    "EXPERIMENTS_CONFIG = experiments_config\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']\n",
    "modeling_folder = MODELING_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "if 'prices_df' not in globals():\n",
    "    market_data_df = td.retrieve_market_data()\n",
    "    market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "    prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve profits data if necessary\n",
    "if 'profits_df' not in globals():\n",
    "    profits_df = None\n",
    "profits_df = i.rebuild_profits_df_if_necessary(\n",
    "                config,\n",
    "                modeling_folder,\n",
    "                prices_df,\n",
    "                profits_df)\n",
    "\n",
    "# filter market_data rows without transfers if configured to do so\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "    prices_df = market_data_df[['coin_id','date','price']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_config.get('target_variable_type', 'mooncrater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, returns_test = i.build_configured_model_input(\n",
    "                                    profits_df,\n",
    "                                    market_data_df,\n",
    "                                    config,\n",
    "                                    metrics_config,\n",
    "                                    modeling_config)\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_folder,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the normal AUC on inverted numbers\n",
    "auc = m.calculate_profitability_auc(y_pred_prob,\n",
    "                            returns_df_test['returns'],\n",
    "                            modeling_config[\"evaluation\"][\"metrics\"][\"profitability_auc\"][\"top_percentage_filter\"],\n",
    "                            modeling_config[\"evaluation\"][\"winsorization_cutoff\"])\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "# calculate the normal AUC on inverted numbers\n",
    "downside_auc = m.calculate_downside_profitability_auc(y_pred_prob,\n",
    "                            returns_df_test['returns'],\n",
    "                            modeling_config[\"evaluation\"][\"metrics\"][\"profitability_auc\"][\"top_percentage_filter\"],\n",
    "                            modeling_config[\"evaluation\"][\"winsorization_cutoff\"])\n",
    "\n",
    "downside_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make negative returns the highest values\n",
    "returns_neg = returns_df_test * -1\n",
    "\n",
    "# find the inverse of model predictions\n",
    "predictions_neg = 1 - predictions\n",
    "\n",
    "# calculate the normal AUC on inverted numbers\n",
    "downside_auc = m.calculate_profitability_auc(predictions_neg,\n",
    "                            returns_neg,\n",
    "                            modeling_config[\"evaluation\"][\"metrics\"][\"profitability_auc\"][\"top_percentage_filter\"],\n",
    "                            modeling_config[\"evaluation\"][\"winsorization_cutoff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make negative returns the highest values\n",
    "returns_neg = returns_df_test * -1\n",
    "\n",
    "# find the inverse of model predictions\n",
    "predictions_neg = 1 - predictions\n",
    "\n",
    "# calculate the normal AUC on inverted numbers\n",
    "downside_auc = m.calculate_profitability_auc(predictions_neg,\n",
    "                            returns_neg,\n",
    "                            modeling_config[\"evaluation\"][\"metrics\"][\"profitability_auc\"][\"top_percentage_filter\"],\n",
    "                            modeling_config[\"evaluation\"][\"winsorization_cutoff\"])\n",
    "\n",
    "downside_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df_test = returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, log_loss\n",
    "\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "\n",
    "# Construct the performance metrics folder path\n",
    "evaluation_folder = os.path.join(modeling_folder, \"outputs\", \"performance_metrics\")\n",
    "predictions_folder = os.path.join(modeling_folder, \"outputs\", \"predictions\")\n",
    "\n",
    "# Ensure the evaluation and predictions folders exist\n",
    "if not os.path.exists(evaluation_folder):\n",
    "    raise FileNotFoundError(f\"The evaluation folder '{evaluation_folder}' does not exist.\")\n",
    "if not os.path.exists(predictions_folder):\n",
    "    raise FileNotFoundError(f\"The predictions folder '{predictions_folder}' does not exist.\")\n",
    "\n",
    "# Predict the probabilities and the labels\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save predictions to CSV with 'coin_id' as the index\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"y_pred_prob\": y_pred_prob,\n",
    "    \"y_pred\": y_pred\n",
    "}, index=X_test.index)  # Use the index which includes 'coin_id'\n",
    "predictions_filename = os.path.join(predictions_folder, f\"predictions_{model_id}.csv\")\n",
    "predictions_df.to_csv(predictions_filename, index=True)\n",
    "\n",
    "# Calculate requested metrics\n",
    "metrics_request = modeling_config['evaluation']['metrics']\n",
    "metrics_dict = {}\n",
    "\n",
    "if \"accuracy\" in metrics_request:\n",
    "    metrics_dict[\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "if \"precision\" in metrics_request:\n",
    "    metrics_dict[\"precision\"] = precision_score(y_test, y_pred)\n",
    "if \"recall\" in metrics_request:\n",
    "    metrics_dict[\"recall\"] = recall_score(y_test, y_pred)\n",
    "if \"f1_score\" in metrics_request:\n",
    "    metrics_dict[\"f1_score\"] = f1_score(y_test, y_pred)\n",
    "if \"roc_auc\" in metrics_request:\n",
    "    metrics_dict[\"roc_auc\"] = roc_auc_score(y_test, y_pred_prob)\n",
    "if \"log_loss\" in metrics_request:\n",
    "    metrics_dict[\"log_loss\"] = log_loss(y_test, y_pred_prob)\n",
    "if \"confusion_matrix\" in metrics_request:\n",
    "    metrics_dict[\"confusion_matrix\"] = confusion_matrix(y_test, y_pred).tolist()  # stored as list\n",
    "if \"profitability_auc\" in metrics_request:\n",
    "    metrics_dict[\"profitability_auc\"] = m.calculate_profitability_auc(\n",
    "                                                y_pred_prob,\n",
    "                                                returns_df_test['returns'],\n",
    "                                                metrics_request[\"profitability_auc\"][\"top_percentage_filter\"],\n",
    "                                                modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'accuracy' in modeling_config['evaluation']['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics_dict = {}\n",
    "metrics_request = modeling_config['evaluation']['metrics']\n",
    "\n",
    "\n",
    "if \"accuracy\" in metrics_request:\n",
    "\n",
    "    metrics_dict['accuracy'] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = returns_df.join(predictions_df, on='coin_id', how='inner')\n",
    "predictions = merged_df['y_pred_prob']\n",
    "performances = merged_df['performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_profitability_scores = m.calculate_running_profitability_score(predictions, performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_score(running_scores, top_n_percentile=1.0):\n",
    "    \"\"\"\n",
    "    Summarizes the running profitability scores into a single score using\n",
    "    trapezoidal integration (similar to AUC), focusing on the top_n_percentile.\n",
    "\n",
    "    Args:\n",
    "    - running_scores (numpy.array or list): The running profitability scores.\n",
    "    - top_n_percentile (float, optional): The top percentile of scores to consider (between 0 and 1).\n",
    "                                           Defaults to 1.0 (use 100% of the data).\n",
    "\n",
    "    Returns:\n",
    "    - float: The summarized score over the specified percentile.\n",
    "    \"\"\"\n",
    "    # Ensure the top_n_percentile is between 0 and 1\n",
    "    if not 0 < top_n_percentile <= 1:\n",
    "        raise ValueError(\"top_n_percentile must be between 0 and 1\")\n",
    "\n",
    "    total_picks = len(running_scores)\n",
    "    top_n_count = int(total_picks * top_n_percentile)\n",
    "\n",
    "    # Limit the x_values and running_scores to the top_n_percentile\n",
    "    x_values = np.arange(1, total_picks + 1) / total_picks\n",
    "    x_values = x_values[:top_n_count]\n",
    "    running_scores = running_scores[:top_n_count]\n",
    "\n",
    "    # Use trapezoidal integration to calculate the area under the curve for the top_n_percentile\n",
    "    auc_score = np.trapezoid(running_scores, x=x_values)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(10):\n",
    "    percentile = (x+1)/10\n",
    "    auc_score = calculate_summary_score(running_profitability_scores,percentile)\n",
    "    print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import integrate\n",
    "\n",
    "def calculate_profitability_auc(predictions, performances, top_percentage_filter=1.0):\n",
    "    \"\"\"\n",
    "    Calculates the Profitability AUC (Area Under the Curve) metric for the top percentage of predictions.\n",
    "\n",
    "    Args:\n",
    "    - predictions (numpy.array or pandas.Series): The model's predictions (probabilities or values).\n",
    "    - performances (numpy.array or pandas.Series): The actual performance values.\n",
    "    - top_percentage_filter (float): The top percentage of predictions to consider, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    - profitability_auc (float): The Profitability AUC score for the filtered data, ranging from 0 to 1.\n",
    "    \"\"\"\n",
    "    if not 0 < top_percentage_filter <= 1:\n",
    "        raise ValueError(\"top_percentage_filter must be between 0 and 1\")\n",
    "\n",
    "    # Sort predictions and performances\n",
    "    df = pd.DataFrame({'predictions': predictions, 'performances': performances})\n",
    "    df_sorted = df.sort_values('predictions', ascending=False)\n",
    "\n",
    "    # Filter top percentage\n",
    "    n_top = int(len(predictions) * top_percentage_filter)\n",
    "    if n_top < 2:\n",
    "        raise ValueError(\"Filtered dataset is too small for meaningful calculation\")\n",
    "\n",
    "    df_filtered = df_sorted.head(n_top)\n",
    "\n",
    "    # Calculate running profitability scores for filtered data\n",
    "    running_scores = calculate_running_profitability_score(\n",
    "        df_filtered['predictions'], df_filtered['performances'])\n",
    "\n",
    "    # Create x-axis values (fraction of filtered predictions)\n",
    "    x = np.linspace(0, 1, len(running_scores))\n",
    "\n",
    "    # Calculate the area under the curve using NumPy's trapezoidal rule\n",
    "    auc = np.trapz(running_scores, x)\n",
    "\n",
    "    return auc  # Return AUC without normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "\n",
    "predictions = np.array([0.55, 0.07, 0.14, 0.02, 0.07, 0.64, 0.04, 0.00, 0.02, 0.39])\n",
    "performances = np.array([0.46, -0.1, -0.09, -0.09, -0.01, 0.57, -0.1, -0.01, -0.02, 2.62])\n",
    "top_percentage_filter = 0.2\n",
    "\n",
    "# Step 1: Sort predictions and performances\n",
    "sorted_indices = np.argsort(predictions)[::-1]\n",
    "sorted_predictions = predictions[sorted_indices]\n",
    "sorted_performances = performances[sorted_indices]\n",
    "\n",
    "# Step 2: Determine the number of top predictions to consider\n",
    "n_top = int(len(predictions) * top_percentage_filter)\n",
    "assert n_top == 2\n",
    "\n",
    "# Step 3: Filter the top predictions and performances\n",
    "top_predictions = np.round(sorted_predictions[:n_top], 3)\n",
    "top_performances = np.round(sorted_performances[:n_top], 3)\n",
    "np.testing.assert_array_equal(top_predictions, np.array([0.640, 0.550]))\n",
    "np.testing.assert_array_equal(top_performances, np.array([0.570, 0.460]))\n",
    "\n",
    "# Step 4: Calculate running profitability scores\n",
    "cumulative_model_returns = np.round(np.cumsum(top_performances), 3)\n",
    "best_possible_returns = np.round(np.sort(performances)[::-1][:n_top], 3)\n",
    "cumulative_best_returns = np.round(np.cumsum(best_possible_returns), 3)\n",
    "running_profitability_scores = np.round(cumulative_model_returns / cumulative_best_returns, 3)\n",
    "np.testing.assert_allclose(cumulative_model_returns, np.array([0.570, 1.030]))\n",
    "np.testing.assert_allclose(best_possible_returns, np.array([2.620, 0.570]))\n",
    "np.testing.assert_allclose(cumulative_best_returns, np.array([2.620, 3.190]))\n",
    "np.testing.assert_allclose(running_profitability_scores, np.array([0.218, 0.323]))\n",
    "\n",
    "# Step 5: Calculate the area under the curve\n",
    "x = np.linspace(0, 1, n_top)\n",
    "expected_auc = np.trapezoid(running_profitability_scores, x)\n",
    "np.testing.assert_almost_equal(expected_auc, 0.27025, decimal=3)\n",
    "\n",
    "# Final step: Compare with the function output\n",
    "calculated_auc = m.calculate_profitability_auc(predictions, performances, top_percentage_filter)\n",
    "np.testing.assert_almost_equal(calculated_auc, expected_auc, decimal=3,\n",
    "                                err_msg=\"Calculated Profitability AUC doesn't match expected value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "m.calculate_running_profitability_score(predictions,returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_profitability_auc(predictions, performances, top_percentage_filter=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(performances)[::-1]  # Sort performances in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.sort_values('performances',ascending=False)['performances'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate best possible returns by sorting by performance descending\n",
    "df.sort_values('performances',ascending=False)['performances'].cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered\n",
    "\n",
    "df_sorted = df_filtered.sort_values('predictions', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['performances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and performances\n",
    "\n",
    "# Calculate the cumulative profits of the model predictions\n",
    "df_sorted = df_filtered.sort_values('predictions', ascending=False)\n",
    "cumulative_model_returns = np.cumsum(df_sorted['performances'])\n",
    "\n",
    "# Calculate best possible returns for each number of picks\n",
    "best_possible_returns = np.sort(df_filtered['performances'])[::-1]  # Sort performances in descending order\n",
    "cumulative_best_returns = np.cumsum(best_possible_returns)\n",
    "\n",
    "# # Calculate running profitability scores\n",
    "# running_profitability_scores = np.divide(\n",
    "#     cumulative_model_returns,\n",
    "#     cumulative_best_returns,\n",
    "#     out=np.zeros_like(cumulative_model_returns),\n",
    "#     where=cumulative_best_returns != 0\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_model_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_best_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and performances\n",
    "df = pd.DataFrame({'predictions': predictions, 'performances': returns})\n",
    "\n",
    "# Calculate the cumulative profits of the model predictions\n",
    "df_sorted = df.sort_values('predictions', ascending=False)\n",
    "cumulative_model_returns = np.cumsum(df_sorted['performances'])\n",
    "\n",
    "# Calculate best possible returns for each number of picks\n",
    "best_possible_returns = np.sort(performances)[::-1]  # Sort performances in descending order\n",
    "cumulative_best_returns = np.cumsum(best_possible_returns)\n",
    "\n",
    "# Calculate running profitability scores\n",
    "running_profitability_scores = np.divide(\n",
    "    cumulative_model_returns,\n",
    "    cumulative_best_returns,\n",
    "    out=np.zeros_like(cumulative_model_returns),\n",
    "    where=cumulative_best_returns != 0\n",
    ")\n",
    "\n",
    "\n",
    "running_profitability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered['returns']=df_filtered['performances']\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances==returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(m)\n",
    "\n",
    "predictions = np.array([0.55, 0.07, 0.14, 0.02, 0.07, 0.64, 0.04, 0.00, 0.02, 0.39])\n",
    "returns = np.array([0.46, -0.1, -0.09, -0.09, -0.01, 0.57, -0.1, -0.01, -0.02, 2.62])\n",
    "\n",
    "\n",
    "# Confirm percentage is between 0 and 1\n",
    "if not 0 < top_percentage_filter <= 1:\n",
    "    raise ValueError(\"top_percentage_filter must be between 0 and 1\")\n",
    "\n",
    "# Calculate the full range of profitability scores\n",
    "running_scores = m.calculate_running_profitability_score(predictions, returns)\n",
    "\n",
    "# Calculate how many scores to look at based on the percentage filter\n",
    "n_top = int(len(predictions) * top_percentage_filter)\n",
    "if n_top < 2:\n",
    "    raise ValueError(\"Filtered dataset is too small for meaningful calculation\")\n",
    "\n",
    "# Limit the scores for AUC calculation to the percentile input\n",
    "filtered_running_scores = running_scores[:n_top]\n",
    "\n",
    "# Create x-axis values (fraction of filtered predictions)\n",
    "x = np.linspace(0, 1, len(filtered_running_scores))\n",
    "\n",
    "# Calculate the area under the curve using NumPy's trapezoidal rule\n",
    "auc = np.trapezoid(filtered_running_scores, x)\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_scores[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([0.55, 0.07, 0.14, 0.02, 0.07, 0.64, 0.04, 0.00, 0.02, 0.39])\n",
    "returns = np.array([0.46, -0.1, -0.09, -0.09, -0.01, 0.57, -0.1, -0.01, -0.02, 2.62])\n",
    "\n",
    "calculate_running_profitability_score(predictions, returns)\n",
    "\n",
    "if not 0 < top_percentage_filter <= 1:\n",
    "    raise ValueError(\"top_percentage_filter must be between 0 and 1\")\n",
    "\n",
    "# Sort predictions and returns\n",
    "df = pd.DataFrame({'predictions': predictions, 'returns': returns})\n",
    "\n",
    "# Filter top percentage\n",
    "n_top = int(len(predictions) * top_percentage_filter)\n",
    "if n_top < 2:\n",
    "    raise ValueError(\"Filtered dataset is too small for meaningful calculation\")\n",
    "\n",
    "\n",
    "# Sort by model predictions and take the n_top returns\n",
    "model_returns = (df.sort_values('predictions', ascending=False) # sort by model predictions\n",
    "                      ['returns'].head(2))\n",
    "\n",
    "# Sort by actual returns and take the n_top returns\n",
    "actual_returns = (df.sort_values('returns', ascending=False) # sort by model predictions\n",
    "                      ['returns'].head(2))\n",
    "\n",
    "# Calculate running profitability scores for filtered data\n",
    "running_scores = m.calculate_running_profitability_score(model_returns,actual_returns)\n",
    "\n",
    "# Create x-axis values (fraction of filtered predictions)\n",
    "x = np.linspace(0, 1, len(running_scores))\n",
    "\n",
    "# Calculate the area under the curve using NumPy's trapezoidal rule\n",
    "auc = np.trapezoid(running_scores, x)\n",
    "\n",
    "running_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_profitability_scores.plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def normal_input_data():\n",
    "    \"\"\"\n",
    "    Fixture to provide normal input data for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Containing predictions and performances as numpy arrays.\n",
    "    \"\"\"\n",
    "    predictions = np.array([0.6, 0.9, 0.2, 0.7, 0.3, 0.5, 0.1, 0.8, 0.4])\n",
    "    performances = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0])\n",
    "    return predictions, performances\n",
    "normal_input_data=normal_input_data()\n",
    "# @pytest.mark.unit\n",
    "# def test_calculate_running_profitability_score_normal_case(normal_input_data):\n",
    "\"\"\"\n",
    "Test the calculate_running_profitability_score function with normal input.\n",
    "\n",
    "This test verifies that the function correctly calculates running profitability\n",
    "scores for a typical set of predictions and performances where predictions\n",
    "are good but not perfect.\n",
    "\"\"\"\n",
    "predictions, performances = normal_input_data\n",
    "\n",
    "x_values, y_values = m.calculate_running_profitability_score(predictions, performances)\n",
    "\n",
    "expected_x = np.array([0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556,\n",
    "                        0.66666667, 0.77777778, 0.88888889, 1.        ])\n",
    "expected_y = np.array([0.        , 0.5       , 0.66666667, 0.75      , 0.6       ,\n",
    "                        0.66666667, 0.71428571, 0.75      , 0.66666667])\n",
    "\n",
    "assert np.allclose(x_values, expected_x, atol=1e-4)\n",
    "assert np.allclose(y_values, expected_y, atol=1e-4)\n",
    "\n",
    "# Additional assertions to check specific properties\n",
    "assert y_values[0] == 0.0  # First value should be 0.0 in this case\n",
    "assert np.all(y_values <= 1.0)  # All values should be <= 1.0\n",
    "assert np.all(y_values >= 0.0)  # All values should be >= 0.0\n",
    "assert np.all(np.diff(x_values) > 0)  # x_values should be strictly increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_order=0\n",
    "y_valuescorrect_order = 0\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(i+1, len(predictions)):\n",
    "        if (predictions[i] > predictions[j] and performances[i] >= performances[j]) or \\\n",
    "           (predictions[i] < predictions[j] and performances[i] <= performances[j]):\n",
    "            correct_order += 1\n",
    "\n",
    "total_comparisons = len(predictions) * (len(predictions) - 1) // 2\n",
    "correctness_percentage = correct_order / total_comparisons * 100\n",
    "\n",
    "print(f\"Percentage of correct orderings: {correctness_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([0.6, 0.9, 0.2, 0.7, 0.3, 0.5, 0.1, 0.8, 0.4])\n",
    "performances = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0])\n",
    "# Create a DataFrame with predictions and performances\n",
    "df = pd.DataFrame({'predictions': predictions, 'performances': performances})\n",
    "df.sort_values('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_mooncrater_targets(returns_df, modeling_config):\n",
    "    \"\"\"\n",
    "    Calculates 'is_moon' and 'is_crater' target variables based on performance.\n",
    "\n",
    "    Parameters:\n",
    "    - returns_df: DataFrame with columns 'coin_id' and 'performance'.\n",
    "    - modeling_config: Configuration for modeling with target variable thresholds.\n",
    "\n",
    "    Returns:\n",
    "    - target_variables_df: DataFrame with columns 'coin_id', 'is_moon', and 'is_crater'.\n",
    "    \"\"\"\n",
    "    moon_threshold = modeling_config['target_variables']['moon_threshold']\n",
    "    crater_threshold = modeling_config['target_variables']['crater_threshold']\n",
    "    moon_minimum_percent = modeling_config['target_variables']['moon_minimum_percent']\n",
    "    crater_minimum_percent = modeling_config['target_variables']['crater_minimum_percent']\n",
    "\n",
    "    target_variables_df = returns_df.copy()\n",
    "    target_variables_df['is_moon'] = (target_variables_df['performance'] >= moon_threshold).astype(int)\n",
    "    target_variables_df['is_crater'] = (target_variables_df['performance'] <= crater_threshold).astype(int)\n",
    "\n",
    "    total_coins = len(target_variables_df)\n",
    "    moons = target_variables_df['is_moon'].sum()\n",
    "    craters = target_variables_df['is_crater'].sum()\n",
    "\n",
    "    # Ensure minimum percentage for moons and craters\n",
    "    if moons / total_coins < moon_minimum_percent:\n",
    "        additional_moons_needed = int(total_coins * moon_minimum_percent) - moons\n",
    "        moon_candidates = target_variables_df[target_variables_df['is_moon'] == 0].nlargest(additional_moons_needed, 'performance')\n",
    "        target_variables_df.loc[moon_candidates.index, 'is_moon'] = 1\n",
    "\n",
    "    if craters / total_coins < crater_minimum_percent:\n",
    "        additional_craters_needed = int(total_coins * crater_minimum_percent) - craters\n",
    "        crater_candidates = target_variables_df[target_variables_df['is_crater'] == 0].nsmallest(additional_craters_needed, 'performance')\n",
    "        target_variables_df.loc[crater_candidates.index, 'is_crater'] = 1\n",
    "\n",
    "    return target_variables_df[['coin_id', 'is_moon', 'is_crater']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fe)\n",
    "target_variables_df, returns_df, outcomes_df = fe.create_target_variables(market_data_df, config['training_data'], modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df[profits_df['wallet_address']==6217496]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_profits_df():\n",
    "    data = {\n",
    "        'coin_id': [\n",
    "            'eeccf0b6-aaaa-464c-a23e-f2fc9e73a350', '3941a874-dbdf-4f53-a38e-a1f4a80855f9',\n",
    "            '1d05fab6-0fc3-4caa-9859-81a5bdf2a7c6', '1d05fab6-0fc3-4caa-9859-81a5bdf2a7c6',\n",
    "            '3941a874-dbdf-4f53-a38e-a1f4a80855f9', 'eeccf0b6-aaaa-464c-a23e-f2fc9e73a350',\n",
    "            '1d05fab6-0fc3-4caa-9859-81a5bdf2a7c6', '3941a874-dbdf-4f53-a38e-a1f4a80855f9',\n",
    "            'eeccf0b6-aaaa-464c-a23e-f2fc9e73a350', '1d05fab6-0fc3-4caa-9859-81a5bdf2a7c6',\n",
    "            '3941a874-dbdf-4f53-a38e-a1f4a80855f9', 'eeccf0b6-aaaa-464c-a23e-f2fc9e73a350'\n",
    "        ],\n",
    "        'date': [\n",
    "            '2024-07-18', '2024-07-18', '2024-07-18', '2024-08-31', '2024-08-31', '2024-08-31',\n",
    "            '2024-09-01', '2024-09-01', '2024-09-01', '2024-09-15', '2024-09-15', '2024-09-15'\n",
    "        ],\n",
    "        'wallet_address': [\n",
    "            6217496, 6217496, 6217496, 6217496, 6217496, 6217496, 6217496, 6217496, 6217496,\n",
    "            6217496, 6217496, 6217496\n",
    "        ],\n",
    "        'profits_cumulative': [\n",
    "            494.1894, -3.555027, 23462700.0, 23462700.0, -5.156542, 494.1894,\n",
    "            23462700.0, -5.554576, 494.1894, 23462700.0, -5.694496, 494.1894\n",
    "        ],\n",
    "        'usd_balance': [\n",
    "            0.0, 12.06798, 3.426631e-09, 2.643053e-09, 10.46647, 0.0,\n",
    "            2.568300e-09, 10.06843, 0.0, 2.637883e-09, 9.928511, 0.0\n",
    "        ],\n",
    "        'usd_net_transfers': [0.0] * 12,\n",
    "        'usd_inflows': [0.0] * 12,\n",
    "        'usd_inflows_cumulative': [\n",
    "            10150.54, 15.62301, 23047870.0, 23047870.0, 15.62301, 10150.54,\n",
    "            23047870.0, 15.62301, 10150.54, 23047870.0, 15.62301, 10150.54\n",
    "        ],\n",
    "        'total_return': [\n",
    "            0.048686, -0.227551, 1.017998, 1.017998, -0.330061, 0.048686,\n",
    "            1.017998, -0.355538, 0.048686, 1.017998, -0.364494, 0.048686\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "sample_profits_df = make_sample_profits_df()\n",
    "sample_profits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_config = config['data_cleaning']\n",
    "profits_df = make_sample_profits_df()\n",
    "\n",
    "# 1. Remove wallets with higher or lower total profits than the profitability_filter\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Group by wallet_address and calculate the total profitability\n",
    "wallet_profits_agg_df = profits_df.sort_values('date').groupby(\n",
    "    'wallet_address', observed=True)['profits_cumulative'].last().reset_index()\n",
    "\n",
    "# Identify wallet_addresses with total profitability that exceeds the threshold\n",
    "# pylint: disable=C0301\n",
    "exclusions_profits_df = wallet_profits_agg_df[\n",
    "    (wallet_profits_agg_df['profits_cumulative'] >= data_cleaning_config['profitability_filter']) |\n",
    "    (wallet_profits_agg_df['profits_cumulative'] <= -data_cleaning_config['profitability_filter'])\n",
    "][['wallet_address']]\n",
    "\n",
    "# Merge to filter out the records with those wallet addresses\n",
    "profits_cleaned_df = profits_df.merge(\n",
    "    exclusions_profits_df, on='wallet_address', how='left', indicator=True)\n",
    "profits_cleaned_df = profits_cleaned_df[profits_cleaned_df['_merge'] == 'left_only']\n",
    "profits_cleaned_df.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# 2. Remove wallets with higher total inflows than the inflows_filter\n",
    "# -------------------------------------------------------------------\n",
    "# Group by wallet_address and calculate the total inflows\n",
    "wallet_inflows_agg_df = profits_df.groupby(\n",
    "    'wallet_address', observed=True)['usd_inflows'].sum().reset_index()\n",
    "\n",
    "# Identify wallet addresses where total inflows exceed the threshold\n",
    "exclusions_inflows_df = wallet_inflows_agg_df[\n",
    "    wallet_inflows_agg_df['usd_inflows'] >= data_cleaning_config['inflows_filter']\n",
    "][['wallet_address']]\n",
    "\n",
    "# Merge to filter out the records with those wallet addresses\n",
    "profits_cleaned_df = profits_cleaned_df.merge(\n",
    "    exclusions_inflows_df, on='wallet_address', how='left', indicator=True)\n",
    "profits_cleaned_df = profits_cleaned_df[profits_cleaned_df['_merge'] == 'left_only']\n",
    "profits_cleaned_df.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# Convert coin_id to categorical\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "\n",
    "# 3. Prepare exclusions_df and output logs\n",
    "# ----------------------------------------\n",
    "# prepare exclusions_logs_df\n",
    "exclusions_profits_df['profits_exclusion'] = True\n",
    "exclusions_inflows_df['inflows_exclusion'] = True\n",
    "exclusions_logs_df = exclusions_profits_df.merge(\n",
    "    exclusions_inflows_df, on='wallet_address', how='outer')\n",
    "\n",
    "# Fill NaN values with False for missing exclusions\n",
    "exclusions_logs_df['profits_exclusion'] = (exclusions_logs_df['profits_exclusion']\n",
    "                                            .astype(bool).fillna(False))\n",
    "\n",
    "exclusions_logs_df['inflows_exclusion'] = (exclusions_logs_df['inflows_exclusion']\n",
    "                                            .astype(bool).fillna(False))\n",
    "# log outputs\n",
    "logger.debug(\"Identified %s coin-wallet pairs beyond profit threshold of $%s and %s pairs\"\n",
    "                \"beyond inflows filter of %s.\",\n",
    "                exclusions_profits_df.shape[0],\n",
    "                data_cleaning_config['profitability_filter'],\n",
    "                exclusions_inflows_df.shape[0],\n",
    "                data_cleaning_config['inflows_filter'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions_logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df = td.retrieve_profits_data(config['training_data']['training_period_start'],\n",
    "                                        config['training_data']['modeling_period_end'],\n",
    "                                        config['data_cleaning']['minimum_wallet_inflows'])\n",
    "split_profits_df, _ = cwm.split_dataframe_by_coverage(profits_df,\n",
    "                                                config['training_data']['training_period_start'],\n",
    "                                                config['training_data']['modeling_period_end'],\n",
    "                                                id_column='coin_id')\n",
    "cleaned_profits_df, _ = td.clean_profits_df(split_profits_df, config['data_cleaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df, exclusions_df = cleaned_profits_df\n",
    "\n",
    "# Check that every excluded wallet breached at least one threshold\n",
    "exclusions_with_breaches = exclusions_df.merge(profits_df, on='wallet_address', how='inner')\n",
    "\n",
    "# Calculate the total profits and inflows per wallet\n",
    "wallet_agg_df = exclusions_with_breaches.sort_values('date').groupby('wallet_address', observed=True).agg({\n",
    "    'profits_cumulative': 'last',\n",
    "    'usd_inflows': 'sum'\n",
    "}).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
