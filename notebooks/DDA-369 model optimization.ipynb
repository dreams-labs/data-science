{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "from typing import Dict,Union,List,Any,Tuple\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "pd.set_option('display.max_colwidth', 70)  # Increase to desired number of characters\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate training data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "training_data_df, prices_df, join_logs_df = tw.generate_all_time_windows_model_inputs(config,metrics_config,modeling_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# 1. Retrieve base datasets used by all windows\n",
    "# ---------------------------------------------\n",
    "macro_trends_df, market_data_df, profits_df, prices_df = tw.prepare_all_windows_base_data(config,\n",
    "                                                                                            metrics_config)\n",
    "\n",
    "\n",
    "# 2. Generate flattened features for each dataset in each window\n",
    "# --------------------------------------------------------------\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = tw.generate_time_windows(config)\n",
    "\n",
    "all_flattened_dfs = []\n",
    "all_flattened_filepaths = []\n",
    "\n",
    "for _, time_window in enumerate(time_windows):\n",
    "\n",
    "    # Prepare time window config files\n",
    "    window_config, window_metrics_config, window_modeling_config = (\n",
    "        exp.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "    # Generate flattened feature dfs for all datasets for the window\n",
    "    window_flattened_dfs, window_flattened_filepaths = tw.generate_window_flattened_dfs(\n",
    "        market_data_df,\n",
    "        macro_trends_df,\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        window_config,\n",
    "        window_metrics_config,\n",
    "        window_modeling_config\n",
    "    )\n",
    "\n",
    "    # Store window's flattened features\n",
    "    all_flattened_dfs.extend(window_flattened_dfs)\n",
    "    all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n",
    "\n",
    "# 3. Combine features from all datasets in all time windows with target variables\n",
    "# -------------------------------------------------------------------------------\n",
    "# Combine all time windows for each dataset, the join the datasets together\n",
    "concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Create target variables for all time windows\n",
    "target_variable_df, returns_df, = tv.create_target_variables_for_all_time_windows(training_data_df,\n",
    "                                                                                prices_df,\n",
    "                                                                                config,\n",
    "                                                                                modeling_config)\n",
    "\n",
    "# Split target variables into the train/test/validation/future sets\n",
    "sets_X_y_dict = ds.perform_train_test_validation_future_splits(training_data_df,\n",
    "                                                                target_variable_df,\n",
    "                                                                modeling_config)\n",
    "\n",
    "# Preprocess X data for all sets\n",
    "preprocessed_sets_X_y_dict = prp.preprocess_sets_X_y(sets_X_y_dict,config,metrics_config,modeling_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "X_train = preprocessed_sets_X_y_dict['train'][0]\n",
    "y_train = preprocessed_sets_X_y_dict['train'][1]\n",
    "X_test = preprocessed_sets_X_y_dict['test'][0]\n",
    "y_test = preprocessed_sets_X_y_dict['test'][1]\n",
    "returns_test = returns_df.loc[y_test.index, ['returns']]\n",
    "\n",
    "\n",
    "# # Winsorize returns\n",
    "# y_train['returns'] = m.winsorize(y_train['returns'],0.01)\n",
    "\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id, cv_scores = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict, y_pred, y_pred_prob = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "\n",
    "m.log_trial_results(modeling_config, model_id)\n",
    "print(cv_scores)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)  # Increase to desired number of characters\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns  # Feature names\n",
    "\n",
    "# Create a DataFrame with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob if y_pred_prob is not None else y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "ia.generate_profitability_curves_by_time_window(predictions, returns, winsorization_cutoff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "df = pd.DataFrame({\n",
    "    'predictions': predictions,\n",
    "    'returns': returns,\n",
    "})\n",
    "\n",
    "df.sort_values('predictions', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_coin_ids = [\n",
    "    \"d710a818-ce6e-4bab-b5ff-e39d06099c1d\"\n",
    "    ,\"7bb63899-80d5-4a35-8ff2-09dc74c6ce0d\"\n",
    "    ,\"f2c5bdc0-93a9-416c-adbe-abf19b0247d8\"\n",
    "    ,\"4f3bd04c-9f8b-47c9-85de-af46b7d095bf\"\n",
    "    ,\"6267c4b3-4f70-45b9-8574-9028d53775ee\"\n",
    "    ,\"184d124c-d38a-4669-93ff-25dda20901d8\"\n",
    "    ,\"ae0e5b04-0e47-480d-abfd-10ed64df0df9\"\n",
    "    ,\"aab2214e-52d9-4506-bc67-6b121e57c735\"\n",
    "    ,\"2b5050a3-4558-4cba-be44-973a4a6dadd9\"\n",
    "]\n",
    "\n",
    "df[df.index.get_level_values('coin_id').isin(bad_coin_ids)].sort_values('predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train\n",
    "y_test.groupby(level='time_window')['is_moon'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Sequence old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "X_train = preprocessed_sets_X_y_dict['train'][0]\n",
    "y_train = preprocessed_sets_X_y_dict['train'][1]\n",
    "X_test = preprocessed_sets_X_y_dict['test'][0]\n",
    "y_test = preprocessed_sets_X_y_dict['test'][1]\n",
    "returns_test = returns_df.loc[y_test.index, ['returns']]\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict, y_pred, y_pred_prob = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns  # Feature names\n",
    "\n",
    "# Create a DataFrame with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob if y_pred_prob is not None else y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ia.generate_profitability_curves_by_time_window(predictions, returns, winsorization_cutoff=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_name = 'whales'\n",
    "dataset_config = config['datasets']['wallet_cohorts'][cohort_name]\n",
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_period_start = config['training_data']['training_period_start']\n",
    "cohort_lookback = config['datasets']['wallet_cohorts'][cohort_name]['lookback_period']\n",
    "cohort_lookback_start = pd.to_datetime(training_period_start) - timedelta(days=cohort_lookback)\n",
    "cohort_lookback_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# 1. Retrieve base datasets used by all windows\n",
    "# ---------------------------------------------\n",
    "macro_trends_df, market_data_df, profits_df, prices_df = tw.prepare_all_windows_base_data(config,\n",
    "                                                                                            metrics_config)\n",
    "\n",
    "\n",
    "# 2. Generate flattened features for each dataset in each window\n",
    "# --------------------------------------------------------------\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = tw.generate_time_windows(config)\n",
    "\n",
    "all_flattened_dfs = []\n",
    "all_flattened_filepaths = []\n",
    "\n",
    "for _, time_window in enumerate(time_windows):\n",
    "\n",
    "    # Prepare time window config files\n",
    "    window_config, window_metrics_config, window_modeling_config = (\n",
    "        exp.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "    # Generate flattened feature dfs for all datasets for the window\n",
    "    window_flattened_dfs, window_flattened_filepaths = tw.generate_window_flattened_dfs(\n",
    "        market_data_df,\n",
    "        macro_trends_df,\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        window_config,\n",
    "        window_metrics_config,\n",
    "        window_modeling_config\n",
    "    )\n",
    "\n",
    "    # Store window's flattened features\n",
    "    all_flattened_dfs.extend(window_flattened_dfs)\n",
    "    all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n",
    "\n",
    "# 3. Combine features from all datasets in all time windows with target variables\n",
    "# -------------------------------------------------------------------------------\n",
    "# Combine all time windows for each dataset, the join the datasets together\n",
    "concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Market data: retrieve and clean full history\n",
    "market_data_df = dr.retrieve_market_data()\n",
    "market_data_df = dr.clean_market_data(market_data_df, config)\n",
    "\n",
    "# Profits: retrieve and clean profits data spanning the earliest to latest training periods\n",
    "profits_df = dr.retrieve_profits_data(config['training_data']['earliest_cohort_lookback_start'],\n",
    "                                    config['training_data']['training_period_end'],\n",
    "                                    config['data_cleaning']['minimum_wallet_inflows'])\n",
    "profits_df, _ = dr.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "\n",
    "# 2. Filtering based on dataset overlap\n",
    "# -------------------------------------\n",
    "# Filter market_data to only coins with transfers data if configured to\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "# Create prices_df: lightweight reference for other functions\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# Filter profits_df to remove records for any coins that were removed in data cleaning\n",
    "profits_df = profits_df[profits_df['coin_id'].isin(market_data_df['coin_id'])]\n",
    "\n",
    "\n",
    "# 3. Add indicators (additional time series)\n",
    "# ------------------------------------------\n",
    "# Macro trends: add indicators\n",
    "macro_trends_df = ind.generate_time_series_indicators(macro_trends_df,\n",
    "                                                    metrics_config['macro_trends'],\n",
    "                                                    # None)\n",
    "# Market data: add indicators\n",
    "market_data_df_full = ind.generate_time_series_indicators(market_data_df,\n",
    "                                                    metrics_config['time_series']['market_data'],\n",
    "                                                    'coin_id')\n",
    "# market_data_df = ind.add_market_data_dualcolumn_indicators(market_data_df)\n",
    "market_data_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "market_data_df = market_data_df_full.copy()\n",
    "market_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "market_data_df = ind.add_mfi_column(market_data_df, price_col='price', volume_col='volume', window=14)\n",
    "market_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "market_data_df = ind.add_market_data_dualcolumn_indicators(market_data_df)\n",
    "market_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market data: retrieve and clean full history\n",
    "market_data_df = dr.retrieve_market_data()\n",
    "market_data_df = dr.clean_market_data(market_data_df, config)\n",
    "market_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_market_data_multicolumn_indicators(market_data_df):\n",
    "    \"\"\"\n",
    "    Adds multi-column indicators to market_data_df\n",
    "    \"\"\"\n",
    "    market_data_df = ind.add_mfi_column(market_data_df, price_col='price', volume_col='volume', window=14)\n",
    "    market_data_df['obv'] = ind.generalized_obv(market_data_df['price'], market_data_df['volume'])\n",
    "\n",
    "    return market_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_indicators(dataset_df, dataset_metrics_config, id_column):\n",
    "    \"\"\"\n",
    "    Generates all indicators for a time series dataframe keyed on coin_id and date. This is\n",
    "    a wrapper function to apply ind.generate_column_time_series_indicators() to each dataset\n",
    "    column with indicator configurations.\n",
    "\n",
    "    Params:\n",
    "    - dataset_df (DataFrame): The df containing dataset metrics and a coin_id and date column,\n",
    "        as well as columns needing indicator calculations.\n",
    "    - dataset_metrics_config (dict): The subcomponent of metrics_config that has keys for the\n",
    "        columns needing indicators, e.g. metrics_config['time_series']['market_data']\n",
    "    - id_column: whether the input df has an id column that needs to be grouped on\n",
    "\n",
    "    Returns:\n",
    "    - dataset_indicators_df (DataFrame): The original dataset_df with added columns for all\n",
    "        configured indicators.\n",
    "    \"\"\"\n",
    "    # Calculate indicators for each value column\n",
    "    for value_column in list(dataset_metrics_config.keys()):\n",
    "\n",
    "        if 'indicators' in dataset_metrics_config[value_column].keys():\n",
    "            dataset_df = generate_column_time_series_indicators(\n",
    "                dataset_df,\n",
    "                value_column,\n",
    "                dataset_metrics_config[value_column]['indicators'],\n",
    "                id_column\n",
    "            )\n",
    "\n",
    "    return dataset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_windows(config):\n",
    "    \"\"\"\n",
    "    Generates the parameter dicts used by i.prepare_configs() to generate the full set\n",
    "    of config files.\n",
    "\n",
    "    Params:\n",
    "        config (dict): config.yaml\n",
    "\n",
    "    Returns:\n",
    "        time_windows (list of dicts): a list of dicts that can be used to override the\n",
    "        config.yaml settings for each time window.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(config['training_data']['modeling_period_start'])\n",
    "    window_frequency = config['training_data']['time_window_frequency']\n",
    "\n",
    "    time_windows = [\n",
    "        {'config.training_data.modeling_period_start': start_date.strftime('%Y-%m-%d')}\n",
    "    ]\n",
    "\n",
    "    for _ in range(config['training_data']['additional_windows']):\n",
    "        start_date -= timedelta(days=window_frequency)\n",
    "        time_windows.append({'config.training_data.modeling_period_start': start_date.strftime('%Y-%m-%d')})\n",
    "\n",
    "    time_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "training_data_config = config['training_data']\n",
    "\n",
    "# Extract the config values\n",
    "modeling_period_start = datetime.strptime(training_data_config['modeling_period_start'],\n",
    "                                            '%Y-%m-%d')\n",
    "modeling_period_duration = training_data_config['modeling_period_duration']  # in days\n",
    "training_period_duration = training_data_config['training_period_duration']  # in days\n",
    "\n",
    "# Training and Modeling Period Dates\n",
    "# ----------------------------------\n",
    "# Calculate modeling_period_end (inclusive of the start date)\n",
    "modeling_period_end = modeling_period_start + timedelta(days=modeling_period_duration - 1)\n",
    "\n",
    "# Calculate training_period_end (just before modeling_period_start)\n",
    "training_period_end = modeling_period_start - timedelta(days=1)\n",
    "\n",
    "# Calculate training_period_start (inclusive of the start date)\n",
    "training_period_start = training_period_end - timedelta(days=training_period_duration - 1)\n",
    "\n",
    "# Lookback Dates\n",
    "# --------------\n",
    "# Calculate the start date of the earliest window\n",
    "window_frequency = training_data_config['time_window_frequency']\n",
    "additional_windows = training_data_config['additional_windows']\n",
    "total_days_range = ((window_frequency * additional_windows) # the number of lookback days from the time windows\n",
    "                    + (modeling_period_duration + training_period_duration))\n",
    "earliest_window_start = pd.to_datetime(modeling_period_end) - timedelta(days=total_days_range)\n",
    "\n",
    "# Calculate the earliest cohort lookback date for the earliest window\n",
    "# Identify all unique cohort lookback periods\n",
    "cohort_lookback_periods = [\n",
    "    cohort['lookback_period']\n",
    "    for cohort in config['datasets']['wallet_cohorts'].values()\n",
    "]\n",
    "earliest_cohort_lookback_start = (earliest_window_start -\n",
    "                                    timedelta(days=max(cohort_lookback_periods)))\n",
    "\n",
    "\n",
    "earliest_window_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(modeling_period_end) - timedelta(days=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
