{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "from typing import Dict,Union,List,Any,Tuple\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "training_data_df, target_variable_df, returns_df, join_logs_df = tw.generate_all_time_windows_model_inputs(config,metrics_config,modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# 1. Retrieve base datasets used by all windows\n",
    "# ---------------------------------------------\n",
    "macro_trends_df, market_data_df, profits_df, prices_df = tw.prepare_all_windows_base_data(config,\n",
    "                                                                                            metrics_config)\n",
    "\n",
    "\n",
    "# 2. Generate flattened features for each dataset in each window\n",
    "# --------------------------------------------------------------\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = tw.generate_time_windows(config)\n",
    "\n",
    "all_flattened_dfs = []\n",
    "all_flattened_filepaths = []\n",
    "\n",
    "for _, time_window in enumerate(time_windows):\n",
    "\n",
    "    # Prepare time window config files\n",
    "    window_config, window_metrics_config, window_modeling_config = (\n",
    "        exp.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "    # Generate flattened feature dfs for all datasets for the window\n",
    "    window_flattened_dfs, window_flattened_filepaths = tw.generate_window_flattened_dfs(\n",
    "        market_data_df,\n",
    "        macro_trends_df,\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        window_config,\n",
    "        window_metrics_config,\n",
    "        window_modeling_config\n",
    "    )\n",
    "\n",
    "    # Store window's flattened features\n",
    "    all_flattened_dfs.extend(window_flattened_dfs)\n",
    "    all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n",
    "\n",
    "# 3. Combine features from all datasets in all time windows with target variables\n",
    "# -------------------------------------------------------------------------------\n",
    "# Combine all time windows for each dataset, the join the datasets together\n",
    "concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Split target variables into the train/test/validation/future sets\n",
    "sets_X_y_dict = ds.perform_train_test_validation_future_splits(training_data_df,\n",
    "                                                                target_variable_df,\n",
    "                                                                modeling_config)\n",
    "\n",
    "# Preprocess X data for all sets\n",
    "preprocessed_sets_X_y_dict = prp.preprocess_sets_X_y(sets_X_y_dict,config,metrics_config,modeling_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set = \"train\"\n",
    "model_period = config['training_data']['modeling_period_start']\n",
    "windows = config['training_data']['additional_windows']\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "filename = f\"preprocessed_X_{set}_{timestamp}_model_period_{model_period}_{windows}_windows.csv\"\n",
    "\n",
    "\n",
    ","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set,(X,y) in preprocessed_sets_X_y_dict.items():\n",
    "    print(set)\n",
    "    print(X.shape)\n",
    "    print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_outputs(set_key, preprocessed_X, y_data, config, modeling_config):\n",
    "    \"\"\"\n",
    "    Saves the flattened DataFrame with descriptive metrics into a CSV file.\n",
    "\n",
    "    Params:\n",
    "    - set_key (str): The name of set (e.g. 'train', 'test', etc)\n",
    "    - preprocessed_X (pd.DataFrame): DataFrame with MultiIndex on time_window,period containing all\n",
    "        features for the set\n",
    "    - y_data (pd.DataFrame): DataFrame with MultiIndex on time_window,period containing target\n",
    "        variables for the set\n",
    "    - config (dict): config.yaml\n",
    "    - modeling_config (dict): modeling_config.yaml\n",
    "\n",
    "    Returns:\n",
    "    none\n",
    "    \"\"\"\n",
    "\n",
    "    # Define filename with metric description and optional description\n",
    "    model_period = config['training_data']['modeling_period_start']\n",
    "    windows = config['training_data']['additional_windows']\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    filename = f\"preprocessed_X_{set_key}_{timestamp}_model_period_{model_period}_{windows}_windows.csv\"\n",
    "\n",
    "    # Join X and y\n",
    "    preprocessed_df = preprocessed_X.join(y_data)\n",
    "\n",
    "    # Save file\n",
    "    output_dir = os.path.join(modeling_config['modeling']['modeling_folder'],\n",
    "                            'outputs/preprocessed_outputs')\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    preprocessed_df.to_csv(output_path, index=False)\n",
    "\n",
    "    logger.debug(\"Saved preprocessed outputs to %s\", output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sets_X_y_dict['train_set'][0]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Load your DataFrames (replace this with your actual data loading code)\n",
    "datasets = {\n",
    "    'train': df\n",
    "}\n",
    "\n",
    "# Initialize and run the preprocessor\n",
    "preprocessor = prp.DataPreprocessor(config, metrics_config, modeling_config)\n",
    "preprocessed_datasets = preprocessor.preprocess(datasets)\n",
    "\n",
    "# Print results\n",
    "for dataset_name, df in preprocessed_datasets.items():\n",
    "    print(f\"Columns in {dataset_name} set: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Confirm there are no null values\n",
    "if df.isnull().values.any():\n",
    "    raise ValueError(\"Missing values detected in the DataFrame.\")\n",
    "\n",
    "# Convert all columns to numeric\n",
    "df = prp.preprocess_categorical_and_boolean(df)\n",
    "\n",
    "# Feature Selection\n",
    "# Drop features specified in modeling_config['drop_features']\n",
    "drop_features = modeling_config['preprocessing'].get('drop_features', [])\n",
    "if drop_features:\n",
    "    df = df.drop(columns=drop_features, errors='warn')\n",
    "\n",
    "# # Apply feature selection based on sameness_threshold and retain_columns from dataset_config\n",
    "# sameness_threshold = dataset_config.get('sameness_threshold', 1.0)\n",
    "# retain_columns = dataset_config.get('retain_columns', [])\n",
    "\n",
    "# # Drop columns with more than `sameness_threshold` of the same value, unless in retain_columns\n",
    "# for column in df.columns:\n",
    "#     if column not in retain_columns:\n",
    "#         max_value_ratio = df[column].value_counts(normalize=True).max()\n",
    "#         if max_value_ratio > sameness_threshold:\n",
    "#             df = df.drop(columns=[column])\n",
    "#             logger.debug(\"Dropped column %s due to sameness_threshold\", column)\n",
    "\n",
    "\n",
    "# # Step 4: Scaling and Transformation\n",
    "# # ----------------------------------------------------\n",
    "# # Apply scaling if df_metrics_config is provided\n",
    "# if df_metrics_config:\n",
    "#     df = apply_scaling(df, df_metrics_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "\n",
    "def calculate_sameness_percentage(column: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of the most common value in a column.\n",
    "\n",
    "    Parameters:\n",
    "    column (pd.Series): The column to analyze.\n",
    "\n",
    "    Returns:\n",
    "    float: The percentage (0 to 1) of the most common value in the column.\n",
    "    \"\"\"\n",
    "    return column.value_counts().iloc[0] / len(column)\n",
    "\n",
    "def create_prefix_mapping(config: Dict[str, Any]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Create a mapping of column prefixes to their config paths and sameness thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    config (Dict[str, Any]): The configuration dictionary containing dataset information.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[str, float]]: A dictionary where keys are column prefixes and values are\n",
    "    dictionaries containing 'path' (str) and 'threshold' (float) for each prefix.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    for dataset_type, dataset_config in config['datasets'].items():\n",
    "        for category, category_config in dataset_config.items():\n",
    "            if isinstance(category_config, dict) and 'sameness_threshold' in category_config:\n",
    "                prefix = f\"{category}_\"\n",
    "                mapping[prefix] = {\n",
    "                    'path': f\"datasets.{dataset_type}.{category}\",\n",
    "                    'threshold': category_config['sameness_threshold']\n",
    "                }\n",
    "            elif isinstance(category_config, dict):\n",
    "                for subcategory, subcategory_config in category_config.items():\n",
    "                    if 'sameness_threshold' in subcategory_config:\n",
    "                        prefix = f\"{subcategory}_\"\n",
    "                        mapping[prefix] = {\n",
    "                            'path': f\"datasets.{dataset_type}.{category}.{subcategory}\",\n",
    "                            'threshold': subcategory_config['sameness_threshold']\n",
    "                        }\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def check_and_drop_columns(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check column sameness and drop columns exceeding the threshold.\n",
    "\n",
    "    This function analyzes each column in the DataFrame, calculates its sameness percentage,\n",
    "    and drops columns that exceed the threshold specified in the configuration.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to process.\n",
    "    config (Dict[str, Any]): The configuration dictionary containing sameness thresholds.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with columns dropped based on the sameness criteria.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If any columns can't be mapped to a sameness threshold or if any config keys\n",
    "                can't be mapped to columns.\n",
    "    \"\"\"\n",
    "    prefix_mapping = create_prefix_mapping(config)\n",
    "    columns_to_drop = []\n",
    "    unmapped_columns = []\n",
    "    used_config_keys = set()\n",
    "\n",
    "    for column in df.columns:\n",
    "        mapped = False\n",
    "        for prefix, config_info in prefix_mapping.items():\n",
    "            if column.startswith(prefix):\n",
    "                mapped = True\n",
    "                used_config_keys.add(prefix)\n",
    "                sameness = calculate_sameness_percentage(df[column])\n",
    "                if sameness > config_info['threshold']:\n",
    "                    columns_to_drop.append(column)\n",
    "                break\n",
    "        if not mapped:\n",
    "            unmapped_columns.append(column)\n",
    "\n",
    "    unused_config_keys = set(prefix_mapping.keys()) - used_config_keys\n",
    "\n",
    "    if unmapped_columns:\n",
    "        raise ValueError(f\"The following columns could not be mapped to a sameness threshold: {unmapped_columns}\")\n",
    "\n",
    "    if unused_config_keys:\n",
    "        raise ValueError(f\"The following config keys could not be mapped to columns: {unused_config_keys}\")\n",
    "\n",
    "    # Drop the columns\n",
    "    df.drop(columns=columns_to_drop)\n",
    "    logger.info(\"Dropped %s columns %s due to sameness thresholds.\", len(columns_to_drop), columns_to_drop)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = check_and_drop_columns(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert categorical and boolean columns to integers\n",
    "# ---------------------------------------------------------------\n",
    "# Convert categorical columns to one-hot encoding (get_dummies)\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_columns = [col for col in categorical_columns]\n",
    "for col in categorical_columns:\n",
    "    num_categories = df[col].nunique()\n",
    "    if num_categories > 8:\n",
    "        logger.warning(\"Column '%s' has %s categories, consider reducing categories.\",\n",
    "                        col, num_categories)\n",
    "    df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "df = df.apply(lambda col: col.astype(int) if col.dtype == bool else col)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All windows datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "def generate_all_time_windows_model_inputs(config,metrics_config,modeling_config):\n",
    "    \"\"\"\n",
    "    Generates the X and y splits for all sets across all time windows.\n",
    "\n",
    "    Sequence:\n",
    "    1. Retrieve the base datasets that contain records across all windows\n",
    "    2. Loop through each time window and generate flattened features for the window\n",
    "    3a. Concat each dataset's window dfs, then join all the dataset dfs with the target variable to\n",
    "        create a comprehensive feature set keyed on coin_id.\n",
    "    3b. Split the full feature set into train/test/validation/future sets.\n",
    "\n",
    "    Params:\n",
    "    - config, metrics_config, modeling_config: loaded config yaml files\n",
    "\n",
    "    Returns:\n",
    "    - sets_X_y_dict (dict[pd.DataFrame, pd.Series]): Dict with keys for each set type (e.g. train_set,\n",
    "        future_set, etc) that contains the X and y data for the set.\n",
    "    - returns_df (pd.DataFrame): DataFrame with MultiIndex on time_window,coin_id that contains a\n",
    "        'returns' column showing actual returns during the each time_window's modeling period.\n",
    "    - join_logs_df (pd.DataFrame): DataFrame showing the outcomes of each dataset's join and fill\n",
    "        methods\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Retrieve base datasets used by all windows\n",
    "    # ---------------------------------------------\n",
    "    macro_trends_df, market_data_df, profits_df, prices_df = tw.prepare_all_windows_base_data(config,\n",
    "                                                                                              metrics_config)\n",
    "\n",
    "\n",
    "    # 2. Generate flattened features for each dataset in each window\n",
    "    # --------------------------------------------------------------\n",
    "    # Generate time_windows config overrides that will modify each window's config settings\n",
    "    time_windows = tw.generate_time_windows(config)\n",
    "\n",
    "    all_flattened_dfs = []\n",
    "    all_flattened_filepaths = []\n",
    "\n",
    "    for n, time_window in enumerate(time_windows):\n",
    "\n",
    "        # Prepare time window config files\n",
    "        window_config, window_metrics_config, window_modeling_config = (\n",
    "            exp.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "        # Generate flattened feature dfs for all datasets for the window\n",
    "        window_flattened_dfs, window_flattened_filepaths = tw.generate_window_flattened_dfs(\n",
    "            market_data_df,\n",
    "            macro_trends_df,\n",
    "            profits_df,\n",
    "            prices_df,\n",
    "            window_config,\n",
    "            window_metrics_config,\n",
    "            window_modeling_config\n",
    "        )\n",
    "\n",
    "        # Store window's flattened features\n",
    "        all_flattened_dfs.extend(window_flattened_dfs)\n",
    "        all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n",
    "\n",
    "    # 3. Combine features from all datasets in all time windows with target variables\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Combine all time windows for each dataset, the join the datasets together\n",
    "    concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "    training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n",
    "\n",
    "    # Create target variables for all time windows\n",
    "    target_variable_df, returns_df, = tw.create_target_variables_for_all_time_windows(training_data_df,\n",
    "                                                                                        prices_df,\n",
    "                                                                                        config,\n",
    "                                                                                        modeling_config)\n",
    "\n",
    "    # Split target variables into the train/test/validation/future sets\n",
    "    sets_X_y_dict = prp.perform_train_test_validation_future_splits(training_data_df,\n",
    "                                                                    target_variable_df,\n",
    "                                                                    modeling_config)\n",
    "\n",
    "    return sets_X_y_dict, returns_df, join_logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "\n",
    "# Initialize empty lists to hold concatenated data\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "returns_test_list = []\n",
    "\n",
    "for n, window in enumerate(time_windows):\n",
    "\n",
    "    model_data = mif.build_time_window_model_input(n, window, config, metrics_config, modeling_config)\n",
    "\n",
    "    # Append the current window's data to the lists\n",
    "    X_train_list.append(model_data['X_train'])\n",
    "    X_test_list.append(model_data['X_test'])\n",
    "    y_train_list.append(model_data['y_train'])\n",
    "    y_test_list.append(model_data['y_test'])\n",
    "    returns_test_list.append(model_data['returns_test'])\n",
    "\n",
    "\n",
    "# Concatenate all the data for each part\n",
    "X_train = pd.concat(X_train_list, axis=0)\n",
    "X_test = pd.concat(X_test_list, axis=0)\n",
    "y_train = pd.concat(y_train_list, axis=0)\n",
    "y_test = pd.concat(y_test_list, axis=0)\n",
    "returns_test = pd.concat(returns_test_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict, y_pred, y_pred_prob = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns  # Feature names\n",
    "\n",
    "# Create a DataFrame with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob or y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
