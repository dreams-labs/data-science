{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG, MODELING_FOLDER\n",
    "\n",
    "CONFIG = u.load_config('../config/config.yaml')\n",
    "METRICS_CONFIG = u.load_config('../config/metrics_config.yaml')\n",
    "MODELING_CONFIG = u.load_config('../config/modeling_config.yaml')\n",
    "EXPERIMENTS_CONFIG = u.load_config('../config/experiments_config.yaml')\n",
    "MODELING_FOLDER = MODELING_CONFIG['modeling']['modeling_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "experiment_id = i.run_experiment(modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = '0926_whale_cohort_cutoffs_844325ab-00f6-4da9-8584-bf9026340770'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df = i.generate_trial_df(modeling_config['modeling']['modeling_folder'], experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.plot_top_feature_importance(MODELING_FOLDER, experiment_id, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss']\n",
    "\n",
    "id_vars = [col for col in trial_df.columns if col not in (value_vars + ['confusion_matrix'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe\n",
    "pd.melt(df, id_vars=id_vars, value_vars=value_vars,\n",
    "                    var_name='metric', value_name='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_feature_performance(trial_df):\n",
    "    \"\"\"\n",
    "    Summarizes the performance of the values for each feature that was experimented on.\n",
    "    \"\"\"\n",
    "    # List of performance metrics\n",
    "    value_vars = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss']\n",
    "\n",
    "    # List of columns to melt (all except performance metrics)\n",
    "    id_vars = [col for col in trial_df.columns if col not in (value_vars + ['confusion_matrix'])]\n",
    "\n",
    "    # Melt the dataframe\n",
    "    melted_df = pd.melt(df, id_vars=id_vars, value_vars=value_vars,\n",
    "                        var_name='metric', value_name='value')\n",
    "\n",
    "    # Create a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through each feature\n",
    "    for feature in id_vars:\n",
    "        # Group by feature value and metric, then calculate the mean\n",
    "        grouped = melted_df.groupby([feature, 'metric'])['value'].mean().unstack()\n",
    "\n",
    "        # Calculate the count of models for each feature value\n",
    "        model_count = melted_df.groupby(feature)['metric'].count().div(len(value_vars)).astype(int)\n",
    "\n",
    "        # Reset index and rename columns\n",
    "        grouped = grouped.reset_index()\n",
    "        grouped.columns.name = None\n",
    "        grouped = grouped.rename(columns={col: f'avg_{col}' for col in value_vars})\n",
    "\n",
    "        # Rename the feature column and add the model count\n",
    "        grouped = grouped.rename(columns={feature: 'value'})\n",
    "        grouped['model_count'] = grouped['value'].map(model_count)\n",
    "        grouped['feature'] = feature\n",
    "\n",
    "        # Reorder columns\n",
    "        column_order = ['feature', 'value', 'model_count'] + [f'avg_{metric}' for metric in value_vars]\n",
    "        grouped = grouped[column_order]\n",
    "\n",
    "        # Append to results\n",
    "        results.append(grouped)\n",
    "\n",
    "    # Concatenate all results\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Sort the dataframe\n",
    "    final_df = final_df.sort_values(['feature', 'value'])\n",
    "\n",
    "    # Reset index\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "feature_performance_df = i.summarize_feature_performance(trial_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_performance_df\n",
    "feature_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the columns that start with 'avg'\n",
    "columns_to_format = [col for col in feature_performance_df.columns if col.startswith('avg')]\n",
    "\n",
    "# Apply conditional formatting to those columns\n",
    "feature_performance_df_styled = feature_performance_df.style.background_gradient(subset=columns_to_format, cmap='RdYlGn')\n",
    "\n",
    "# Display the styled DataFrame\n",
    "feature_performance_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the data is already in a DataFrame called 'df'\n",
    "# If it's not, you'll need to read it from a file or create it from the provided data\n",
    "\n",
    "# List of columns to apply conditional formatting\n",
    "performance_columns = ['avg_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score', 'avg_roc_auc', 'avg_log_loss']\n",
    "\n",
    "def color_scale(series, cmap='RdYlGn', low=0, high=1):\n",
    "    return [f'background-color: {plt.cm.get_cmap(cmap)(x)}'\n",
    "            for x in plt.Normalize(low, high)(series.astype(float))]\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "\n",
    "# Apply styling\n",
    "styled_df = df.style.apply(color_scale, subset=performance_columns)\n",
    "\n",
    "# Highlight max values in green and min values in red for each column\n",
    "for col in performance_columns:\n",
    "    styled_df = styled_df.apply(highlight_max, props='color: green; font-weight: bold', subset=[col])\n",
    "    styled_df = styled_df.apply(highlight_min, props='color: red; font-weight: bold', subset=[col])\n",
    "\n",
    "# Format numeric columns to display 3 decimal places\n",
    "styled_df = styled_df.format({col: '{:.3f}' for col in performance_columns})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the data is already in a DataFrame called 'df'\n",
    "# If it's not, you'll need to read it from a file or create it from the provided data\n",
    "\n",
    "# List of columns to apply conditional formatting\n",
    "performance_columns = ['avg_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score', 'avg_roc_auc', 'avg_log_loss']\n",
    "\n",
    "def color_scale(series, cmap='RdYlGn', low=0, high=1):\n",
    "    return [f'background-color: {color}' for color in plt.cm.get_cmap(cmap)(np.linspace(low, high, len(series)))]\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "\n",
    "# Apply styling\n",
    "styled_df = df.style.apply(color_scale, subset=performance_columns)\n",
    "\n",
    "# Highlight max values in green and min values in red for each column\n",
    "for col in performance_columns:\n",
    "    styled_df = styled_df.apply(highlight_max, props='color: green; font-weight: bold', subset=[col])\n",
    "    styled_df = styled_df.apply(highlight_min, props='color: red; font-weight: bold', subset=[col])\n",
    "\n",
    "# Format numeric columns to display 3 decimal places\n",
    "styled_df = styled_df.format({col: '{:.3f}' for col in performance_columns})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the data is already in a DataFrame called 'df'\n",
    "# If it's not, you'll need to read it from a file or create it from the provided data\n",
    "\n",
    "# List of columns to apply conditional formatting\n",
    "performance_columns = ['avg_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score', 'avg_roc_auc', 'avg_log_loss']\n",
    "\n",
    "def color_scale(val, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Returns a string representing a color on a scale from red to yellow to green.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return ''\n",
    "    normalized = (val - min_val) / (max_val - min_val)\n",
    "    r = int(255 * (1 - normalized))\n",
    "    g = int(255 * normalized)\n",
    "    b = 0\n",
    "    return f'background-color: rgb({r},{g},{b})'\n",
    "\n",
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "\n",
    "# Apply styling\n",
    "def style_dataframe(df):\n",
    "    styled = feature_performance_df.style\n",
    "\n",
    "    # Apply color scale to performance columns\n",
    "    for col in performance_columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        styled = styled.applymap(lambda x: color_scale(x, min_val, max_val), subset=[col])\n",
    "\n",
    "    # Highlight max values in green and min values in red for each column\n",
    "    for col in performance_columns:\n",
    "        styled = styled.apply(highlight_max, props='color: #00FFFF; font-weight: bold', subset=[col])\n",
    "        styled = styled.apply(highlight_min, props='color: #202020; font-weight: bold', subset=[col])\n",
    "\n",
    "    # Format numeric columns to display 3 decimal places\n",
    "    styled = styled.format({col: '{:.3f}' for col in performance_columns})\n",
    "\n",
    "    return styled\n",
    "\n",
    "# Apply styling to the DataFrame\n",
    "styled_df = style_dataframe(feature_performance_df)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your original dataframe\n",
    "\n",
    "# Melt the dataframe to reshape it for easier grouping\n",
    "df_melted = pd.melt(df, id_vars=[\"modeling_config.target_variables.moon_threshold\",\n",
    "                                 \"config.datasets.wallet_cohorts.whales.wallet_minimum_inflows\"],\n",
    "                    value_vars=[\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"roc_auc\", \"log_loss\"],\n",
    "                    var_name=\"metric\", value_name=\"value\")\n",
    "\n",
    "df_melted\n",
    "\n",
    "# # Group by each feature and compute the mean of the metrics\n",
    "# df_avg = df_melted.groupby([\"modeling_config.target_variables.moon_threshold\",\n",
    "#                             \"config.datasets.wallet_cohorts.whales.wallet_minimum_inflows\", \"metric\"]).mean().reset_index()\n",
    "\n",
    "# # Pivot the dataframe to get desired format\n",
    "# df_pivoted = df_avg.pivot(index=[\"modeling_config.target_variables.moon_threshold\",\n",
    "#                                  \"config.datasets.wallet_cohorts.whales.wallet_minimum_inflows\"],\n",
    "#                           columns=\"metric\", values=\"value\").reset_index()\n",
    "\n",
    "# # Rename the columns to indicate averages\n",
    "# df_pivoted.columns = ['feature', 'value', 'avg_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score', 'avg_roc_auc', 'avg_log_loss']\n",
    "\n",
    "# # Display the final structured dataframe\n",
    "# df_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.plot_top_feature_importance(MODELING_FOLDER, experiment_id, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.analyze_experiment(MODELING_FOLDER, experiment_id, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# retreive market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df,_ = td.fill_market_data_gaps(market_data_df,config['data_cleaning']['max_gap_days'])\n",
    "market_data_df,_,_ = cwm.split_dataframe_by_coverage(\n",
    "    market_data_df,\n",
    "    start_date=config['training_data']['training_period_start'],\n",
    "    end_date=config['training_data']['modeling_period_end'],\n",
    "    id_column='coin_id'\n",
    ")\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# retrieve transfers data\n",
    "transfers_df = td.retrieve_transfers_data(\n",
    "    config['training_data']['training_period_start'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end']\n",
    "    )\n",
    "\n",
    "# compile profits_df\n",
    "profits_df = td.prepare_profits_data(transfers_df, prices_df)\n",
    "profits_df = td.calculate_wallet_profitability(profits_df)\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "# remove records from market_data_df that don't have transfers if configured to do so\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
