{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "\n",
    "# Load all configs as global variables\n",
    "global CONFIG, METRICS_CONFIG, MODELING_CONFIG, EXPERIMENTS_CONFIG\n",
    "\n",
    "CONFIG = u.load_config('../config/config.yaml')\n",
    "METRICS_CONFIG = u.load_config('../config/metrics_config.yaml')\n",
    "MODELING_CONFIG = u.load_config('../config/modeling_config.yaml')\n",
    "EXPERIMENTS_CONFIG = u.load_config('../config/experiments_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# retreive market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df,_ = td.fill_market_data_gaps(market_data_df,config['data_cleaning']['max_gap_days'])\n",
    "market_data_df,_,_ = cwm.split_dataframe_by_coverage(\n",
    "    market_data_df,\n",
    "    start_date=config['training_data']['training_period_start'],\n",
    "    end_date=config['training_data']['modeling_period_end'],\n",
    "    id_column='coin_id'\n",
    ")\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "\n",
    "# retrieve transfers data\n",
    "transfers_df = td.retrieve_transfers_data(\n",
    "    config['training_data']['training_period_start'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end']\n",
    "    )\n",
    "\n",
    "\n",
    "# compile profits_df\n",
    "profits_df = td.prepare_profits_data(transfers_df, prices_df)\n",
    "profits_df = td.calculate_wallet_profitability(profits_df)\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# initial steps for this model\n",
    "filtered_market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "\n",
    "training_data_tuples = []\n",
    "training_data_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# 1. Extract config variables and store experiment metadata\n",
    "# ---------------------------------------------------------\n",
    "# Extract folder paths from modeling_config\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "config_folder = modeling_config['modeling']['config_folder']\n",
    "\n",
    "# Load experiments_config.yaml\n",
    "experiments_config = u.load_config(os.path.join(config_folder, 'experiments_config.yaml'))\n",
    "\n",
    "# Extract metadata and experiment details from experiments_config\n",
    "experiment_name = experiments_config['metadata']['experiment_name']\n",
    "experiment_id = f\"{experiment_name}_{uuid.uuid4()}\"\n",
    "search_method = experiments_config['metadata']['search_method']\n",
    "max_evals = experiments_config['metadata']['max_evals']\n",
    "\n",
    "# Add a timestamp to the metadata\n",
    "metadata = experiments_config['metadata']\n",
    "metadata['experiment_id'] = experiment_id\n",
    "metadata['start_time'] = datetime.now().isoformat()\n",
    "metadata['trial_logs'] = []  # Initialize the array for trial log filenames\n",
    "\n",
    "\n",
    "# 2. Initialize trial configurations and initial variables\n",
    "# -------------------------------------------------------------------------\n",
    "# Generate the trial configurations based on variable_overrides\n",
    "trial_configurations = i.generate_experiment_configurations(config_folder, method=search_method, max_evals=max_evals)\n",
    "\n",
    "# Cap the number of trials if 'max_evals' is set\n",
    "max_evals = experiments_config['metadata'].get('max_evals', len(trial_configurations))\n",
    "total_trials = min(len(trial_configurations), max_evals)\n",
    "\n",
    "# Generate prices_df\n",
    "config = u.load_config(os.path.join(config_folder, 'config.yaml'))\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = td.fill_market_data_gaps(market_data_df, config['data_cleaning']['max_gap_days'])\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# Initialize progress bar and empty variables\n",
    "trials_bar = u.create_progress_bar(total_trials)\n",
    "profits_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "trial = trial_configurations[:total_trials][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3.1 Prepare the full configuration by applying overrides from the current trial config\n",
    "config, metrics_config, modeling_config = i.prepare_configs(config_folder, trial)\n",
    "\n",
    "# Store the configuration settings used in this trial in metadata\n",
    "metadata['config_settings'] = {\n",
    "    \"config\": config,\n",
    "    \"metrics_config\": metrics_config,\n",
    "    \"modeling_config\": modeling_config\n",
    "}\n",
    "\n",
    "# # 3.2 Retrieve or rebuild profits_df based on config changes\n",
    "# profits_df = i.rebuild_profits_df_if_necessary(config, modeling_folder, prices_df, profits_df)\n",
    "\n",
    "# 3.3 Build the configured model input data (train/test data)\n",
    "X_train, X_test, y_train, y_test = i.build_configured_model_input(profits_df, prices_df, config, metrics_config, modeling_config)\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate and log the model's performance on the test set\n",
    "_ = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "\n",
    "# 3.6 Log the trial results for this configuration\n",
    "# Include the trial name, metadata, and other relevant details\n",
    "trial_log_filename = m.log_trial_results(modeling_folder, model_id, experiment_id, trial)\n",
    "\n",
    "# Append the trial log filename to the metadata\n",
    "metadata['trial_logs'].append(trial_log_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prices Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# dataset variables\n",
    "dataset_category = 'time_series'\n",
    "dataset_name = 'market_data'\n",
    "dataset_df = filtered_market_data_df.copy()\n",
    "dataset_metrics_config = metrics_config[dataset_category][dataset_name]\n",
    "\n",
    "market_data_tuples, market_data_training_dfs = fe.generate_time_series_features(\n",
    "        dataset_name,\n",
    "        dataset_df,\n",
    "        dataset_metrics_config,\n",
    "        config,\n",
    "        modeling_config\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metrics_config\n",
    "training_data_tuples, training_data_dfs = fe.generate_time_series_features(\n",
    "        dataset_name,\n",
    "        dataset_df,\n",
    "        dataset_metrics_config,\n",
    "        config,\n",
    "        modeling_config\n",
    "    )\n",
    "\n",
    "training_data_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tuples.append(training_data_tuples)\n",
    "training_data_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through each dataset\n",
    "for dataset_name in metrics_config['time_series']:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wallet Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# dataset variables\n",
    "dataset_category = 'wallet_cohorts'\n",
    "\n",
    "\n",
    "for cohort_name in metrics_config[dataset_category]:\n",
    "\n",
    "    # load configs\n",
    "    dataset_metrics_config = metrics_config[dataset_category][cohort_name]\n",
    "    dataset_config = config['datasets'][dataset_category][cohort_name]\n",
    "    cohort_description = dataset_config['description']\n",
    "\n",
    "    # identify wallets in the cohort\n",
    "    cohort_summary_df = cwm.classify_wallet_cohort(profits_df, dataset_config)\n",
    "    cohort_wallets = cohort_summary_df[cohort_summary_df['in_cohort']==True]['wallet_address']\n",
    "\n",
    "    # If no cohort members were identified, continue\n",
    "    if len(cohort_wallets) == 0:\n",
    "        logger.info(\"No wallets identified as members of cohort '%s'\", cohort_name)\n",
    "        continue\n",
    "\n",
    "    # generate cohort buysell_metrics\n",
    "    cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,config['training_data']['training_period_end'],cohort_wallets)\n",
    "\n",
    "    # generate features from the metrics\n",
    "    dataset_features_df, dataset_tuple = fe.convert_dataset_metrics_to_features(\n",
    "        cohort_metrics_df,\n",
    "        dataset_config,\n",
    "        dataset_metrics_config,\n",
    "        config,\n",
    "        modeling_config\n",
    "    )\n",
    "\n",
    "    # identify columns for logging\n",
    "    dataset_features = dataset_features_df.columns.tolist()\n",
    "    dataset_features.remove('coin_id')\n",
    "\n",
    "    logger.info(\"Generated %s features for %s '%s'.\",\n",
    "                len(dataset_features), dataset_category, cohort_name)\n",
    "    logger.debug('Features generated: %s', dataset_features)\n",
    "\n",
    "    training_data_tuples.append(dataset_tuple)\n",
    "    training_data_dfs.append(dataset_features_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_category = 'coin_facts'\n",
    "# dataset_name = 'coin_metadata'\n",
    "\n",
    "\n",
    "# # load configs\n",
    "# dataset_config = config['datasets'][dataset_category][dataset_name]\n",
    "\n",
    "\n",
    "# # generate features\n",
    "# metadata_df = td.retrieve_metadata_data()\n",
    "# metadata_features_df = td.generate_coin_metadata_features(metadata_df, config)\n",
    "# metadata_features_df.head()\n",
    "\n",
    "# # save flattened output\n",
    "# flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "# flattened_metadata_df, flattened_metadata_filepath = fe.save_flattened_outputs(\n",
    "#     metadata_features_df,\n",
    "#     flattened_output_directory,\n",
    "#     dataset_config['description'],\n",
    "#     config['training_data']['modeling_period_start']\n",
    "# )\n",
    "\n",
    "# # check preprocessed file\n",
    "# preprocessed_metadata_df, preprocessed_metadata_output_path = fe.preprocess_coin_df(\n",
    "#     flattened_metadata_filepath,\n",
    "#     modeling_config,\n",
    "#     dataset_config\n",
    "# )\n",
    "\n",
    "# metadata_tuple = (preprocessed_metadata_output_path.split('preprocessed_outputs/')[1], dataset_config['fill_method'])\n",
    "\n",
    "\n",
    "# training_data_tuples.append(metadata_tuple)\n",
    "# training_data_dfs.append(preprocessed_metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# merge training data\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "training_data_df, merge_logs_df = fe.create_training_data_df(modeling_folder, training_data_tuples)\n",
    "\n",
    "# create the target variable df\n",
    "target_variable_df,_ = fe.create_target_variables_mooncrater(filtered_market_data_df, config['training_data'], modeling_config)\n",
    "\n",
    "# merge the two into the final model input df\n",
    "model_input_df = fe.prepare_model_input_df(training_data_df, target_variable_df, modeling_config['modeling']['target_column'])\n",
    "\n",
    "# split the df into train and test sets\n",
    "X_train, X_test, y_train, y_test = m.split_model_input(\n",
    "    model_input_df,\n",
    "    modeling_config['modeling']['target_column'],\n",
    "    modeling_config['modeling']['train_test_split'],\n",
    "    modeling_config['modeling']['random_state']\n",
    ")\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate the model's performance on the test set\n",
    "metrics = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "\n",
    "# 3.6 Log the experiment results for this configuration\n",
    "m.log_trial_results(modeling_folder, model_id)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your trained model and `feature_names` is a list of your feature names\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = X_train.columns  # Replace with the correct source of your feature names if different\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance (optional)\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Display the feature importance\n",
    "feature_importance_df.sort_values('importance',ascending=False)\n",
    "# feature_importance_df.sort_values('feature',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.unit\n",
    "def test_fe_flatten_date_features_bucketing():\n",
    "    \"\"\"\n",
    "    Unit test for checking the bucketing functionality in flatten_date_features function.\n",
    "\n",
    "    Test Cases:\n",
    "    1. Checks that the function correctly buckets the aggregated metrics based on the specified bucket ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample DataFrame for testing\n",
    "    sample_coin_high_df = pd.DataFrame({\n",
    "        'buyers_new': [10, 20, 30, 40, 50, 60],\n",
    "        'sellers_new': [5, 10, 15, 20, 25, 30]\n",
    "    })\n",
    "\n",
    "    # Sample DataFrame for testing\n",
    "    sample_coin_low_df = pd.DataFrame({\n",
    "        'buyers_new': [0, 3, 4, 15, 33, 12],\n",
    "        'sellers_new': [5, 10, 15, 20, 25, 30]\n",
    "    })\n",
    "\n",
    "\n",
    "    # Sample configuration for metrics with buckets\n",
    "    metrics_config = {\n",
    "        'buyers_new': {\n",
    "            'aggregations': {\n",
    "                'sum': {'buckets': [{'low': 100}, {'medium': 200}, {'high': 'remainder'}]},  # Define the bucket ranges\n",
    "                'mean': {'scaling': 'none'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    flat_features_high = fe.flatten_date_features(sample_coin_high_df, metrics_config)\n",
    "    flat_features_low = fe.flatten_date_features(sample_coin_low_df, metrics_config)\n",
    "\n",
    "    # Test Case 1: Bucketing functionality\n",
    "    assert flat_features_high['buyers_new_sum_bucket'] == 'high'  # Sum = 210\n",
    "    assert flat_features_low['buyers_new_sum_bucket'] == 'low'  # Sum = 67\n",
    "\n",
    "    # Test Case 2: Non-bucketed aggregation still returns raw value\n",
    "    assert flat_features_high['buyers_new_mean'] == 35  # Mean of buyers_new column\n",
    "    assert round(flat_features_low['buyers_new_mean']) == 11  # Mean of buyers_new column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(flat_features_high['buyers_new_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
