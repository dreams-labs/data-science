{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingModuleSource=false\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "from utils import load_config, cw_filter_df, create_progress_bar\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "# load configs\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "prices_metrics_config = metrics_config['time_series']['prices']\n",
    "prices_dataset_config = config['datasets']['time_series']['prices']\n",
    "\n",
    "# retrieve transfers data\n",
    "transfers_df = td.retrieve_transfers_data(\n",
    "    config['training_data']['training_period_start'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end']\n",
    "    )\n",
    "\n",
    "# retrieve and clean prices data\n",
    "prices_df = td.retrieve_prices_data()\n",
    "prices_df,prices_log = td.fill_prices_gaps(prices_df,config['data_cleaning']['max_gap_days'])\n",
    "\n",
    "# compile profits_df\n",
    "profits_df = td.prepare_profits_data(transfers_df, prices_df)\n",
    "profits_df = td.calculate_wallet_profitability(profits_df)\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "dataset_config = config['datasets']['coin_facts']['coin_metadata']\n",
    "\n",
    "\n",
    "\n",
    "# generate features\n",
    "metadata_df = td.retrieve_metadata_data()\n",
    "metadata_features_df = td.generate_coin_metadata_features(metadata_df, config)\n",
    "metadata_features_df.head()\n",
    "\n",
    "# save flattened output\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "flattened_metadata_df, flattened_metadata_filepath = fe.save_flattened_outputs(\n",
    "    metadata_features_df,\n",
    "    flattened_output_directory,\n",
    "    dataset_config['description'],\n",
    "    config['training_data']['modeling_period_start']\n",
    ")\n",
    "\n",
    "# check preprocessed file\n",
    "preprocessed_metadata_df, preprocessed_metadata_output_path = fe.preprocess_coin_df(\n",
    "    flattened_metadata_filepath,\n",
    "    modeling_config,\n",
    "    dataset_config\n",
    ")\n",
    "\n",
    "preprocessed_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prices Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "dataset_config = config['datasets']['time_series']['prices']\n",
    "dataset_metrics_config = metrics_config['time_series']['prices']\n",
    "\n",
    "\n",
    "# generate prices metrics metrics\n",
    "prices_metrics_df,partial_prices_metrics_df = cwm.generate_time_series_metrics(prices_df, config, metrics_config, dataset_key='prices', colname='price')\n",
    "\n",
    "# flatten, save, and preprocess the flattened df\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "\n",
    "flattened_prices_metrics_df = fe.flatten_coin_date_df(\n",
    "    prices_metrics_df,\n",
    "    prices_metrics_config,\n",
    "    config['training_data']['training_period_end']\n",
    ")\n",
    "flattened_prices_metrics_df, flattened_prices_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_prices_metrics_df,\n",
    "    flattened_output_directory,\n",
    "    prices_dataset_config['description'],\n",
    "    config['training_data']['modeling_period_start']\n",
    ")\n",
    "prices_preprocessed_df, prices_preprocessed_filepath = fe.preprocess_coin_df(\n",
    "    flattened_prices_metrics_filepath\n",
    "    ,modeling_config\n",
    "    ,prices_dataset_config\n",
    "    ,prices_metrics_config\n",
    ")\n",
    "\n",
    "\n",
    "prices_preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wallet Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configs\n",
    "cohort_name = 'sharks'\n",
    "sharks_metrics_config = metrics_config['wallet_cohorts'][cohort_name]\n",
    "sharks_dataset_config = config['datasets']['wallet_cohorts'][cohort_name]\n",
    "cohort_description = sharks_dataset_config['description']\n",
    "\n",
    "\n",
    "# identify wallets in the cohort\n",
    "cohort_summary_df = cwm.classify_wallet_cohort(profits_df, sharks_dataset_config)\n",
    "cohort_wallets = cohort_summary_df[cohort_summary_df['in_cohort']==True]['wallet_address']\n",
    "\n",
    "# generate cohort buysell_metrics\n",
    "buysell_metrics_df = cwm.generate_buysell_metrics_df(profits_df,config['training_data']['training_period_end'],cohort_wallets)\n",
    "\n",
    "# flatten, save, and preprocess the flattened df\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "\n",
    "flattened_buysell_metrics_df = fe.flatten_coin_date_df(\n",
    "    buysell_metrics_df,\n",
    "    sharks_metrics_config,\n",
    "    config['training_data']['training_period_end']\n",
    ")\n",
    "flattened_buysell_metrics_df, flattened_buysell_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_buysell_metrics_df,\n",
    "    flattened_output_directory,\n",
    "    cohort_description,\n",
    "    config['training_data']['modeling_period_start']\n",
    ")\n",
    "buysell_preprocessed_df, buysell_preprocessed_filepath = fe.preprocess_coin_df(\n",
    "    flattened_buysell_metrics_filepath,\n",
    "    modeling_config,\n",
    "    sharks_dataset_config,\n",
    "    sharks_metrics_config\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharks_dataset_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the training data df\n",
    "input_filenames = [\n",
    "    (buysell_preprocessed_filepath.split('preprocessed_outputs/')[1], 'drop_records'),\n",
    "    (prices_preprocessed_filepath.split('preprocessed_outputs/')[1], 'drop_records')\n",
    "]\n",
    "training_data_df, merge_logs_df = fe.create_training_data_df(modeling_config['modeling']['modeling_folder'], input_filenames)\n",
    "\n",
    "# create the target variable df\n",
    "target_variable_df,_ = fe.create_target_variables_mooncrater(prices_df, config['training_data'], modeling_config)\n",
    "\n",
    "# merge the two into the final model input df\n",
    "model_input_df = fe.prepare_model_input_df(training_data_df, target_variable_df, modeling_config['modeling']['target_column'])\n",
    "\n",
    "# split the df into train and test sets\n",
    "X_train, X_test, y_train, y_test = m.split_model_input(\n",
    "    model_input_df,\n",
    "    modeling_config['modeling']['target_column'],\n",
    "    modeling_config['modeling']['train_test_split'],\n",
    "    modeling_config['modeling']['random_state']\n",
    ")\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate the model's performance on the test set\n",
    "metrics = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "\n",
    "# 3.6 Log the experiment results for this configuration\n",
    "m.log_trial_results(modeling_folder, model_id)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test fixes etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_modeling_config():\n",
    "    \"\"\"\n",
    "    Returns a mock modeling configuration dictionary.\n",
    "    The configuration includes preprocessing options such as features to drop.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'preprocessing': {\n",
    "            'drop_features': ['feature_to_drop']\n",
    "        }\n",
    "    }\n",
    "\n",
    "def mock_metrics_config():\n",
    "    \"\"\"\n",
    "    Returns a mock metrics configuration dictionary.\n",
    "    This configuration includes settings for scaling different features.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'feature_1': {\n",
    "            'aggregations': {\n",
    "                'sum': {'scaling': 'standard'},\n",
    "                'max': {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def mock_input_df():\n",
    "    \"\"\"\n",
    "    Creates a mock DataFrame and saves it as a CSV for testing.\n",
    "    The CSV file is saved in the 'tests/test_modeling/outputs/flattened_outputs' directory.\n",
    "\n",
    "    Returns:\n",
    "    - input_path: Path to the CSV file.\n",
    "    - df: Original mock DataFrame.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'feature_1_sum': [1, 2, 3],\n",
    "        'feature_to_drop': [10, 20, 30],\n",
    "        'feature_3': [100, 200, 300]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    input_path = 'temp/mock_input.csv'\n",
    "    df.to_csv(input_path, index=False)\n",
    "    return input_path, df\n",
    "\n",
    "mock_modeling_config = mock_modeling_config()\n",
    "mock_metrics_config = mock_metrics_config()\n",
    "mock_input_df = mock_input_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests that the preprocess_coin_df function correctly applies scaling to the specified features.\n",
    "\n",
    "Steps:\n",
    "- Preprocesses the mock DataFrame by applying standard scaling to 'feature_1'.\n",
    "- Asserts that the column is scaled correctly.\n",
    "- Cleans up the test files after execution.\n",
    "\"\"\"\n",
    "input_path, original_df = mock_input_df\n",
    "dataset_config = {}\n",
    "\n",
    "# Call the function\n",
    "output_df, output_path = fe.preprocess_coin_df(input_path, mock_modeling_config, dataset_config, mock_metrics_config)\n",
    "\n",
    "# Check that 'feature_1' is scaled (mean should be near 0 and std should be near 1)\n",
    "scaled_column = output_df['feature_1_sum']\n",
    "assert abs(scaled_column.mean()) < 1e-6, \"Standard scaling not applied correctly to 'feature_1_sum'.\"\n",
    "assert abs(np.std(scaled_column) - 1) < 1e-6, \"Standard scaling not applied correctly to 'feature_1_sum'.\"\n",
    "\n",
    "# Cleanup (remove the test files)\n",
    "os.remove(output_path)\n",
    "os.remove(input_path)\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
