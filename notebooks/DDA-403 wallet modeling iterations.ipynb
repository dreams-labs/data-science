{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import datetime,timedelta\n",
    "import json\n",
    "import warnings\n",
    "import yaml\n",
    "from typing import Dict,Union,List,Any,Tuple\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "from pyxirr import xirr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"MallocStackLogging\")\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "import wallet_modeling.wallet_orchestrator as wo\n",
    "import wallet_modeling.wallet_training_data as wtd\n",
    "import wallet_modeling.wallet_modeling as wm\n",
    "import wallet_features.wallet_features as wf\n",
    "import wallet_features.wallet_coin_features as wcf\n",
    "import wallet_features.wallet_coin_date_features as wcdf\n",
    "from wallet_modeling.wallets_config_manager import WalletsConfig\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp, wo, wtd, wm, wf, wcf, wcdf]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "wallets_config = WalletsConfig.load_from_yaml('../config/wallets_config.yaml')\n",
    "wallets_metrics_config = u.load_config('../config/wallets_metrics_config.yaml')\n",
    "wallets_features_config = yaml.safe_load(Path('../config/wallets_features_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "wallets_metrics_config = u.load_config('../config/wallets_metrics_config.yaml')\n",
    "wallets_features_config = yaml.safe_load(Path('../config/wallets_features_config.yaml').read_text(encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_config['training_data']['validation_period_start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Retrieve datasets\n",
    "profits_df,market_data_df = wo.retrieve_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Define wallet cohort after cleaning\n",
    "training_wallet_metrics_df,wallet_cohort = wo.define_wallet_cohort(profits_df,market_data_df)\n",
    "\n",
    "# Generate profits_df for all training windows and the modeling period\n",
    "training_profits_df, training_windows_profits_dfs, modeling_profits_df, validation_profits_df = wo.split_profits_df(profits_df,\n",
    "                                                                               market_data_df,wallet_cohort)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [importlib.reload(module) for module in modules]\n",
    "# wallets_config.reload()\n",
    "\n",
    "# # Generate features for the full training dataset\n",
    "# training_wallet_features_df = wf.calculate_wallet_features(training_profits_df, market_data_df, wallet_cohort)\n",
    "\n",
    "# # Define the full feature set by appending a suffix for each window\n",
    "# training_data_df = training_wallet_features_df.add_suffix(\"_all_windows\")\n",
    "\n",
    "# # Generate features for each window\n",
    "# for i, window_profits_df in enumerate(training_windows_profits_dfs, 1):\n",
    "#     # Generate the features\n",
    "#     window_wallet_features_df = wf.calculate_wallet_features(window_profits_df, market_data_df, wallet_cohort)\n",
    "\n",
    "#     # Add column suffix and join to training_data_df\n",
    "#     window_wallet_features_df = window_wallet_features_df.add_suffix(f'_w{i}')\n",
    "#     training_data_df = training_data_df.join(window_wallet_features_df, how='left')\n",
    "\n",
    "\n",
    "# training_data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market timing features dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "wallets_metrics_config = u.load_config('../config/wallets_metrics_config.yaml')\n",
    "wallets_features_config = yaml.safe_load(Path('../config/wallets_features_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "\n",
    "# Market data: add indicators\n",
    "market_indicators_df = ind.generate_time_series_indicators(market_data_df,\n",
    "                                                           wallets_metrics_config['time_series']['market_data'],\n",
    "                                                           'coin_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "wallets_metrics_config = u.load_config('../config/wallets_metrics_config.yaml')\n",
    "wallets_features_config = yaml.safe_load(Path('../config/wallets_features_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "# add timing offset features\n",
    "market_timing_df = wcdf.calculate_offsets(market_indicators_df,wallets_features_config)\n",
    "market_timing_df,relative_change_columns = wcdf.calculate_relative_changes(market_timing_df,wallets_features_config)\n",
    "\n",
    "relative_change_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "wallets_metrics_config = u.load_config('../config/wallets_metrics_config.yaml')\n",
    "wallets_features_config = yaml.safe_load(Path('../config/wallets_features_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "\n",
    "wallet_timing_features_df = wcf.generate_all_timing_features(\n",
    "    training_profits_df,\n",
    "    market_timing_df,\n",
    "    relative_change_columns,\n",
    "    wallets_config['features']['timing_metrics_min_transaction_size'],\n",
    ")\n",
    "\n",
    "wallet_timing_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_timing_features_df = wallet_timing_features_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "wallets_metrics_config = u.load_config('../config/wallets_metrics_config.yaml')\n",
    "wallets_features_config = yaml.safe_load(Path('../config/wallets_features_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "\n",
    "wallet_timing_features_df = wcf.generate_all_timing_features(\n",
    "    window_profits_df,\n",
    "    market_timing_df,\n",
    "    relative_change_columns,\n",
    "    wallets_config['features']['timing_metrics_min_transaction_size'],\n",
    ")\n",
    "\n",
    "wallet_timing_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "def generate_all_timing_features(\n",
    "    profits_df,\n",
    "    market_timing_df,\n",
    "    relative_change_columns,\n",
    "    min_transaction_size=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate timing features for multiple market metric columns.\n",
    "\n",
    "    Args:\n",
    "        profits_df (pd.DataFrame): DataFrame with columns [coin_id, date, wallet_address, usd_net_transfers]\n",
    "        market_timing_df (pd.DataFrame): DataFrame with market timing metrics indexed by (coin_id, date)\n",
    "        relative_change_columns (list): List of column names from market_timing_df to analyze\n",
    "        min_transaction_size (float): Minimum absolute USD value of transaction to consider\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting timing feature generation for columns: %s\", relative_change_columns)\n",
    "\n",
    "    if not relative_change_columns:\n",
    "        return pd.DataFrame(index=profits_df['wallet_address'].unique())\n",
    "\n",
    "    # Filter and merge data\n",
    "    filtered_profits = profits_df[\n",
    "        abs(profits_df['usd_net_transfers']) >= min_transaction_size\n",
    "    ].copy()\n",
    "\n",
    "    timing_profits_df = filtered_profits.merge(\n",
    "        market_timing_df[relative_change_columns + ['coin_id', 'date']],\n",
    "        on=['coin_id', 'date'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Split into buys and sells\n",
    "    buys_df = timing_profits_df[timing_profits_df['usd_net_transfers'] > 0]\n",
    "    sells_df = timing_profits_df[timing_profits_df['usd_net_transfers'] < 0]\n",
    "\n",
    "    buy_features = {}\n",
    "    sell_features = {}\n",
    "\n",
    "    # Process buys\n",
    "    for col in relative_change_columns:\n",
    "        # Simple mean\n",
    "        buy_features[f\"{col}_buy_mean\"] = buys_df.groupby('wallet_address')[col].mean()\n",
    "        # Weighted mean\n",
    "        buy_features[f\"{col}_buy_weighted\"] = (\n",
    "            buys_df.groupby('wallet_address')\n",
    "            .apply(lambda x: np.average(x[col], weights=abs(x['usd_net_transfers'])))\n",
    "        )\n",
    "\n",
    "    # Process sells\n",
    "    for col in relative_change_columns:\n",
    "        # Simple mean\n",
    "        sell_features[f\"{col}_sell_mean\"] = sells_df.groupby('wallet_address')[col].mean()\n",
    "        # Weighted mean\n",
    "        sell_features[f\"{col}_sell_weighted\"] = (\n",
    "            sells_df.groupby('wallet_address')\n",
    "            .apply(lambda x: np.average(x[col], weights=abs(x['usd_net_transfers'])))\n",
    "        )\n",
    "\n",
    "    # Combine results\n",
    "    result = pd.concat([\n",
    "        pd.DataFrame(buy_features),\n",
    "        pd.DataFrame(sell_features)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Ensure all wallets are included\n",
    "    all_wallets = pd.DataFrame(index=filtered_profits['wallet_address'].unique())\n",
    "    result = all_wallets.join(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_all_timing_features(\n",
    "    window_profits_df,\n",
    "    market_timing_df,\n",
    "    relative_change_columns,\n",
    "    wallets_config['features']['timing_metrics_min_transaction_size'],\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_timing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = market_timing_df['volume_sma_7_vs_lead_30']\n",
    "cutoff = 0.01\n",
    "\n",
    "lower_bound = np.percentile(data, cutoff * 100)\n",
    "upper_bound = np.percentile(data, (1 - cutoff) * 100)\n",
    "\n",
    "np.clip(data, lower_bound, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_timing_df['volume_sma_7_vs_lead_30_w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_timing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Generate additional features\n",
    "# ----------------------------\n",
    "# Retrieve the buy numbers for wallets in the cohort\n",
    "buyer_numbers_df = wcf.retrieve_buyer_numbers()\n",
    "\n",
    "# Append buyer numbers to the merged_df\n",
    "buyer_averages_df = buyer_numbers_df.groupby('wallet_id').mean('buyer_number')\n",
    "buyer_averages_df.columns = ['average_buyer_number']\n",
    "training_data_df = training_data_df.join(buyer_averages_df)\n",
    "training_data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge to modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Clean inactive wallets from modeling period data\n",
    "modeling_wallets_df = wo.filter_modeling_period_wallets(modeling_profits_df)\n",
    "\n",
    "# Generate target variables\n",
    "target_vars_df = wm.generate_target_variables(modeling_wallets_df)\n",
    "\n",
    "# Merge training data and target variables\n",
    "modeling_df = training_data_df.join(target_vars_df[wallets_config['modeling']['target_variable']],\n",
    "                                    how='inner')\n",
    "\n",
    "modeling_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# make df\n",
    "df = modeling_df.copy()\n",
    "\n",
    "if wallets_config['modeling']['drop_columns']:\n",
    "    # Get list of columns to drop from config\n",
    "    columns_to_drop = wallets_config['modeling']['drop_columns']\n",
    "\n",
    "    # Only drop columns that actually exist in the DataFrame\n",
    "    existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "    # Drop the columns if any exist\n",
    "    if existing_columns:\n",
    "        df = df.drop(columns=existing_columns)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your dataframe is called 'df'\n",
    "# Separate features and target\n",
    "X = df.drop(wallets_config['modeling']['target_variable'], axis=1)  # dropping both return columns\n",
    "y = df[wallets_config['modeling']['target_variable']]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_features = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n",
    "\n",
    "# Define the gradient boosting regressor with key parameters\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    max_features=1.0,\n",
    "    min_samples_leaf=0.005,\n",
    "    min_samples_split=0.02,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', gbr)\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "model=pipeline.named_steps['regressor']  # Pass the actual model object\n",
    "# model=grid_search.best_estimator_.named_steps['regressor']\n",
    "\n",
    "# Example usage with your existing code:\n",
    "evaluation = wm.evaluate_regression_model(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    model=model,\n",
    "    feature_names=X.columns.tolist()\n",
    ")\n",
    "\n",
    "# Print summary report\n",
    "print(evaluation['summary_report'])\n",
    "\n",
    "# Access specific metrics\n",
    "print(f\"RÂ² Score: {evaluation['r2']:.3f}\")\n",
    "\n",
    "# The figure can be displayed or saved\n",
    "if evaluation['figures'] is not None:\n",
    "    plt.show()  # or evaluation['figures'].savefig('model_evaluation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate validation period wallet metrics\n",
    "validation_profits_df = wcf.add_cash_flow_transfers_logic(validation_profits_df)\n",
    "validation_wallets_df = wf.calculate_wallet_level_metrics(validation_profits_df)\n",
    "\n",
    "# Attach validation period performance to modeling period scores\n",
    "validation_df = pd.DataFrame()\n",
    "validation_df['wallet_address'] = X_test.index.values\n",
    "validation_df['score'] = y_pred\n",
    "validation_df['score_rounded'] = np.ceil(validation_df['score']*20)/20\n",
    "validation_df = validation_df.set_index('wallet_address')\n",
    "validation_df = validation_df.join(validation_wallets_df,how='left')\n",
    "\n",
    "# Group wallets by score bucket and assess performance\n",
    "grouped_val = validation_df.groupby('score_rounded').agg(\n",
    "    wallets=('score','count'),\n",
    "    total_invested=('invested','sum'),\n",
    "    total_net_gain=('net_gain','sum'),\n",
    "    median_invested=('invested','median'),\n",
    "    median_net_gain=('net_gain','median'),\n",
    ")\n",
    "grouped_val['total_return'] = grouped_val['total_net_gain']/grouped_val['total_invested']\n",
    "grouped_val['median_return'] = grouped_val['median_net_gain']/grouped_val['median_invested']\n",
    "grouped_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_date = pd.to_datetime(wallets_config['training_data']['validation_period_start'])\n",
    "validation_start_df = validation_profits_df[validation_profits_df['date']==validation_start_date].copy()\n",
    "validation_start_df = validation_start_df[['coin_id','wallet_address','usd_balance']]\n",
    "validation_start_df = validation_start_df[validation_start_df['usd_balance']>0]\n",
    "validation_start_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def safe_weighted_average(scores, weights):\n",
    "    \"\"\"Calculate weighted average, handling zero weights safely\"\"\"\n",
    "    if np.sum(weights) == 0:\n",
    "        return np.mean(scores) if len(scores) > 0 else 0\n",
    "    return np.sum(scores * weights) / np.sum(weights)\n",
    "\n",
    "def rank_coins_by_wallet_scores(validation_start_df, wallet_scores_df):\n",
    "    \"\"\"\n",
    "    Create multiple ranking metrics for coins based on wallet scores and balances.\n",
    "    \"\"\"\n",
    "    # Merge wallet scores with balance data\n",
    "    analysis_df = validation_start_df.merge(\n",
    "        wallet_scores_df[['score']],\n",
    "        left_on='wallet_address',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Ensure no negative balances and fill any NA scores\n",
    "    analysis_df['usd_balance'] = analysis_df['usd_balance'].clip(lower=0)\n",
    "    analysis_df['score'] = analysis_df['score'].fillna(0)\n",
    "\n",
    "    # Calculate weighted average score differently\n",
    "    weighted_scores = analysis_df.groupby('coin_id').apply(\n",
    "        lambda x: safe_weighted_average(x['score'].values, x['usd_balance'].values)\n",
    "    ).reset_index()\n",
    "    weighted_scores.columns = ['coin_id', 'weighted_avg_score']\n",
    "\n",
    "    # Top wallet concentration\n",
    "    high_score_threshold = wallet_scores_df['score'].quantile(0.8)\n",
    "    top_wallet_metrics = analysis_df[analysis_df['score'] >= high_score_threshold].groupby('coin_id').agg({\n",
    "        'usd_balance': 'sum',\n",
    "        'wallet_address': 'count'\n",
    "    }).reset_index()\n",
    "    top_wallet_metrics.columns = ['coin_id', 'top_wallet_balance', 'top_wallet_count']\n",
    "\n",
    "    # Calculate total metrics\n",
    "    total_metrics = analysis_df.groupby('coin_id').agg({\n",
    "        'usd_balance': 'sum',\n",
    "        'wallet_address': 'count',\n",
    "        'score': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    total_metrics.columns = ['coin_id', 'total_balance', 'total_wallets',\n",
    "                           'mean_score', 'score_std', 'score_count']\n",
    "\n",
    "    # Combine metrics\n",
    "    final_rankings = pd.merge(weighted_scores, top_wallet_metrics, on='coin_id', how='left')\n",
    "    final_rankings = pd.merge(final_rankings, total_metrics, on='coin_id', how='left')\n",
    "\n",
    "    # Fill NaN values\n",
    "    fill_columns = ['top_wallet_balance', 'top_wallet_count', 'score_std']\n",
    "    final_rankings[fill_columns] = final_rankings[fill_columns].fillna(0)\n",
    "\n",
    "    # Calculate percentages safely\n",
    "    final_rankings['top_wallet_balance_pct'] = np.where(\n",
    "        final_rankings['total_balance'] > 0,\n",
    "        final_rankings['top_wallet_balance'] / final_rankings['total_balance'],\n",
    "        0\n",
    "    )\n",
    "\n",
    "    final_rankings['top_wallet_count_pct'] = np.where(\n",
    "        final_rankings['total_wallets'] > 0,\n",
    "        final_rankings['top_wallet_count'] / final_rankings['total_wallets'],\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Create composite score\n",
    "    final_rankings['composite_score'] = (\n",
    "        final_rankings['weighted_avg_score'] * 0.4 +\n",
    "        final_rankings['top_wallet_balance_pct'] * 0.3 +\n",
    "        final_rankings['top_wallet_count_pct'] * 0.3\n",
    "    )\n",
    "\n",
    "    # Additional metrics\n",
    "    final_rankings['avg_wallet_balance'] = final_rankings['total_balance'] / final_rankings['total_wallets']\n",
    "    final_rankings['score_confidence'] = 1 - (1 / np.sqrt(final_rankings['score_count'] + 1))  # Added +1 to avoid division by zero\n",
    "\n",
    "    # Filter for minimum activity\n",
    "    min_wallets = 5\n",
    "    min_balance = 10000\n",
    "    final_rankings = final_rankings[\n",
    "        (final_rankings['total_wallets'] >= min_wallets) &\n",
    "        (final_rankings['total_balance'] >= min_balance)\n",
    "    ]\n",
    "\n",
    "    # Set index\n",
    "    final_rankings=final_rankings.set_index('coin_id')\n",
    "\n",
    "    # Sort by composite score\n",
    "    final_rankings = final_rankings.sort_values('composite_score', ascending=False)\n",
    "\n",
    "\n",
    "    return final_rankings\n",
    "\n",
    "# Example usage\n",
    "wallet_scores_df = pd.DataFrame({'score': y_pred}, index=validation_df.index)\n",
    "coin_rankings = rank_coins_by_wallet_scores(validation_start_df, wallet_scores_df)\n",
    "\n",
    "# Display summary statistics to verify calculations\n",
    "coin_rankings.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_date = pd.to_datetime(wallets_config['training_data']['validation_period_start'])\n",
    "validation_end_date = pd.to_datetime(wallets_config['training_data']['validation_period_end'])\n",
    "\n",
    "# Get prices at start and end dates\n",
    "start_prices = market_data_df[market_data_df['date'] == validation_start_date].set_index('coin_id')['price']\n",
    "end_prices = market_data_df[market_data_df['date'] == validation_end_date].set_index('coin_id')['price']\n",
    "\n",
    "# Combine into a single dataframe\n",
    "validation_prices_df = pd.DataFrame({\n",
    "    'starting_price': start_prices,\n",
    "    'ending_price': end_prices\n",
    "})\n",
    "\n",
    "# Calculate returns\n",
    "validation_prices_df['coin_return'] = (validation_prices_df['ending_price'] / validation_prices_df['starting_price']) - 1\n",
    "\n",
    "# Clean up any missing values\n",
    "validation_prices_df = validation_prices_df.dropna()\n",
    "validation_prices_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get market cap data\n",
    "start_market_cap = market_data_df[market_data_df['date'] == validation_start_date].set_index('coin_id')['market_cap']\n",
    "start_market_cap_filled = market_data_df[market_data_df['date'] == validation_start_date].set_index('coin_id')['market_cap_filled']\n",
    "\n",
    "# Create market cap dataframe\n",
    "market_cap_df = pd.DataFrame({\n",
    "    'market_cap': start_market_cap,\n",
    "    'market_cap_filled': start_market_cap_filled\n",
    "})\n",
    "\n",
    "market_cap_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_performance_df = coin_rankings.join(validation_prices_df['coin_return'],how='left')\n",
    "coin_performance_df = coin_performance_df.join(market_cap_df,how='left')\n",
    "coin_performance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metrics_performance(df):\n",
    "   \"\"\"\n",
    "   For each metric in the dataframe, analyze return performance of top 10 coins sorted by that metric\n",
    "   \"\"\"\n",
    "   results = {}\n",
    "\n",
    "   # Skip these columns as they're not useful ranking metrics\n",
    "   skip_columns = ['coin_return', 'coin_id','market_cap','market_cap_filled']\n",
    "\n",
    "   # Calculate performance for each metric\n",
    "   for column in df.columns:\n",
    "       if column not in skip_columns:\n",
    "           # Sort by metric and get top 10 coins\n",
    "           top_10 = df.sort_values(column, ascending=False).head(10)\n",
    "\n",
    "           # Calculate average return\n",
    "           avg_return = top_10['coin_return'].mean()\n",
    "           median_return = top_10['coin_return'].median()\n",
    "           min_return = top_10['coin_return'].min()\n",
    "           max_return = top_10['coin_return'].max()\n",
    "\n",
    "           results[column] = {\n",
    "               'mean_return': avg_return,\n",
    "               'median_return': median_return,\n",
    "               'min_return': min_return,\n",
    "               'max_return': max_return\n",
    "           }\n",
    "\n",
    "   # Convert to dataframe\n",
    "   results_df = pd.DataFrame(results).T\n",
    "\n",
    "   # Sort by mean return\n",
    "   results_df = results_df.sort_values('mean_return', ascending=False)\n",
    "\n",
    "   return results_df\n",
    "\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "analyze_df = coin_performance_df[\n",
    "    (coin_performance_df['market_cap_filled']<=5000000)\n",
    "    & (coin_performance_df['market_cap_filled']>1000000)\n",
    "]\n",
    "performance_by_metric = analyze_metrics_performance(analyze_df)\n",
    "performance_by_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_df.sort_values('top_wallet_count',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_df.sort_values('top_wallet_count',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_notification(sound_file_path=\"/Users/jeremymeadow/DreamsData/Local/assets/sounds/mixkit-alert-bells-echo-765.wav\"):\n",
    "    \"\"\"\n",
    "    Play a notification sound from a local audio file using pygame.\n",
    "\n",
    "    Parameters:\n",
    "    sound_file_path (str): Path to the sound file (supports .mp3, .wav, etc.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(sound_file_path)\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        # Wait for the sound to finish playing\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            pygame.time.Clock().tick(10)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error playing sound: {e}\")\n",
    "    finally:\n",
    "        pygame.mixer.quit()\n",
    "\n",
    "\n",
    "play_notification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your dataframe is called 'df'\n",
    "# Separate features and target\n",
    "X = df.drop(wallets_config['modeling']['target_variable'], axis=1)\n",
    "y = df[wallets_config['modeling']['target_variable']]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_features = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [2, 3, 4],\n",
    "    'regressor__subsample': [0.8, 0.9],\n",
    "    'regressor__min_samples_split': [0.01, 0.02, 0.03],\n",
    "    'regressor__min_samples_leaf': [0.005, 0.01, 0.015],\n",
    "    'regressor__max_features': [0.8, 0.9, 1.0]  # This replaces colsample_bytree\n",
    "}\n",
    "\n",
    "# Create grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", np.sqrt(-grid_search.best_score_))  # RMSE\n",
    "\n",
    "# Make predictions with best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate and print test set RMSE\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test set RMSE: {test_rmse}\")\n",
    "\n",
    "# If you want to examine feature importance\n",
    "best_model = grid_search.best_estimator_.named_steps['regressor']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': numeric_features,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_notification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=pipeline.named_steps['regressor']\n",
    "feature_names=X.columns.tolist()\n",
    "\n",
    "\n",
    "importances = model.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# drop_columns = importance_df[importance_df['Importance']<=0.005]['Feature']\n",
    "# drop_columns\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "\n",
    "\n",
    "# Split out modeling and training records to calculate return separately\n",
    "modeling_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['modeling_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['modeling_period_end']))\n",
    "]\n",
    "modeling_performance_df = wf.calculate_wallet_investment_return(modeling_df)\n",
    "\n",
    "\n",
    "training_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['training_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['training_period_end']))\n",
    "]\n",
    "training_performance_df = wf.calculate_wallet_investment_return(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_invested = 10000\n",
    "filtered_df = training_performance_df[training_performance_df['invested']>=min_invested]\n",
    "print(training_performance_df.shape)\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*5)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*5)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_correlation_matrix(df):\n",
    "    \"\"\"\n",
    "    Create and visualize a correlation matrix for the given DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Correlation matrix\n",
    "    \"\"\"\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix,\n",
    "                annot=True,  # Show correlation values\n",
    "                cmap='coolwarm',  # Color scheme from red (negative) to blue (positive)\n",
    "                vmin=-1, vmax=1,  # Fix the scale\n",
    "                center=0,  # Center the colormap at 0\n",
    "                fmt='.2f')  # Round to 2 decimal places\n",
    "\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "# create_correlation_matrix(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.set_index('coin_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_analysis_df = market_data_df.copy().set_index('coin_id')\n",
    "\n",
    "start_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_start'])]\n",
    "end_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_end'])]\n",
    "\n",
    "# coin_modeling_returns_df = start_prices.join(end_prices)\n",
    "\n",
    "    # (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['modeling_period_start'])) &\n",
    "    # (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['modeling_period_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coin returns during modeling period\n",
    "coin_modeling_returns_df = start_prices[['price']].join(end_prices[['price']],lsuffix='_start',rsuffix='_end')\n",
    "coin_modeling_returns_df['coin_modeling_return'] = coin_modeling_returns_df['price_end']/coin_modeling_returns_df['price_start']\n",
    "coin_modeling_returns_df[\"coin_modeling_percentile_return\"] = coin_modeling_returns_df[\"coin_modeling_return\"].rank(ascending=True, pct=True)\n",
    "\n",
    "coin_modeling_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wallet ending balances\n",
    "min_end_balance = 1000\n",
    "\n",
    "# Calculate period end balance for each coin-wallet pair\n",
    "end_balances_df = adj_profits_df[adj_profits_df['date']==pd.to_datetime(config['training_data']['training_period_end'])]\n",
    "end_balances_df = end_balances_df[end_balances_df['usd_net_transfers']<=-min_end_balance]\n",
    "end_balances_df['usd_balance'] = end_balances_df['usd_net_transfers'].abs()\n",
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]\n",
    "end_balances_df = end_balances_df.set_index(['coin_id','wallet_address'])\n",
    "end_balances_df.head()\n",
    "\n",
    "# Add wallet performance metrics\n",
    "end_balances_df = end_balances_df.join(performance_df,on='wallet_address')\n",
    "end_balances_df = end_balances_df[end_balances_df['return_training'].notna()]\n",
    "\n",
    "end_balances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_return_column = 'return_training'\n",
    "\n",
    "# Assess average wallet return during training period\n",
    "coin_wallet_performance = pd.DataFrame(end_balances_df.reset_index().groupby('coin_id',observed=True)[wallet_return_column].mean())\n",
    "coin_wallet_performance.columns = ['avg_wallet_training_return']\n",
    "\n",
    "coin_wallet_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coin_return_column = 'coin_modeling_return'\n",
    "coin_return_column = 'coin_modeling_percentile_return'\n",
    "\n",
    "wallet_forecast_df = coin_modeling_returns_df[[coin_return_column]].join(coin_wallet_performance)\n",
    "wallet_forecast_df[coin_return_column].corr(wallet_forecast_df['avg_wallet_training_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*10)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*10)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_performance_df = training_performance_df\n",
    "wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "wallet_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_performance_df.sample(10)\n",
    "\n",
    "wallet_performance_df[wallet_performance_df['invested']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0xca6cfaa7d61371310d84b63a4ca90cbf7883a9db'\n",
    "\n",
    "df = wallets_df_filtered.loc[w]\n",
    "\n",
    "# print(xirr(df.index.get_level_values('date'), df['usd_net_transfers']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_profits_df[adj_profits_df['wallet_address']==w]\n",
    "profits_df[profits_df['wallet_address']==w].sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df[adj_profits_df['wallet_address']==w].sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wallets_xirr(profits_df, min_wallet_volume):\n",
    "    \"\"\"\n",
    "    Calculates the XIRR of each wallet based on their cash flows across all coins they've\n",
    "    interacted with in profits_df.\n",
    "\n",
    "    Parameters:\n",
    "    - profits_df (pd.DataFrame): shows daily coin-wallet transfers in USD\n",
    "    - min_wallet_volume (int): wallets with less than this total USD volume will be excluded\n",
    "\n",
    "    Returns:\n",
    "    - xirr_df (pd.DataFrame): shows the XIRR of each wallet over the provided transactions\n",
    "    \"\"\"\n",
    "    logger.info('Beginning XIRR calculation sequence...')\n",
    "\n",
    "    # 1. Summarize cash flows on a wallet level\n",
    "    # -----------------------------------------\n",
    "    # Sum cash flows on a wallet level\n",
    "    wallets_df = pd.DataFrame(profits_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "    # 2. Filter wallets on data quality\n",
    "    # ---------------------------------\n",
    "    # Identify wallets with no transactions\n",
    "    wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "    low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "    # Remove transactionless wallets\n",
    "    wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "    logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "    # Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "    wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "        lambda x: (x > 0).any() and (x < 0).any()\n",
    "    )\n",
    "    wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "    # Filter wallet addresses that do not have both positive and negative transfers\n",
    "    wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "    logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "    # 3. Calculate XIRR\n",
    "    # -----------------\n",
    "    # Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "    start_time = time.time()\n",
    "    logger.info('Calculating XIRR values...')\n",
    "    xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "        lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    "    )\n",
    "    logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    xirr_df = pd.DataFrame(xirr_results)\n",
    "    xirr_df.columns = ['xirr']\n",
    "\n",
    "    # Fill empty values with 0s\n",
    "    xirr_df = xirr_df.fillna(0)\n",
    "\n",
    "\n",
    "    return xirr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 1\n",
    "\n",
    "# 1. Summarize cash flows on a wallet level\n",
    "# -----------------------------------------\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(modeling_df.copy().groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "# 2. Filter wallets on data quality\n",
    "# ---------------------------------\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "# Filter wallet addresses that do not have both positive and negative transfers\n",
    "wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "# 3. Calculate XIRR\n",
    "# -----------------\n",
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "start_time = time.time()\n",
    "logger.info('Calculating XIRR values...')\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Fill empty values with 0s\n",
    "xirr_df = xirr_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: -df['usd_net_transfers'].sum()/df['usd_net_transfers'].cumsum().max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df.loc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wallet_metrics(group):\n",
    "    cumsum = group['usd_net_transfers'].cumsum()\n",
    "    invested = cumsum.max()\n",
    "    net_gain = group['usd_net_transfers'].sum()\n",
    "\n",
    "    return pd.Series({\n",
    "        'invested': invested,\n",
    "        'net_gain': net_gain,\n",
    "        'return': net_gain/invested if invested != 0 else np.nan\n",
    "    })\n",
    "\n",
    "# Calculate metrics for all wallets at once\n",
    "results = wallets_df_filtered.groupby(level='wallet_address').apply(wallet_metrics)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 10000\n",
    "\n",
    "# Calculate XIRR\n",
    "training_xirr_df = calculate_wallets_xirr(training_df,min_wallet_volume)\n",
    "modeling_xirr_df = calculate_wallets_xirr(modeling_df,min_wallet_volume=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate percentiles\n",
    "xirr_df[\"training_xirr_percentile\"] = xirr_df[\"training_xirr\"].rank(ascending=True, pct=True)\n",
    "xirr_df[\"modeling_xirr_percentile\"] = xirr_df[\"modeling_xirr\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "xirr_df['training_xirr_decile'] = np.ceil(xirr_df['training_xirr_percentile']*10)\n",
    "xirr_df['modeling_xirr_decile'] = np.ceil(xirr_df['modeling_xirr_percentile']*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    xirr_df['training_xirr_decile'],\n",
    "    xirr_df['modeling_xirr_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = training_xirr_df.rename(columns={'xirr': 'training_xirr'}).join(\n",
    "    modeling_xirr_df.rename(columns={'xirr': 'modeling_xirr'}),\n",
    "    how='inner'\n",
    ").fillna({'modeling_xirr': 0})\n",
    "\n",
    "\n",
    "\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df['training_xirr_percentile'].corr(xirr_df['modeling_xirr_percentile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(training_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "\n",
    "# Filter wallet addresses that do not meet the condition\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "logger.info('Found %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = '0x036783df7aec54b5dfca9e1f870577bbcca95481'\n",
    "# wallets_df.loc[w]\n",
    "\n",
    "# profits_df[profits_df['wallet_address']==w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIRR sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0x0000000000000000000000000000000000000014'\n",
    "\n",
    "dates = wallets_df.loc[w].index.values\n",
    "cash_flows = wallets_df.loc[w]['usd_net_transfers']\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(xirr_results.shape)\n",
    "xirr_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xirr(dates,cash_flows)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '77e2cf4b-d18a-4026-a2f2-f083f48fe1be'\n",
    "w = '0xaff2943cfe3e95f66142a1729079418d78e42236'\n",
    "\n",
    "# u.cw_filter_df(training_df,c,w)\n",
    "\n",
    "df = u.cw_filter_df(training_df,c,w)\n",
    "df = df.sort_values('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['date']\n",
    "cash_flows = df['usd_net_transfers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxirr import xirr\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fractions = (np.datetime64(dates) - np.datetime64(dates[0])).astype('timedelta64[D]') / np.timedelta64(1, 'Y')\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     select cwp.wallet_address\n",
    "#     ,cwp.coin_id\n",
    "#     ,cwp.date\n",
    "#     ,round(cwp.usd_net_transfers) as usd_net_transfers\n",
    "#     ,round(cwp.usd_balance) as usd_balance\n",
    "#     ,round(cwp.usd_net_transfers/cmd.price) as token_transfers\n",
    "#     ,round(cwp.usd_balance/cmd.price) as token_balance\n",
    "#     ,cmd.price\n",
    "#     from wallets w\n",
    "#     join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#     join core.coin_wallet_profits cwp on cwp.wallet_address = wc.wallet_address\n",
    "#         and cwp.coin_id = wc.coin_id\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = cwp.coin_id\n",
    "#         and cmd.date = cwp.date\n",
    "#     order by 1,2,3\n",
    "#     '''\n",
    "# transfers_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert wallet_address to categorical, store the mapping, and convert the column to int32\n",
    "# wallet_address_categorical = transfers_df['wallet_address'].astype('category')\n",
    "# # wallet_address_mapping = wallet_address_categorical.cat.categories\n",
    "# # transfers_df['wallet_address'] = wallet_address_categorical.cat.codes.astype('uint32')\n",
    "\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# transfers_df['coin_id'] = transfers_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_net_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'price', 'float32')\n",
    "\n",
    "# print(transfers_df.info())\n",
    "# print(u.df_mem(transfers_df))\n",
    "# transfers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     ,coins as (\n",
    "#         select wc.coin_id\n",
    "#         from wallets w\n",
    "#         join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#         group by 1\n",
    "#     )\n",
    "\n",
    "#     select cmd.coin_id\n",
    "#     ,cmd.date\n",
    "#     ,cmd.price\n",
    "#     ,cmd.market_cap\n",
    "#     from coins c\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = c.coin_id\n",
    "#     order by 1,2\n",
    "#     '''\n",
    "# prices_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# prices_df['coin_id'] = prices_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# prices_df = u.safe_downcast(prices_df, 'price', 'float32')\n",
    "# prices_df = u.safe_downcast(prices_df, 'market_cap', 'int32')\n",
    "\n",
    "# print(prices_df.info())\n",
    "# print(u.df_mem(prices_df))\n",
    "# prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Create sample data\n",
    "test_df = pd.DataFrame({\n",
    "    'wallet_address': ['wallet_a'] * 3,\n",
    "    'usd_net_transfers': [100, 200, -150],\n",
    "    'test_metric': [0.5, 1.0, -0.5]\n",
    "})\n",
    "\n",
    "# Calculate features\n",
    "result = wcf.calculate_timing_features_for_column(test_df, 'test_metric')\n",
    "\n",
    "# Expected values - calculating each component:\n",
    "# Buy weighted: (100 * 0.5 + 200 * 1.0) / (100 + 200) = 0.833333\n",
    "# Buy mean: (0.5 + 1.0) / 2 = 0.75\n",
    "# Sell weighted: (-0.5 * 150) / 150 = -0.5\n",
    "# Sell mean: Single value = -0.5\n",
    "expected = pd.DataFrame(\n",
    "    {\n",
    "        'test_metric_buy_mean': [0.75],\n",
    "        'test_metric_buy_weighted': [0.833333],\n",
    "        'test_metric_sell_mean': [-0.5],\n",
    "        'test_metric_sell_weighted': [-0.5],\n",
    "    },\n",
    "    index=pd.Index(['wallet_a'], name='wallet_address')\n",
    ")\n",
    "\n",
    "# Verify all values match expected\n",
    "assert np.allclose(\n",
    "    result,\n",
    "    expected,\n",
    "    equal_nan=True\n",
    "), f\"Expected {expected}\\nGot {result}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = u.winsorize(input_data, cutoff=0.1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.unit\n",
    "def test_calculate_mfi_scenario1():\n",
    "    \"\"\"\n",
    "    Unit test for calculating Money Flow Index (MFI) for a normal case.\n",
    "\n",
    "    Scenario: Calculate MFI for a price series [10, 12, 15, 14, 13, 16, 17, 18, 19, 20] and\n",
    "    volume series [100, 150, 200, 250, 300, 350, 400, 450, 500, 550] with a window of 3.\n",
    "\n",
    "    Expected Behavior: MFI should be calculated based on price and volume over the given window,\n",
    "    reflecting positive and negative money flows. Initial NaN values should be forward filled,\n",
    "    and any remaining NaNs should be filled with 0.5.\n",
    "    \"\"\"\n",
    "    # Define test data directly in the test\n",
    "    price_series = pd.Series([10, 12, 15, 14, 13, 16, 17, 18, 19, 20])\n",
    "    volume_series = pd.Series([100, 150, 200, 250, 300, 350, 400, 450, 500, 550])\n",
    "\n",
    "    # Call the function under test\n",
    "    result_mfi = ind.calculate_mfi(price_series, volume_series, window=3)\n",
    "\n",
    "    # Step-by-step expected MFI calculation for the first few values after the initial window (simplified):\n",
    "    # Step 1: Calculate raw money flow (price * volume)\n",
    "    money_flow = price_series * volume_series\n",
    "\n",
    "    # Step 2: Calculate positive and negative money flow (simplified):\n",
    "    positive_money_flow = money_flow.where(price_series > price_series.shift(1), 0)\n",
    "    negative_money_flow = money_flow.where(price_series < price_series.shift(1), 0)\n",
    "\n",
    "    # Step 3: Calculate the money flow ratio and MFI\n",
    "    money_flow_ratio = positive_money_flow.rolling(window=3).sum() / negative_money_flow.rolling(window=3).sum()\n",
    "    expected_mfi = 100 - (100 / (1 + money_flow_ratio))\n",
    "\n",
    "    # Adjust for the window, the third value is expected to be 100 if only positive flows exist\n",
    "    expected_mfi.iloc[2] = 100  # since only positive money flows exist in the first window\n",
    "\n",
    "    # Fill NaN values according to the new function behavior\n",
    "    expected_mfi = expected_mfi.ffill().fillna(0.5)\n",
    "\n",
    "    # Assert all values are close, including the previously NaN values that are now filled\n",
    "    assert np.allclose(result_mfi, expected_mfi, atol=1e-4), \\\n",
    "        f\"Expected MFI values: {expected_mfi.values}, but got {result_mfi.values}\"\n",
    "\n",
    "    # Assert there are no NaN values in the result\n",
    "    assert not result_mfi.isna().any(), \"Expected no NaN values in the result due to forward fill and 0.5 filling\"\n",
    "\n",
    "    # Assert the first value is 0.5 (since it can't be forward filled)\n",
    "    assert result_mfi[0] == 0.5, \"Expected first value to be 0.5 since it cannot be forward filled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test that when a coin's 'market_cap' is missing at the start of its time series\n",
    "but meets the 'min_coverage' threshold, the function imputes the missing values correctly.\n",
    "\"\"\"\n",
    "# Define the input DataFrame with 'market_cap' missing at the start for BTC and DOGE\n",
    "input_data = {\n",
    "    'coin_id': ['BTC', 'BTC', 'BTC', 'ETH', 'ETH', 'DOGE', 'DOGE'],\n",
    "    'date': ['2023-01-01', '2023-01-02', '2023-01-03',\n",
    "                '2023-01-01', '2023-01-02', '2023-01-01', '2023-01-02'],\n",
    "    'price': [30000, 31000, 32000, 2000, 2100, 0.05, 0.06],\n",
    "    'market_cap': [np.nan, 620000000, 640000000, 400000000, 420000000, np.nan, 6000000]\n",
    "}\n",
    "input_df = pd.DataFrame(input_data)\n",
    "\n",
    "# Define the expected 'market_cap_imputed' values\n",
    "expected_market_cap_imputed = pd.Series(\n",
    "    [np.nan, 620000000, 640000000, 400000000, 420000000, np.nan, 6000000],\n",
    "    dtype='Int64'\n",
    ")\n",
    "expected_df = input_df.copy()\n",
    "expected_df['market_cap_imputed'] = expected_market_cap_imputed\n",
    "\n",
    "# Invoke the impute_market_cap function with min_coverage=0.7\n",
    "result_df = dr.impute_market_cap(input_df, min_coverage=0.7)\n",
    "\n",
    "# Assert that 'market_cap_imputed' matches the expected values\n",
    "assert np.allclose(\n",
    "    result_df.sort_values(['coin_id','date'])['market_cap_imputed'],\n",
    "    expected_df.sort_values(['coin_id','date'])['market_cap_imputed'],\n",
    "    equal_nan=True\n",
    "), \"The 'market_cap_imputed' does not correctly handle missing values at the start of the time series.\"\n",
    "\n",
    "# Assert that the 'market_cap_imputed' column is of type Int64 where applicable\n",
    "assert result_df['market_cap_imputed'].dtype == 'Int64', (\n",
    "    \"'market_cap_imputed' column is not of type Int64.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.sort_values(['coin_id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_df.sort_values(['coin_id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of input data\n",
    "df_copy = input_df.copy()\n",
    "df_copy = df_copy.sort_values(['coin_id','date'])\n",
    "\n",
    "# Calculate coverage and historical maximums per coin\n",
    "coverage = df_copy.groupby('coin_id').agg(\n",
    "    records=('price', 'count'),\n",
    "    has_cap=('market_cap', 'count'),\n",
    "    max_cap=('market_cap', 'max')\n",
    ")\n",
    "coverage['coverage'] = coverage['has_cap'] / coverage['records']\n",
    "\n",
    "# Get eligible coins\n",
    "eligible_coins = coverage[\n",
    "    (coverage['coverage'] >= min_coverage) &\n",
    "    (coverage['coverage'] < 1)\n",
    "].index\n",
    "\n",
    "# Initialize imputed column with original values as int64\n",
    "df_copy['market_cap_imputed'] = df_copy['market_cap'].astype('Int64')\n",
    "\n",
    "# Process only eligible coins\n",
    "mask_eligible = df_copy['coin_id'].isin(eligible_coins)\n",
    "\n",
    "# Calculate ratio for all valid records of eligible coins\n",
    "df_copy.loc[mask_eligible, 'ratio'] = (\n",
    "    df_copy.loc[mask_eligible, 'market_cap'] /\n",
    "    df_copy.loc[mask_eligible, 'price']\n",
    ")\n",
    "\n",
    "# # Backfill and forward fill ratios within each coin group\n",
    "# df_copy['ratio'] = df_copy.groupby('coin_id')['ratio'].bfill()\n",
    "# df_copy['ratio'] = df_copy.groupby('coin_id')['ratio'].ffill()\n",
    "\n",
    "# # Calculate imputed market caps using the filled ratios\n",
    "# mask_missing = df_copy['market_cap_imputed'].isna() & mask_eligible\n",
    "# df_copy.loc[mask_missing, 'market_cap_imputed'] = (\n",
    "#     (df_copy.loc[mask_missing, 'price'] *\n",
    "#         df_copy.loc[mask_missing, 'ratio']).round().astype('Int64')\n",
    "# )\n",
    "\n",
    "# # Join max historical values and apply max_multiple check vectorized\n",
    "# df_copy = df_copy.merge(\n",
    "#     coverage[['max_cap']],\n",
    "#     left_on='coin_id',\n",
    "#     right_index=True,\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# # Set imputed values exceeding max_multiple * historical max to np.nan\n",
    "# mask_exceeds_max = (\n",
    "#     df_copy['market_cap_imputed'] >\n",
    "#     (df_copy['max_cap'] * max_multiple)\n",
    "# )\n",
    "# df_copy.loc[mask_exceeds_max, 'market_cap_imputed'] = pd.NA\n",
    "\n",
    "# # Drop temporary columns\n",
    "# df_copy = df_copy.drop(['ratio', 'max_cap'], axis=1)\n",
    "\n",
    "\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of input data\n",
    "df_copy = input_df.copy()\n",
    "\n",
    "# Calculate coverage and historical maximums per coin\n",
    "coverage = df_copy.groupby('coin_id').agg(\n",
    "    records=('price', 'count'),\n",
    "    has_cap=('market_cap', 'count'),\n",
    "    max_cap=('market_cap', 'max')\n",
    ")\n",
    "coverage['coverage'] = coverage['has_cap'] / coverage['records']\n",
    "\n",
    "# Get eligible coins\n",
    "eligible_coins = coverage[\n",
    "    (coverage['coverage'] >= min_coverage) &\n",
    "    (coverage['coverage'] < 1)\n",
    "].index\n",
    "\n",
    "# Initialize imputed column with original values as int64\n",
    "df_copy['market_cap_imputed'] = df_copy['market_cap'].astype('Int64')\n",
    "\n",
    "# Process only eligible coins\n",
    "mask_eligible = df_copy['coin_id'].isin(eligible_coins)\n",
    "\n",
    "# Calculate ratio for all valid records of eligible coins\n",
    "df_copy.loc[mask_eligible, 'ratio'] = (\n",
    "    df_copy.loc[mask_eligible, 'market_cap'] /\n",
    "    df_copy.loc[mask_eligible, 'price']\n",
    ")\n",
    "\n",
    "# Backfill and forward fill ratios within each coin group\n",
    "df_copy['ratio'] = df_copy.groupby('coin_id')['ratio'].bfill()\n",
    "df_copy['ratio'] = df_copy.groupby('coin_id')['ratio'].ffill()\n",
    "\n",
    "# Calculate imputed market caps using the filled ratios\n",
    "mask_missing = df_copy['market_cap_imputed'].isna() & mask_eligible\n",
    "df_copy.loc[mask_missing, 'market_cap_imputed'] = (\n",
    "    (df_copy.loc[mask_missing, 'price'] *\n",
    "        df_copy.loc[mask_missing, 'ratio']).round().astype('Int64')\n",
    ")\n",
    "\n",
    "# Check for imputed values exceeding historical maximums\n",
    "for coin_id in eligible_coins:\n",
    "    max_historical = coverage.loc[coin_id, 'max_cap']\n",
    "    coin_mask = (df_copy['coin_id'] == coin_id) & df_copy['market_cap_imputed'].notna()\n",
    "    max_imputed = df_copy.loc[coin_mask, 'market_cap_imputed'].max()\n",
    "\n",
    "    if max_imputed > max_historical:\n",
    "        logger.warning(\n",
    "            f\"Coin {coin_id}: Imputed market cap ({max_imputed:.0f}) \"\n",
    "            f\"exceeds historical maximum ({max_historical:.0f})\"\n",
    "        )\n",
    "\n",
    "# Drop the temporary ratio column\n",
    "df_copy = df_copy.drop('ratio', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# def test_multiple_coins_per_wallet():\n",
    "\"\"\"\n",
    "Test scenario where wallets own multiple coins, some exceeding thresholds when aggregated.\n",
    "Checks filtering based on specified date range.\n",
    "\"\"\"\n",
    "# Create test data\n",
    "sample_profits_df = pd.DataFrame({\n",
    "    'coin_id': ['BTC', 'ETH', 'BTC', 'ETH', 'LTC', 'BTC', 'ETH'],\n",
    "    'wallet_address': ['wallet1', 'wallet1', 'wallet2', 'wallet2', 'wallet2',\n",
    "                        'wallet3', 'wallet3'],\n",
    "    'date': pd.date_range(start='2023-01-01', periods=7),\n",
    "    'profits_cumulative': [5000, 3000, 1000, 500, 500, 100, 50],\n",
    "    'usd_inflows_cumulative': [10000, 8000, 2000, 1500, 1500, 500, 250]\n",
    "})\n",
    "\n",
    "config = {\n",
    "    'profitability_filter': 7500,\n",
    "    'inflows_filter': 15000,\n",
    "    'date_range': {\n",
    "        'start': '2023-01-02',\n",
    "        'end': '2023-01-05'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the function with date range\n",
    "cleaned_df, exclusions_logs_df = dr.clean_profits_df(\n",
    "    sample_profits_df,\n",
    "    config,\n",
    "    earliest_date=config['date_range']['start'],\n",
    "    latest_date=config['date_range']['end']\n",
    ")\n",
    "\n",
    "# Expected results - only checking within date window but removing all records\n",
    "expected_cleaned_df = sample_profits_df[\n",
    "    sample_profits_df['wallet_address'].isin(['wallet2', 'wallet3'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "expected_exclusions = pd.DataFrame({\n",
    "    'wallet_address': ['wallet1'],\n",
    "    'profits_exclusion': [True],\n",
    "    'inflows_exclusion': [True]\n",
    "})\n",
    "\n",
    "# Assertions\n",
    "assert len(cleaned_df) == 5  # wallet2 (3 records) and wallet3 (2 records) should remain\n",
    "assert np.array_equal(cleaned_df.values, expected_cleaned_df.values)\n",
    "assert np.array_equal(exclusions_logs_df.values, expected_exclusions.values)\n",
    "\n",
    "# Check if profits and inflows are approximately correct for the remaining wallets\n",
    "# Should include ALL records for passing wallets (1000 + 500 + 500 + 100 + 50)\n",
    "assert pytest.approx(cleaned_df['profits_cumulative'].sum(), abs=1e-4) == 2150\n",
    "# Should include ALL records for passing wallets (2000 + 1500 + 1500 + 500 + 250)\n",
    "assert pytest.approx(cleaned_df['usd_inflows_cumulative'].sum(), abs=1e-4) == 5750\n",
    "\n",
    "# Additional date-specific checks\n",
    "date_mask = ((cleaned_df['date'] >= config['date_range']['start']) &\n",
    "                (cleaned_df['date'] <= config['date_range']['end']))\n",
    "date_filtered = cleaned_df[date_mask]\n",
    "\n",
    "# Verify we have the expected number of records in the date range\n",
    "assert len(date_filtered) == 3  # Should only have records between Jan 2-5 for remaining wallets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
