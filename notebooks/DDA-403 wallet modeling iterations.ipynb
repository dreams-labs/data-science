{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import datetime,timedelta\n",
    "import json\n",
    "import warnings\n",
    "import yaml\n",
    "from typing import Dict,Union,List,Any,Tuple\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "from pyxirr import xirr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"MallocStackLogging\")\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "import wallet_modeling.wallet_orchestrator as wo\n",
    "import wallet_modeling.wallet_training_data as wtd\n",
    "import wallet_modeling.wallet_modeling as wm\n",
    "import wallet_features.wallet_features as wf\n",
    "import wallet_features.wallet_coin_features as wcf\n",
    "import wallet_features.wallet_coin_date_features as wcdf\n",
    "from wallet_modeling.wallets_config_manager import WalletsConfig\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp, wo, wtd, wm, wf, wcf, wcdf]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "wallets_config = WalletsConfig.load_from_yaml('../config/wallets_config.yaml')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Retrieve datasets\n",
    "market_data_df,profits_df = wo.retrieve_datasets()\n",
    "\n",
    "# Define wallet cohort after cleaning\n",
    "training_wallet_metrics_df,wallet_cohort = wo.define_wallet_cohort(profits_df,market_data_df)\n",
    "\n",
    "# Generate profits_df for all training windows and the modeling period\n",
    "training_windows_profits_dfs, modeling_period_profits_df = wo.split_profits_df(profits_df,\n",
    "                                                                               market_data_df,wallet_cohort)\n",
    "\n",
    "# Generate features for all windows and merge them together\n",
    "training_data_df = wo.generate_wallet_performance_features(training_windows_profits_dfs,\n",
    "                                                           training_wallet_metrics_df,wallet_cohort)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# 2. Define modeling wallet cohort via data cleaning\n",
    "# --------------------------------------------------\n",
    "# Impute the training period boundary dates\n",
    "training_period_boundary_dates = [\n",
    "    wallets_config['training_data']['training_period_start'],\n",
    "    wallets_config['training_data']['training_period_end']\n",
    "]\n",
    "imputed_profits_df = pri.impute_profits_for_multiple_dates(profits_df, market_data_df,\n",
    "                                                           training_period_boundary_dates, n_threads=24)\n",
    "\n",
    "# Create a training period only profits_df\n",
    "training_profits_df = imputed_profits_df[\n",
    "    imputed_profits_df['date']<=wallets_config['training_data']['training_period_end']\n",
    "    ].copy()\n",
    "\n",
    "# Add cash flows logic column\n",
    "training_profits_df = wcf.add_cash_flow_transfers_logic(training_profits_df)\n",
    "\n",
    "# Compute wallet level metrics over duration of training period\n",
    "training_wallet_metrics_df = wf.calculate_wallet_level_metrics(training_profits_df)\n",
    "\n",
    "# Apply filters based on wallet behavior during the training period\n",
    "filtered_training_wallet_metrics_df = wtd.apply_wallet_thresholds(training_wallet_metrics_df)\n",
    "\n",
    "# Upload the cohort to BigQuery for additional complex feature generation\n",
    "wallet_cohort = filtered_training_wallet_metrics_df.index.values\n",
    "wtd.upload_wallet_cohort(wallet_cohort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# 3. Impute all required dates for wallet cohort\n",
    "# ----------------------------------------------\n",
    "# Filter to only wallet cohort\n",
    "cohort_profits_df = profits_df[profits_df['wallet_address'].isin(wallet_cohort)]\n",
    "\n",
    "# Impute all required dates\n",
    "imputation_dates = wtd.generate_imputation_dates()\n",
    "windows_profits_df = pri.impute_profits_for_multiple_dates(cohort_profits_df, market_data_df, imputation_dates, n_threads=24)\n",
    "windows_profits_df = (windows_profits_df[(windows_profits_df['date'] >= pd.to_datetime(min(imputation_dates))) &\n",
    "                                    (windows_profits_df['date'] <= pd.to_datetime(max(imputation_dates)))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# 4. Generate features for each training window\n",
    "# ---------------------------------------------\n",
    "# Split profits_df into training windows and the modeling period\n",
    "training_windows_dfs, modeling_period_df =  wtd.split_window_dfs(windows_profits_df)\n",
    "\n",
    "# Create training data df with full training period metrics\n",
    "training_data_df = wf.fill_missing_wallet_data(filtered_training_wallet_metrics_df, wallet_cohort)\n",
    "training_data_df = training_data_df.add_suffix(\"_all_windows\")\n",
    "\n",
    "# Generate and join dfs for each trianing window\n",
    "for i in range(len(training_windows_dfs)):\n",
    "    # Add metrics\n",
    "    window_df = training_windows_dfs[i]\n",
    "    window_df = wcf.add_cash_flow_transfers_logic(window_df)\n",
    "    window_wallets_df = wf.calculate_wallet_level_metrics(window_df)\n",
    "\n",
    "    # Fill missing values and Join to training_data_df\n",
    "    window_wallets_df = wf.fill_missing_wallet_data(window_wallets_df, wallet_cohort)\n",
    "    window_wallets_df = window_wallets_df.add_suffix(f'_w{i+1}')\n",
    "    training_data_df = training_data_df.join(window_wallets_df,how='left')\n",
    "\n",
    "\n",
    "training_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Generate additional features\n",
    "# ----------------------------\n",
    "# Retrieve the buy numbers for wallets in the cohort\n",
    "buyer_numbers_df = wcf.retrieve_buyer_numbers()\n",
    "\n",
    "# Append buyer numbers to the merged_df\n",
    "buyer_averages_df = buyer_numbers_df.groupby('wallet_id').mean('buyer_number')\n",
    "buyer_averages_df.columns = ['average_buyer_number']\n",
    "training_data_df = training_data_df.join(buyer_averages_df)\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Calculate modeling period metrics\n",
    "# ---------------------------------\n",
    "# Calculate modeling period wallet metrics\n",
    "modeling_period_df = wcf.add_cash_flow_transfers_logic(modeling_period_df)\n",
    "modeling_wallets_df = wf.calculate_wallet_level_metrics(modeling_period_df)\n",
    "\n",
    "# Remove wallets with below the minimum investment threshold\n",
    "base_wallets = len(modeling_wallets_df)\n",
    "modeling_wallets_df = modeling_wallets_df[\n",
    "    modeling_wallets_df['invested']>=wallets_config['data_cleaning']['min_modeling_investment']]\n",
    "logger.info(\"Removed %s/%s wallets with modeling period investments below the threshold.\",\n",
    "            base_wallets - len(modeling_wallets_df), base_wallets)\n",
    "\n",
    "# Remove wallets with transaction counts below the threshold\n",
    "base_wallets = len(modeling_wallets_df)\n",
    "modeling_wallets_df = modeling_wallets_df[\n",
    "    modeling_wallets_df['transaction_days']>=wallets_config['data_cleaning']['min_modeling_transaction_days']]\n",
    "logger.info(\"Removed %s/%s wallets with modeling period transaction days below the threshold.\",\n",
    "            base_wallets - len(modeling_wallets_df), base_wallets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "winsorization=0.01\n",
    "target_variable = 'performance_score'\n",
    "\n",
    "# Generate the target variables\n",
    "target_vars_df = modeling_wallets_df[['invested','net_gain']].copy()\n",
    "target_vars_df = wm.generate_target_variables(target_vars_df,winsorization=winsorization)\n",
    "\n",
    "target_vars_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training data and target variables\n",
    "modeling_df = training_data_df.join(target_vars_df[target_variable],how='inner')\n",
    "modeling_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "df = modeling_df.copy()\n",
    "\n",
    "# Assuming your dataframe is called 'df'\n",
    "# Separate features and target\n",
    "X = df.drop(target_variable, axis=1)  # dropping both return columns\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_features = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config.reload()\n",
    "\n",
    "# Example usage with your existing code:\n",
    "evaluation = wm.evaluate_regression_model(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    model=pipeline.named_steps['regressor'],  # Pass the actual model object\n",
    "    X_test=X_test,\n",
    "    feature_names=X.columns.tolist()\n",
    ")\n",
    "\n",
    "# Print summary report\n",
    "print(evaluation['summary_report'])\n",
    "\n",
    "# Access specific metrics\n",
    "print(f\"RÂ² Score: {evaluation['r2']:.3f}\")\n",
    "\n",
    "# The figure can be displayed or saved\n",
    "if evaluation['figures'] is not None:\n",
    "    plt.show()  # or evaluation['figures'].savefig('model_evaluation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "profits_df = training_profits_df.copy().set_index('wallet_address')\n",
    "\n",
    "# Precompute necessary transformations\n",
    "profits_df['abs_usd_net_transfers'] = profits_df['usd_net_transfers'].abs()\n",
    "profits_df['cumsum_usd_net_transfers'] = profits_df.groupby('wallet_address')['usd_net_transfers'].cumsum()\n",
    "\n",
    "# Group and aggregate metrics\n",
    "wallet_metrics_df = profits_df.groupby('wallet_address').agg(\n",
    "    invested=('cumsum_usd_net_transfers', 'max'),\n",
    "    total_net_transfers=('usd_net_transfers', 'sum'),\n",
    "    unique_coins=('coin_id', 'nunique'),\n",
    "    transaction_days=('date', 'count'),\n",
    "    total_volume=('abs_usd_net_transfers', 'sum'),\n",
    "    average_transaction=('abs_usd_net_transfers', 'mean')\n",
    ")\n",
    "\n",
    "\n",
    "# Compute additional derived metrics\n",
    "wallet_metrics_df['net_gain'] = -wallet_metrics_df['total_net_transfers']\n",
    "wallet_metrics_df['return'] = wallet_metrics_df['net_gain'] / wallet_metrics_df['invested']\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate amount invested\n",
    "# wallet_invested_df = pd.DataFrame(\n",
    "#     profits_df\n",
    "#     .groupby(level='wallet_address')['usd_net_transfers'].cumsum()\n",
    "#     .groupby(level='wallet_address').max()\n",
    "# )\n",
    "# wallet_invested_df.columns = ['invested']\n",
    "\n",
    "# # Calculate net gains\n",
    "# wallet_gain_df = pd.DataFrame(\n",
    "#     -profits_df.groupby(level='wallet_address')['usd_net_transfers'].sum()\n",
    "# )\n",
    "# wallet_gain_df.columns = ['net_gain']\n",
    "\n",
    "# # Join dfs\n",
    "# wallet_performance_df = wallet_invested_df.join(wallet_gain_df)\n",
    "\n",
    "# # Compute return\n",
    "# wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "\n",
    "# return wallet_performance_df\n",
    "wallet_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "\n",
    "\n",
    "# Split out modeling and training records to calculate return separately\n",
    "modeling_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['modeling_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['modeling_period_end']))\n",
    "]\n",
    "modeling_performance_df = wf.calculate_wallet_investment_return(modeling_df)\n",
    "\n",
    "\n",
    "training_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['training_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['training_period_end']))\n",
    "]\n",
    "training_performance_df = wf.calculate_wallet_investment_return(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_invested = 10000\n",
    "filtered_df = training_performance_df[training_performance_df['invested']>=min_invested]\n",
    "print(training_performance_df.shape)\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*5)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*5)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_correlation_matrix(df):\n",
    "    \"\"\"\n",
    "    Create and visualize a correlation matrix for the given DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Correlation matrix\n",
    "    \"\"\"\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix,\n",
    "                annot=True,  # Show correlation values\n",
    "                cmap='coolwarm',  # Color scheme from red (negative) to blue (positive)\n",
    "                vmin=-1, vmax=1,  # Fix the scale\n",
    "                center=0,  # Center the colormap at 0\n",
    "                fmt='.2f')  # Round to 2 decimal places\n",
    "\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "# create_correlation_matrix(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.set_index('coin_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_analysis_df = market_data_df.copy().set_index('coin_id')\n",
    "\n",
    "start_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_start'])]\n",
    "end_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_end'])]\n",
    "\n",
    "# coin_modeling_returns_df = start_prices.join(end_prices)\n",
    "\n",
    "    # (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['modeling_period_start'])) &\n",
    "    # (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['modeling_period_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coin returns during modeling period\n",
    "coin_modeling_returns_df = start_prices[['price']].join(end_prices[['price']],lsuffix='_start',rsuffix='_end')\n",
    "coin_modeling_returns_df['coin_modeling_return'] = coin_modeling_returns_df['price_end']/coin_modeling_returns_df['price_start']\n",
    "coin_modeling_returns_df[\"coin_modeling_percentile_return\"] = coin_modeling_returns_df[\"coin_modeling_return\"].rank(ascending=True, pct=True)\n",
    "\n",
    "coin_modeling_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wallet ending balances\n",
    "min_end_balance = 1000\n",
    "\n",
    "# Calculate period end balance for each coin-wallet pair\n",
    "end_balances_df = adj_profits_df[adj_profits_df['date']==pd.to_datetime(config['training_data']['training_period_end'])]\n",
    "end_balances_df = end_balances_df[end_balances_df['usd_net_transfers']<=-min_end_balance]\n",
    "end_balances_df['usd_balance'] = end_balances_df['usd_net_transfers'].abs()\n",
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]\n",
    "end_balances_df = end_balances_df.set_index(['coin_id','wallet_address'])\n",
    "end_balances_df.head()\n",
    "\n",
    "# Add wallet performance metrics\n",
    "end_balances_df = end_balances_df.join(performance_df,on='wallet_address')\n",
    "end_balances_df = end_balances_df[end_balances_df['return_training'].notna()]\n",
    "\n",
    "end_balances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_return_column = 'return_training'\n",
    "\n",
    "# Assess average wallet return during training period\n",
    "coin_wallet_performance = pd.DataFrame(end_balances_df.reset_index().groupby('coin_id',observed=True)[wallet_return_column].mean())\n",
    "coin_wallet_performance.columns = ['avg_wallet_training_return']\n",
    "\n",
    "coin_wallet_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coin_return_column = 'coin_modeling_return'\n",
    "coin_return_column = 'coin_modeling_percentile_return'\n",
    "\n",
    "wallet_forecast_df = coin_modeling_returns_df[[coin_return_column]].join(coin_wallet_performance)\n",
    "wallet_forecast_df[coin_return_column].corr(wallet_forecast_df['avg_wallet_training_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_forecast_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.loc['0037051e-677f-439f-9353-4dc896fe9ecd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df = aggregate_wallet_performance(training_df)\n",
    "training_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_performance_df = aggregate_wallet_performance(modeling_df)\n",
    "modeling_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*10)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*10)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_performance_df = training_performance_df\n",
    "wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "wallet_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_performance_df.sample(10)\n",
    "\n",
    "wallet_performance_df[wallet_performance_df['invested']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0xca6cfaa7d61371310d84b63a4ca90cbf7883a9db'\n",
    "\n",
    "df = wallets_df_filtered.loc[w]\n",
    "\n",
    "# print(xirr(df.index.get_level_values('date'), df['usd_net_transfers']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_profits_df[adj_profits_df['wallet_address']==w]\n",
    "profits_df[profits_df['wallet_address']==w].sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df[adj_profits_df['wallet_address']==w].sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wallets_xirr(profits_df, min_wallet_volume):\n",
    "    \"\"\"\n",
    "    Calculates the XIRR of each wallet based on their cash flows across all coins they've\n",
    "    interacted with in profits_df.\n",
    "\n",
    "    Parameters:\n",
    "    - profits_df (pd.DataFrame): shows daily coin-wallet transfers in USD\n",
    "    - min_wallet_volume (int): wallets with less than this total USD volume will be excluded\n",
    "\n",
    "    Returns:\n",
    "    - xirr_df (pd.DataFrame): shows the XIRR of each wallet over the provided transactions\n",
    "    \"\"\"\n",
    "    logger.info('Beginning XIRR calculation sequence...')\n",
    "\n",
    "    # 1. Summarize cash flows on a wallet level\n",
    "    # -----------------------------------------\n",
    "    # Sum cash flows on a wallet level\n",
    "    wallets_df = pd.DataFrame(profits_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "    # 2. Filter wallets on data quality\n",
    "    # ---------------------------------\n",
    "    # Identify wallets with no transactions\n",
    "    wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "    low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "    # Remove transactionless wallets\n",
    "    wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "    logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "    # Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "    wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "        lambda x: (x > 0).any() and (x < 0).any()\n",
    "    )\n",
    "    wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "    # Filter wallet addresses that do not have both positive and negative transfers\n",
    "    wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "    logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "    # 3. Calculate XIRR\n",
    "    # -----------------\n",
    "    # Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "    start_time = time.time()\n",
    "    logger.info('Calculating XIRR values...')\n",
    "    xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "        lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    "    )\n",
    "    logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    xirr_df = pd.DataFrame(xirr_results)\n",
    "    xirr_df.columns = ['xirr']\n",
    "\n",
    "    # Fill empty values with 0s\n",
    "    xirr_df = xirr_df.fillna(0)\n",
    "\n",
    "\n",
    "    return xirr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 1\n",
    "\n",
    "# 1. Summarize cash flows on a wallet level\n",
    "# -----------------------------------------\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(modeling_df.copy().groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "# 2. Filter wallets on data quality\n",
    "# ---------------------------------\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "# Filter wallet addresses that do not have both positive and negative transfers\n",
    "wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "# 3. Calculate XIRR\n",
    "# -----------------\n",
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "start_time = time.time()\n",
    "logger.info('Calculating XIRR values...')\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Fill empty values with 0s\n",
    "xirr_df = xirr_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: -df['usd_net_transfers'].sum()/df['usd_net_transfers'].cumsum().max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df.loc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wallet_metrics(group):\n",
    "    cumsum = group['usd_net_transfers'].cumsum()\n",
    "    invested = cumsum.max()\n",
    "    net_gain = group['usd_net_transfers'].sum()\n",
    "\n",
    "    return pd.Series({\n",
    "        'invested': invested,\n",
    "        'net_gain': net_gain,\n",
    "        'return': net_gain/invested if invested != 0 else np.nan\n",
    "    })\n",
    "\n",
    "# Calculate metrics for all wallets at once\n",
    "results = wallets_df_filtered.groupby(level='wallet_address').apply(wallet_metrics)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 10000\n",
    "\n",
    "# Calculate XIRR\n",
    "training_xirr_df = calculate_wallets_xirr(training_df,min_wallet_volume)\n",
    "modeling_xirr_df = calculate_wallets_xirr(modeling_df,min_wallet_volume=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate percentiles\n",
    "xirr_df[\"training_xirr_percentile\"] = xirr_df[\"training_xirr\"].rank(ascending=True, pct=True)\n",
    "xirr_df[\"modeling_xirr_percentile\"] = xirr_df[\"modeling_xirr\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "xirr_df['training_xirr_decile'] = np.ceil(xirr_df['training_xirr_percentile']*10)\n",
    "xirr_df['modeling_xirr_decile'] = np.ceil(xirr_df['modeling_xirr_percentile']*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    xirr_df['training_xirr_decile'],\n",
    "    xirr_df['modeling_xirr_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = training_xirr_df.rename(columns={'xirr': 'training_xirr'}).join(\n",
    "    modeling_xirr_df.rename(columns={'xirr': 'modeling_xirr'}),\n",
    "    how='inner'\n",
    ").fillna({'modeling_xirr': 0})\n",
    "\n",
    "\n",
    "\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df['training_xirr_percentile'].corr(xirr_df['modeling_xirr_percentile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(training_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "\n",
    "# Filter wallet addresses that do not meet the condition\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "logger.info('Found %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = '0x036783df7aec54b5dfca9e1f870577bbcca95481'\n",
    "# wallets_df.loc[w]\n",
    "\n",
    "# profits_df[profits_df['wallet_address']==w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIRR sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0x0000000000000000000000000000000000000014'\n",
    "\n",
    "dates = wallets_df.loc[w].index.values\n",
    "cash_flows = wallets_df.loc[w]['usd_net_transfers']\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(xirr_results.shape)\n",
    "xirr_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xirr(dates,cash_flows)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '77e2cf4b-d18a-4026-a2f2-f083f48fe1be'\n",
    "w = '0xaff2943cfe3e95f66142a1729079418d78e42236'\n",
    "\n",
    "# u.cw_filter_df(training_df,c,w)\n",
    "\n",
    "df = u.cw_filter_df(training_df,c,w)\n",
    "df = df.sort_values('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['date']\n",
    "cash_flows = df['usd_net_transfers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxirr import xirr\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fractions = (np.datetime64(dates) - np.datetime64(dates[0])).astype('timedelta64[D]') / np.timedelta64(1, 'Y')\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     select cwp.wallet_address\n",
    "#     ,cwp.coin_id\n",
    "#     ,cwp.date\n",
    "#     ,round(cwp.usd_net_transfers) as usd_net_transfers\n",
    "#     ,round(cwp.usd_balance) as usd_balance\n",
    "#     ,round(cwp.usd_net_transfers/cmd.price) as token_transfers\n",
    "#     ,round(cwp.usd_balance/cmd.price) as token_balance\n",
    "#     ,cmd.price\n",
    "#     from wallets w\n",
    "#     join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#     join core.coin_wallet_profits cwp on cwp.wallet_address = wc.wallet_address\n",
    "#         and cwp.coin_id = wc.coin_id\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = cwp.coin_id\n",
    "#         and cmd.date = cwp.date\n",
    "#     order by 1,2,3\n",
    "#     '''\n",
    "# transfers_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert wallet_address to categorical, store the mapping, and convert the column to int32\n",
    "# wallet_address_categorical = transfers_df['wallet_address'].astype('category')\n",
    "# # wallet_address_mapping = wallet_address_categorical.cat.categories\n",
    "# # transfers_df['wallet_address'] = wallet_address_categorical.cat.codes.astype('uint32')\n",
    "\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# transfers_df['coin_id'] = transfers_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_net_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'price', 'float32')\n",
    "\n",
    "# print(transfers_df.info())\n",
    "# print(u.df_mem(transfers_df))\n",
    "# transfers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     ,coins as (\n",
    "#         select wc.coin_id\n",
    "#         from wallets w\n",
    "#         join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#         group by 1\n",
    "#     )\n",
    "\n",
    "#     select cmd.coin_id\n",
    "#     ,cmd.date\n",
    "#     ,cmd.price\n",
    "#     ,cmd.market_cap\n",
    "#     from coins c\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = c.coin_id\n",
    "#     order by 1,2\n",
    "#     '''\n",
    "# prices_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# prices_df['coin_id'] = prices_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# prices_df = u.safe_downcast(prices_df, 'price', 'float32')\n",
    "# prices_df = u.safe_downcast(prices_df, 'market_cap', 'int32')\n",
    "\n",
    "# print(prices_df.info())\n",
    "# print(u.df_mem(prices_df))\n",
    "# prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_multiple_coins_per_wallet():\n",
    "\"\"\"\n",
    "Test scenario where wallets own multiple coins, some exceeding thresholds when aggregated.\n",
    "Checks filtering based on specified date range.\n",
    "\"\"\"\n",
    "# Create test data\n",
    "sample_profits_df = pd.DataFrame({\n",
    "    'coin_id': ['BTC', 'ETH', 'BTC', 'ETH', 'LTC', 'BTC', 'ETH'],\n",
    "    'wallet_address': ['wallet1', 'wallet1', 'wallet2', 'wallet2', 'wallet2',\n",
    "                        'wallet3', 'wallet3'],\n",
    "    'date': pd.date_range(start='2023-01-01', periods=7),\n",
    "    'profits_cumulative': [5000, 3000, 1000, 500, 500, 100, 50],\n",
    "    'usd_inflows_cumulative': [10000, 8000, 2000, 1500, 1500, 500, 250]\n",
    "})\n",
    "\n",
    "config = {\n",
    "    'profitability_filter': 7500,\n",
    "    'inflows_filter': 15000,\n",
    "    'date_range': {\n",
    "        'start': '2023-01-02',\n",
    "        'end': '2023-01-05'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the function with date range\n",
    "cleaned_df, exclusions_logs_df = dr.clean_profits_df(\n",
    "    sample_profits_df,\n",
    "    config,\n",
    "    earliest_date=config['date_range']['start'],\n",
    "    latest_date=config['date_range']['end']\n",
    ")\n",
    "\n",
    "# Expected results - only checking within date window but removing all records\n",
    "expected_cleaned_df = sample_profits_df[\n",
    "    sample_profits_df['wallet_address'].isin(['wallet2', 'wallet3'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "expected_exclusions = pd.DataFrame({\n",
    "    'wallet_address': ['wallet1'],\n",
    "    'profits_exclusion': [True],\n",
    "    'inflows_exclusion': [True]\n",
    "})\n",
    "\n",
    "# Assertions\n",
    "assert len(cleaned_df) == 5  # wallet2 (3 records) and wallet3 (2 records) should remain\n",
    "assert np.array_equal(cleaned_df.values, expected_cleaned_df.values)\n",
    "assert np.array_equal(exclusions_logs_df.values, expected_exclusions.values)\n",
    "\n",
    "# Check if profits and inflows are approximately correct for the remaining wallets\n",
    "# Should include ALL records for passing wallets (1000 + 500 + 500 + 100 + 50)\n",
    "assert pytest.approx(cleaned_df['profits_cumulative'].sum(), abs=1e-4) == 2150\n",
    "# Should include ALL records for passing wallets (2000 + 1500 + 1500 + 500 + 250)\n",
    "assert pytest.approx(cleaned_df['usd_inflows_cumulative'].sum(), abs=1e-4) == 5750\n",
    "\n",
    "# Additional date-specific checks\n",
    "date_mask = ((cleaned_df['date'] >= config['date_range']['start']) &\n",
    "                (cleaned_df['date'] <= config['date_range']['end']))\n",
    "date_filtered = cleaned_df[date_mask]\n",
    "\n",
    "# Verify we have the expected number of records in the date range\n",
    "assert len(date_filtered) == 3  # Should only have records between Jan 2-5 for remaining wallets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
