{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import datetime,timedelta\n",
    "import json\n",
    "import warnings\n",
    "import yaml\n",
    "from typing import Dict,Union,List,Any,Tuple\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "from pyxirr import xirr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"MallocStackLogging\")\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "import wallet_analysis.wallet_utilities as wu\n",
    "import wallet_analysis.wallet_features as wf\n",
    "import wallet_analysis.wallet_coin_features as wcf\n",
    "import wallet_analysis.wallet_coin_date_features as wcdf\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp, wu, wf, wcf, wcdf]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "# config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# wallets config\n",
    "config_path = '../config/wallets_config.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as file:\n",
    "    wallets_config = yaml.safe_load(file)\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config = yaml.safe_load(Path('../config/wallets_config.yaml').read_text(encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config = yaml.safe_load(Path('../config/wallets_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "# 1a. Data Retrieval\n",
    "# ------------------\n",
    "earliest_date = next(iter(wallets_config['training_data']['training_period_starts'].values()))\n",
    "latest_date = wallets_config['training_data']['modeling_period_end']\n",
    "\n",
    "# Market data: retrieve all\n",
    "market_data_df = dr.retrieve_market_data()\n",
    "\n",
    "# Profits: retrieve all\n",
    "profits_df = dr.retrieve_profits_data(earliest_date,latest_date,\n",
    "                                    wallets_config['data_cleaning']['minimum_wallet_inflows'])\n",
    "\n",
    "# 1b. Data Cleaning\n",
    "# -------------\n",
    "# Clean profits\n",
    "profits_df, _ = dr.clean_profits_df(profits_df, wallets_config['data_cleaning'])\n",
    "\n",
    "# Remove the filtered coins from market_data_df\n",
    "if wallets_config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "\n",
    "# Filter market_data_df\n",
    "market_data_df = dr.clean_market_data(\n",
    "    market_data_df,\n",
    "    wallets_config,\n",
    "    earliest_date,\n",
    "    latest_date\n",
    ")\n",
    "\n",
    "# Remove the filtered coins from profits_df\n",
    "profits_df = profits_df[profits_df['coin_id'].isin(market_data_df['coin_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "wallets_config = yaml.safe_load(Path('../config/wallets_config.yaml').read_text(encoding='utf-8'))\n",
    "\n",
    "def generate_imputation_dates(wallets_config):\n",
    "    \"\"\"\n",
    "    Generates a list of all dates that need imputation, including the first\n",
    "    and last date of each training and modeling period.\n",
    "\n",
    "    Params:\n",
    "    - wallets_config (dict): config with relevant training period and modeling period dates\n",
    "\n",
    "    Returns:\n",
    "    - imputation_dates (list): list that includes the start and end dates of each period\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all period start dates\n",
    "    period_start_dates = sorted([datetime.strptime(date, \"%Y-%m-%d\")\n",
    "                    for date in wallets_config['training_data']['training_period_starts'].values()\n",
    "                    ])\n",
    "\n",
    "    # Generate the output array\n",
    "    imputation_dates = [period_start_dates[0].strftime(\"%Y-%m-%d\")]  # Include the first date\n",
    "    for i in range(1, len(period_start_dates)):\n",
    "        # Append the day before each period start\n",
    "        imputation_dates.append((period_start_dates[i] - timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
    "        # Append period start date\n",
    "        imputation_dates.append(period_start_dates[i].strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # Append day before modeling period\n",
    "    modeling_start = datetime.strptime(wallets_config['training_data']['modeling_period_start'], \"%Y-%m-%d\")\n",
    "    imputation_dates.append((modeling_start - timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # Append modeling period dates\n",
    "    imputation_dates.append(wallets_config['training_data']['modeling_period_start'])\n",
    "    imputation_dates.append(wallets_config['training_data']['modeling_period_end'])\n",
    "\n",
    "    return imputation_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(imputation_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Impute all required dates\n",
    "# ----------------------------\n",
    "imputation_dates = [\n",
    "    wallets_config['training_data']['training_period_start'],\n",
    "    wallets_config['training_data']['training_period_end'],\n",
    "    wallets_config['training_data']['modeling_period_start'],\n",
    "    wallets_config['training_data']['modeling_period_end']\n",
    "]\n",
    "\n",
    "# Impute all required dates\n",
    "window_profits_df = pri.impute_profits_for_multiple_dates(profits_df, market_data_df, imputation_dates, n_threads=24)\n",
    "window_profits_df = (window_profits_df[(window_profits_df['date'] >= pd.to_datetime(min(imputation_dates))) &\n",
    "                                    (window_profits_df['date'] <= pd.to_datetime(max(imputation_dates)))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert period start and end balances to transfers for cash flows calculations\n",
    "# ---------------------------------------------------------------------------------\n",
    "def adjust_end_transfers(df, target_date):\n",
    "    df.loc[df['date'] == target_date, 'cash_flow_transfers'] -= df.loc[df['date'] == target_date, 'usd_balance']\n",
    "    return df\n",
    "\n",
    "def adjust_start_transfers(df, target_date):\n",
    "    df.loc[df['date'] == target_date, 'cash_flow_transfers'] = df.loc[df['date'] == target_date, 'usd_balance']\n",
    "    return df\n",
    "\n",
    "# Copy df and add cash flow column\n",
    "adj_profits_df = window_profits_df.copy()\n",
    "adj_profits_df['cash_flow_transfers'] = adj_profits_df['usd_net_transfers']\n",
    "\n",
    "# Adjust imputed rows\n",
    "adj_profits_df = adjust_start_transfers(adj_profits_df,wallets_config['training_data']['training_period_start'])\n",
    "adj_profits_df = adjust_end_transfers(adj_profits_df,wallets_config['training_data']['training_period_end'])\n",
    "adj_profits_df = adjust_start_transfers(adj_profits_df,wallets_config['training_data']['modeling_period_start'])\n",
    "adj_profits_df = adjust_end_transfers(adj_profits_df,wallets_config['training_data']['modeling_period_end'])\n",
    "\n",
    "# Round final values\n",
    "adj_profits_df['usd_net_transfers'] = np.trunc(adj_profits_df['usd_net_transfers'])\n",
    "\n",
    "# Sort records\n",
    "adj_profits_df = adj_profits_df.sort_values(['wallet_address','coin_id','date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wallet-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute wallet level metrics\n",
    "# -------------------------------\n",
    "# Split training and modeling profits_df\n",
    "training_profits_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['training_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['training_period_end']))\n",
    "].copy()\n",
    "\n",
    "modeling_profits_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['modeling_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['modeling_period_end']))\n",
    "].copy()\n",
    "\n",
    "# Compute wallet level metrics\n",
    "training_wallet_metrics_df = wf.calculate_wallet_level_metrics(training_profits_df)\n",
    "modeling_wallet_metrics_df = wf.calculate_wallet_level_metrics(modeling_profits_df)\n",
    "filtered_modeling_wallet_metrics_df = modeling_wallet_metrics_df[modeling_wallet_metrics_df['invested']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# 5. Apply filters based on wallet-level metrics\n",
    "# ----------------------------------------------\n",
    "filtered_training_wallet_metrics_df = wu.apply_wallet_thresholds(training_wallet_metrics_df, wallets_config)\n",
    "\n",
    "filtered_training_wallet_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and modeling returns\n",
    "merged_df = filtered_training_wallet_metrics_df.join(\n",
    "    filtered_modeling_wallet_metrics_df['return'],\n",
    "    rsuffix='_modeling',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "merged_df['return'] = m.winsorize(merged_df['return'],0.05)\n",
    "merged_df['return_modeling'] = m.winsorize(merged_df['return_modeling'],0.05)\n",
    "\n",
    "merged_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merged_df Supplemental Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# Upload the wallets in the modeling cohort\n",
    "wu.upload_wallet_cohort(merged_df)\n",
    "\n",
    "# Retrieve the buy numbers for wallets in the cohort\n",
    "buyer_numbers_df = wcf.retrieve_buyer_numbers()\n",
    "\n",
    "# Append buyer numbers to the merged_df\n",
    "buyer_averages_df = buyer_numbers_df.groupby('wallet_id').mean('buyer_number')\n",
    "buyer_averages_df.columns = ['average_buyer_number']\n",
    "merged_df = merged_df.join(buyer_averages_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.join(buyer_averages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "df[df['unique_coins_held']>=2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "df = merged_df.copy()\n",
    "df[df['unique_coins_held']>=3]\n",
    "\n",
    "# Assuming your dataframe is called 'df'\n",
    "# Separate features and target\n",
    "X = df.drop(['return_modeling', 'return'], axis=1)  # dropping both return columns\n",
    "y = df['return_modeling']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create preprocessing steps\n",
    "numeric_features = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# For ROC AUC, we need to convert the problem to binary classification\n",
    "# Let's use the median as a threshold\n",
    "y_test_binary = (y_test > y_test.median()).astype(int)\n",
    "y_pred_binary = (y_pred > np.median(y_pred)).astype(int)\n",
    "roc_auc = roc_auc_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': pipeline.named_steps['regressor'].feature_importances_\n",
    "})\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': pipeline.named_steps['regressor'].feature_importances_\n",
    "})\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1320621\n",
    "\n",
    "profits_df[profits_df['wallet_address']==w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_training_wallet_metrics_df.loc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_training_wallet_metrics_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "profits_df = training_profits_df.copy().set_index('wallet_address')\n",
    "\n",
    "# Precompute necessary transformations\n",
    "profits_df['abs_usd_net_transfers'] = profits_df['usd_net_transfers'].abs()\n",
    "profits_df['cumsum_usd_net_transfers'] = profits_df.groupby('wallet_address')['usd_net_transfers'].cumsum()\n",
    "\n",
    "# Group and aggregate metrics\n",
    "wallet_metrics_df = profits_df.groupby('wallet_address').agg(\n",
    "    invested=('cumsum_usd_net_transfers', 'max'),\n",
    "    total_net_transfers=('usd_net_transfers', 'sum'),\n",
    "    unique_coins=('coin_id', 'nunique'),\n",
    "    transaction_days=('date', 'count'),\n",
    "    total_volume=('abs_usd_net_transfers', 'sum'),\n",
    "    average_transaction=('abs_usd_net_transfers', 'mean')\n",
    ")\n",
    "\n",
    "\n",
    "# Compute additional derived metrics\n",
    "wallet_metrics_df['net_gain'] = -wallet_metrics_df['total_net_transfers']\n",
    "wallet_metrics_df['return'] = wallet_metrics_df['net_gain'] / wallet_metrics_df['invested']\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate amount invested\n",
    "# wallet_invested_df = pd.DataFrame(\n",
    "#     profits_df\n",
    "#     .groupby(level='wallet_address')['usd_net_transfers'].cumsum()\n",
    "#     .groupby(level='wallet_address').max()\n",
    "# )\n",
    "# wallet_invested_df.columns = ['invested']\n",
    "\n",
    "# # Calculate net gains\n",
    "# wallet_gain_df = pd.DataFrame(\n",
    "#     -profits_df.groupby(level='wallet_address')['usd_net_transfers'].sum()\n",
    "# )\n",
    "# wallet_gain_df.columns = ['net_gain']\n",
    "\n",
    "# # Join dfs\n",
    "# wallet_performance_df = wallet_invested_df.join(wallet_gain_df)\n",
    "\n",
    "# # Compute return\n",
    "# wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "\n",
    "# return wallet_performance_df\n",
    "wallet_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "\n",
    "\n",
    "# Split out modeling and training records to calculate return separately\n",
    "modeling_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['modeling_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['modeling_period_end']))\n",
    "]\n",
    "modeling_performance_df = wf.calculate_wallet_investment_return(modeling_df)\n",
    "\n",
    "\n",
    "training_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(wallets_config['training_data']['training_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(wallets_config['training_data']['training_period_end']))\n",
    "]\n",
    "training_performance_df = wf.calculate_wallet_investment_return(training_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_invested = 10000\n",
    "filtered_df = training_performance_df[training_performance_df['invested']>=min_invested]\n",
    "print(training_performance_df.shape)\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*5)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*5)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_correlation_matrix(df):\n",
    "    \"\"\"\n",
    "    Create and visualize a correlation matrix for the given DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Correlation matrix\n",
    "    \"\"\"\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix,\n",
    "                annot=True,  # Show correlation values\n",
    "                cmap='coolwarm',  # Color scheme from red (negative) to blue (positive)\n",
    "                vmin=-1, vmax=1,  # Fix the scale\n",
    "                center=0,  # Center the colormap at 0\n",
    "                fmt='.2f')  # Round to 2 decimal places\n",
    "\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "# create_correlation_matrix(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.set_index('coin_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_analysis_df = market_data_df.copy().set_index('coin_id')\n",
    "\n",
    "start_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_start'])]\n",
    "end_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_end'])]\n",
    "\n",
    "# coin_modeling_returns_df = start_prices.join(end_prices)\n",
    "\n",
    "    # (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['modeling_period_start'])) &\n",
    "    # (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['modeling_period_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coin returns during modeling period\n",
    "coin_modeling_returns_df = start_prices[['price']].join(end_prices[['price']],lsuffix='_start',rsuffix='_end')\n",
    "coin_modeling_returns_df['coin_modeling_return'] = coin_modeling_returns_df['price_end']/coin_modeling_returns_df['price_start']\n",
    "coin_modeling_returns_df[\"coin_modeling_percentile_return\"] = coin_modeling_returns_df[\"coin_modeling_return\"].rank(ascending=True, pct=True)\n",
    "\n",
    "coin_modeling_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wallet ending balances\n",
    "min_end_balance = 1000\n",
    "\n",
    "# Calculate period end balance for each coin-wallet pair\n",
    "end_balances_df = adj_profits_df[adj_profits_df['date']==pd.to_datetime(config['training_data']['training_period_end'])]\n",
    "end_balances_df = end_balances_df[end_balances_df['usd_net_transfers']<=-min_end_balance]\n",
    "end_balances_df['usd_balance'] = end_balances_df['usd_net_transfers'].abs()\n",
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]\n",
    "end_balances_df = end_balances_df.set_index(['coin_id','wallet_address'])\n",
    "end_balances_df.head()\n",
    "\n",
    "# Add wallet performance metrics\n",
    "end_balances_df = end_balances_df.join(performance_df,on='wallet_address')\n",
    "end_balances_df = end_balances_df[end_balances_df['return_training'].notna()]\n",
    "\n",
    "end_balances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_return_column = 'return_training'\n",
    "\n",
    "# Assess average wallet return during training period\n",
    "coin_wallet_performance = pd.DataFrame(end_balances_df.reset_index().groupby('coin_id',observed=True)[wallet_return_column].mean())\n",
    "coin_wallet_performance.columns = ['avg_wallet_training_return']\n",
    "\n",
    "coin_wallet_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coin_return_column = 'coin_modeling_return'\n",
    "coin_return_column = 'coin_modeling_percentile_return'\n",
    "\n",
    "wallet_forecast_df = coin_modeling_returns_df[[coin_return_column]].join(coin_wallet_performance)\n",
    "wallet_forecast_df[coin_return_column].corr(wallet_forecast_df['avg_wallet_training_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_forecast_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.loc['0037051e-677f-439f-9353-4dc896fe9ecd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df = aggregate_wallet_performance(training_df)\n",
    "training_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_performance_df = aggregate_wallet_performance(modeling_df)\n",
    "modeling_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*10)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*10)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_performance_df = training_performance_df\n",
    "wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "wallet_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_performance_df.sample(10)\n",
    "\n",
    "wallet_performance_df[wallet_performance_df['invested']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0xca6cfaa7d61371310d84b63a4ca90cbf7883a9db'\n",
    "\n",
    "df = wallets_df_filtered.loc[w]\n",
    "\n",
    "# print(xirr(df.index.get_level_values('date'), df['usd_net_transfers']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_profits_df[adj_profits_df['wallet_address']==w]\n",
    "profits_df[profits_df['wallet_address']==w].sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df[adj_profits_df['wallet_address']==w].sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wallets_xirr(profits_df, min_wallet_volume):\n",
    "    \"\"\"\n",
    "    Calculates the XIRR of each wallet based on their cash flows across all coins they've\n",
    "    interacted with in profits_df.\n",
    "\n",
    "    Parameters:\n",
    "    - profits_df (pd.DataFrame): shows daily coin-wallet transfers in USD\n",
    "    - min_wallet_volume (int): wallets with less than this total USD volume will be excluded\n",
    "\n",
    "    Returns:\n",
    "    - xirr_df (pd.DataFrame): shows the XIRR of each wallet over the provided transactions\n",
    "    \"\"\"\n",
    "    logger.info('Beginning XIRR calculation sequence...')\n",
    "\n",
    "    # 1. Summarize cash flows on a wallet level\n",
    "    # -----------------------------------------\n",
    "    # Sum cash flows on a wallet level\n",
    "    wallets_df = pd.DataFrame(profits_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "    # 2. Filter wallets on data quality\n",
    "    # ---------------------------------\n",
    "    # Identify wallets with no transactions\n",
    "    wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "    low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "    # Remove transactionless wallets\n",
    "    wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "    logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "    # Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "    wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "        lambda x: (x > 0).any() and (x < 0).any()\n",
    "    )\n",
    "    wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "    # Filter wallet addresses that do not have both positive and negative transfers\n",
    "    wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "    logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "    # 3. Calculate XIRR\n",
    "    # -----------------\n",
    "    # Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "    start_time = time.time()\n",
    "    logger.info('Calculating XIRR values...')\n",
    "    xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "        lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    "    )\n",
    "    logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    xirr_df = pd.DataFrame(xirr_results)\n",
    "    xirr_df.columns = ['xirr']\n",
    "\n",
    "    # Fill empty values with 0s\n",
    "    xirr_df = xirr_df.fillna(0)\n",
    "\n",
    "\n",
    "    return xirr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 1\n",
    "\n",
    "# 1. Summarize cash flows on a wallet level\n",
    "# -----------------------------------------\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(modeling_df.copy().groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "# 2. Filter wallets on data quality\n",
    "# ---------------------------------\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "# Filter wallet addresses that do not have both positive and negative transfers\n",
    "wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "# 3. Calculate XIRR\n",
    "# -----------------\n",
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "start_time = time.time()\n",
    "logger.info('Calculating XIRR values...')\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Fill empty values with 0s\n",
    "xirr_df = xirr_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: -df['usd_net_transfers'].sum()/df['usd_net_transfers'].cumsum().max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df.loc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wallet_metrics(group):\n",
    "    cumsum = group['usd_net_transfers'].cumsum()\n",
    "    invested = cumsum.max()\n",
    "    net_gain = group['usd_net_transfers'].sum()\n",
    "\n",
    "    return pd.Series({\n",
    "        'invested': invested,\n",
    "        'net_gain': net_gain,\n",
    "        'return': net_gain/invested if invested != 0 else np.nan\n",
    "    })\n",
    "\n",
    "# Calculate metrics for all wallets at once\n",
    "results = wallets_df_filtered.groupby(level='wallet_address').apply(wallet_metrics)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 10000\n",
    "\n",
    "# Calculate XIRR\n",
    "training_xirr_df = calculate_wallets_xirr(training_df,min_wallet_volume)\n",
    "modeling_xirr_df = calculate_wallets_xirr(modeling_df,min_wallet_volume=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate percentiles\n",
    "xirr_df[\"training_xirr_percentile\"] = xirr_df[\"training_xirr\"].rank(ascending=True, pct=True)\n",
    "xirr_df[\"modeling_xirr_percentile\"] = xirr_df[\"modeling_xirr\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "xirr_df['training_xirr_decile'] = np.ceil(xirr_df['training_xirr_percentile']*10)\n",
    "xirr_df['modeling_xirr_decile'] = np.ceil(xirr_df['modeling_xirr_percentile']*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    xirr_df['training_xirr_decile'],\n",
    "    xirr_df['modeling_xirr_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = training_xirr_df.rename(columns={'xirr': 'training_xirr'}).join(\n",
    "    modeling_xirr_df.rename(columns={'xirr': 'modeling_xirr'}),\n",
    "    how='inner'\n",
    ").fillna({'modeling_xirr': 0})\n",
    "\n",
    "\n",
    "\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df['training_xirr_percentile'].corr(xirr_df['modeling_xirr_percentile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(training_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "\n",
    "# Filter wallet addresses that do not meet the condition\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "logger.info('Found %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = '0x036783df7aec54b5dfca9e1f870577bbcca95481'\n",
    "# wallets_df.loc[w]\n",
    "\n",
    "# profits_df[profits_df['wallet_address']==w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIRR sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0x0000000000000000000000000000000000000014'\n",
    "\n",
    "dates = wallets_df.loc[w].index.values\n",
    "cash_flows = wallets_df.loc[w]['usd_net_transfers']\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(xirr_results.shape)\n",
    "xirr_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xirr(dates,cash_flows)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '77e2cf4b-d18a-4026-a2f2-f083f48fe1be'\n",
    "w = '0xaff2943cfe3e95f66142a1729079418d78e42236'\n",
    "\n",
    "# u.cw_filter_df(training_df,c,w)\n",
    "\n",
    "df = u.cw_filter_df(training_df,c,w)\n",
    "df = df.sort_values('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['date']\n",
    "cash_flows = df['usd_net_transfers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxirr import xirr\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fractions = (np.datetime64(dates) - np.datetime64(dates[0])).astype('timedelta64[D]') / np.timedelta64(1, 'Y')\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     select cwp.wallet_address\n",
    "#     ,cwp.coin_id\n",
    "#     ,cwp.date\n",
    "#     ,round(cwp.usd_net_transfers) as usd_net_transfers\n",
    "#     ,round(cwp.usd_balance) as usd_balance\n",
    "#     ,round(cwp.usd_net_transfers/cmd.price) as token_transfers\n",
    "#     ,round(cwp.usd_balance/cmd.price) as token_balance\n",
    "#     ,cmd.price\n",
    "#     from wallets w\n",
    "#     join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#     join core.coin_wallet_profits cwp on cwp.wallet_address = wc.wallet_address\n",
    "#         and cwp.coin_id = wc.coin_id\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = cwp.coin_id\n",
    "#         and cmd.date = cwp.date\n",
    "#     order by 1,2,3\n",
    "#     '''\n",
    "# transfers_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert wallet_address to categorical, store the mapping, and convert the column to int32\n",
    "# wallet_address_categorical = transfers_df['wallet_address'].astype('category')\n",
    "# # wallet_address_mapping = wallet_address_categorical.cat.categories\n",
    "# # transfers_df['wallet_address'] = wallet_address_categorical.cat.codes.astype('uint32')\n",
    "\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# transfers_df['coin_id'] = transfers_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_net_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'price', 'float32')\n",
    "\n",
    "# print(transfers_df.info())\n",
    "# print(u.df_mem(transfers_df))\n",
    "# transfers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     ,coins as (\n",
    "#         select wc.coin_id\n",
    "#         from wallets w\n",
    "#         join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#         group by 1\n",
    "#     )\n",
    "\n",
    "#     select cmd.coin_id\n",
    "#     ,cmd.date\n",
    "#     ,cmd.price\n",
    "#     ,cmd.market_cap\n",
    "#     from coins c\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = c.coin_id\n",
    "#     order by 1,2\n",
    "#     '''\n",
    "# prices_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# prices_df['coin_id'] = prices_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# prices_df = u.safe_downcast(prices_df, 'price', 'float32')\n",
    "# prices_df = u.safe_downcast(prices_df, 'market_cap', 'int32')\n",
    "\n",
    "# print(prices_df.info())\n",
    "# print(u.df_mem(prices_df))\n",
    "# prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_multiple_coins_per_wallet():\n",
    "\"\"\"\n",
    "Test scenario where wallets own multiple coins, some exceeding thresholds when aggregated.\n",
    "Checks filtering based on specified date range.\n",
    "\"\"\"\n",
    "# Create test data\n",
    "sample_profits_df = pd.DataFrame({\n",
    "    'coin_id': ['BTC', 'ETH', 'BTC', 'ETH', 'LTC', 'BTC', 'ETH'],\n",
    "    'wallet_address': ['wallet1', 'wallet1', 'wallet2', 'wallet2', 'wallet2',\n",
    "                        'wallet3', 'wallet3'],\n",
    "    'date': pd.date_range(start='2023-01-01', periods=7),\n",
    "    'profits_cumulative': [5000, 3000, 1000, 500, 500, 100, 50],\n",
    "    'usd_inflows_cumulative': [10000, 8000, 2000, 1500, 1500, 500, 250]\n",
    "})\n",
    "\n",
    "config = {\n",
    "    'profitability_filter': 7500,\n",
    "    'inflows_filter': 15000,\n",
    "    'date_range': {\n",
    "        'start': '2023-01-02',\n",
    "        'end': '2023-01-05'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the function with date range\n",
    "cleaned_df, exclusions_logs_df = dr.clean_profits_df(\n",
    "    sample_profits_df,\n",
    "    config,\n",
    "    earliest_date=config['date_range']['start'],\n",
    "    latest_date=config['date_range']['end']\n",
    ")\n",
    "\n",
    "# Expected results - only checking within date window but removing all records\n",
    "expected_cleaned_df = sample_profits_df[\n",
    "    sample_profits_df['wallet_address'].isin(['wallet2', 'wallet3'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "expected_exclusions = pd.DataFrame({\n",
    "    'wallet_address': ['wallet1'],\n",
    "    'profits_exclusion': [True],\n",
    "    'inflows_exclusion': [True]\n",
    "})\n",
    "\n",
    "# Assertions\n",
    "assert len(cleaned_df) == 5  # wallet2 (3 records) and wallet3 (2 records) should remain\n",
    "assert np.array_equal(cleaned_df.values, expected_cleaned_df.values)\n",
    "assert np.array_equal(exclusions_logs_df.values, expected_exclusions.values)\n",
    "\n",
    "# Check if profits and inflows are approximately correct for the remaining wallets\n",
    "# Should include ALL records for passing wallets (1000 + 500 + 500 + 100 + 50)\n",
    "assert pytest.approx(cleaned_df['profits_cumulative'].sum(), abs=1e-4) == 2150\n",
    "# Should include ALL records for passing wallets (2000 + 1500 + 1500 + 500 + 250)\n",
    "assert pytest.approx(cleaned_df['usd_inflows_cumulative'].sum(), abs=1e-4) == 5750\n",
    "\n",
    "# Additional date-specific checks\n",
    "date_mask = ((cleaned_df['date'] >= config['date_range']['start']) &\n",
    "                (cleaned_df['date'] <= config['date_range']['end']))\n",
    "date_filtered = cleaned_df[date_mask]\n",
    "\n",
    "# Verify we have the expected number of records in the date range\n",
    "assert len(date_filtered) == 3  # Should only have records between Jan 2-5 for remaining wallets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
