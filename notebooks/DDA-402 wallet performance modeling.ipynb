{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "from typing import Dict,Union,List,Any,Tuple\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "from pyxirr import xirr\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "# config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# wallets config\n",
    "config_path = '../config/wallets_config.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as file:\n",
    "    wallets_config = yaml.safe_load(file)\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = '../config/wallets_config.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as file:\n",
    "    wallets_config = yaml.safe_load(file)\n",
    "\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "\n",
    "# 1. Data Retrieval, Cleaning, Indicator Calculation\n",
    "# --------------------------------------------------\n",
    "# Market data: retrieve and clean full history\n",
    "market_data_df = dr.retrieve_market_data()\n",
    "market_data_df = dr.clean_market_data(\n",
    "    market_data_df,\n",
    "    wallets_config,\n",
    "    wallets_config['training_data']['earliest_transfer_date'],\n",
    "    wallets_config['training_data']['latest_transfer_date']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3(93656) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
    "# [03/Dec/2024 12:02:39] INFO [data_retrieval.retrieve_market_data:61] Retrieved market_data_df with 8431 unique coins and 4904096 rows after 26.2 seconds\n",
    "# [03/Dec/2024 12:02:40] INFO [data_retrieval.clean_market_data:98] Max gap days threshold of 25 day removed 538786 market data records for 605 coins.\n",
    "# [03/Dec/2024 12:02:40] INFO [data_retrieval.clean_market_data:112] Min daily volume threshold of $5000 removed 278259 additional market data records for 577 coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# Profits: retrieve and clean profits data spanning the earliest to latest training periods\n",
    "profits_df = dr.retrieve_profits_data(wallets_config['training_data']['earliest_transfer_date'],\n",
    "                                    wallets_config['training_data']['latest_transfer_date'],\n",
    "                                    wallets_config['data_cleaning']['minimum_wallet_inflows'])\n",
    "profits_df, _ = dr.clean_profits_df(profits_df, wallets_config['data_cleaning'])\n",
    "\n",
    "profits_df.head()\n",
    "\n",
    "\n",
    "# Filter market_data to only coins with transfers data if configured to\n",
    "if wallets_config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "# Create prices_df: lightweight reference for other functions\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# Filter profits_df to remove records for any coins that were removed in data cleaning\n",
    "profits_df = profits_df[profits_df['coin_id'].isin(market_data_df['coin_id'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Impute all required dates\n",
    "# ----------------------------\n",
    "# Identify all required imputation dates\n",
    "imputation_dates = [\n",
    "    '2024-01-01',\n",
    "    '2024-06-30',\n",
    "    '2024-07-01',\n",
    "    wallets_config['training_data']['modeling_period_end']\n",
    "]\n",
    "\n",
    "# Impute all required dates\n",
    "window_profits_df = pri.impute_profits_for_multiple_dates(profits_df, prices_df, imputation_dates, n_threads=24)\n",
    "window_profits_df = (window_profits_df[(window_profits_df['date'] >= pd.to_datetime(min(imputation_dates))) &\n",
    "                                    (window_profits_df['date'] <= pd.to_datetime(max(imputation_dates)))])\n",
    "\n",
    "window_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert period start and end balances to transfers for cash flows calculations\n",
    "def adjust_end_transfers(df, target_date):\n",
    "    df.loc[df['date'] == target_date, 'usd_net_transfers'] -= df.loc[df['date'] == target_date, 'usd_balance']\n",
    "    df.loc[df['date'] == target_date, 'usd_balance'] = 0\n",
    "    return df\n",
    "\n",
    "def adjust_start_transfers(df, target_date):\n",
    "    df.loc[df['date'] == target_date, 'usd_net_transfers'] = df.loc[df['date'] == target_date, 'usd_balance']\n",
    "    return df\n",
    "\n",
    "adj_profits_df = window_profits_df.copy()\n",
    "\n",
    "adj_profits_df = adjust_start_transfers(adj_profits_df,config['training_data']['training_period_start'])\n",
    "adj_profits_df = adjust_end_transfers(adj_profits_df,config['training_data']['training_period_end'])\n",
    "adj_profits_df = adjust_start_transfers(adj_profits_df,config['training_data']['modeling_period_start'])\n",
    "adj_profits_df = adjust_end_transfers(adj_profits_df,config['training_data']['modeling_period_end'])\n",
    "\n",
    "# Round final values\n",
    "adj_profits_df['usd_net_transfers'] = np.trunc(adj_profits_df['usd_net_transfers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove wallets with no modeling period balances or activity\n",
    "modeling_df_full = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['modeling_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['modeling_period_end']))\n",
    "]\n",
    "inactive_wallets = modeling_df_full.groupby('wallet_address')['usd_net_transfers'] \\\n",
    "    .apply(lambda x: x.abs().sum()) \\\n",
    "    .loc[lambda x: x == 0] \\\n",
    "    .index\n",
    "\n",
    "adj_profits_df = adj_profits_df[~adj_profits_df['wallet_address'].isin(inactive_wallets)]\n",
    "logger.info(\"Removed records for %s wallets with no modeling period balances or transfers.\",\n",
    "            len(inactive_wallets))\n",
    "\n",
    "# Sort records\n",
    "adj_profits_df = adj_profits_df.sort_values(['wallet_address','coin_id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wallet_performance(profits_df):\n",
    "\n",
    "    profits_df = profits_df.set_index('wallet_address')\n",
    "\n",
    "    # Calculate amount invested\n",
    "    wallet_invested_df = pd.DataFrame(\n",
    "        profits_df\n",
    "        .groupby(level='wallet_address')['usd_net_transfers'].cumsum()\n",
    "        .groupby(level='wallet_address').max()\n",
    "    )\n",
    "    wallet_invested_df.columns = ['invested']\n",
    "\n",
    "    # Calculate net gains\n",
    "    wallet_gain_df = pd.DataFrame(\n",
    "        -profits_df.groupby(level='wallet_address')['usd_net_transfers'].sum()\n",
    "    )\n",
    "    wallet_gain_df.columns = ['net_gain']\n",
    "\n",
    "    # Join dfs\n",
    "    wallet_performance_df = wallet_invested_df.join(wallet_gain_df)\n",
    "\n",
    "    # Compute return\n",
    "    wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "\n",
    "    return wallet_performance_df\n",
    "\n",
    "\n",
    "\n",
    "# Split out modeling and training records to calculate return separately\n",
    "modeling_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['modeling_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['modeling_period_end']))\n",
    "]\n",
    "modeling_performance_df = aggregate_wallet_performance(modeling_df)\n",
    "\n",
    "\n",
    "training_df = adj_profits_df[\n",
    "    (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['training_period_start'])) &\n",
    "    (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['training_period_end']))\n",
    "]\n",
    "training_performance_df = aggregate_wallet_performance(training_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_invested = 10000\n",
    "filtered_df = training_performance_df[training_performance_df['invested']>=min_invested]\n",
    "print(training_performance_df.shape)\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*10)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*10)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_correlation_matrix(df):\n",
    "    \"\"\"\n",
    "    Create and visualize a correlation matrix for the given DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Correlation matrix\n",
    "    \"\"\"\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr(method='pearson')\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix,\n",
    "                annot=True,  # Show correlation values\n",
    "                cmap='coolwarm',  # Color scheme from red (negative) to blue (positive)\n",
    "                vmin=-1, vmax=1,  # Fix the scale\n",
    "                center=0,  # Center the colormap at 0\n",
    "                fmt='.2f')  # Round to 2 decimal places\n",
    "\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "# create_correlation_matrix(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.set_index('coin_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_analysis_df = market_data_df.copy().set_index('coin_id')\n",
    "\n",
    "start_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_start'])]\n",
    "end_prices = price_analysis_df[price_analysis_df['date']== pd.to_datetime(config['training_data']['modeling_period_end'])]\n",
    "\n",
    "# coin_modeling_returns_df = start_prices.join(end_prices)\n",
    "\n",
    "    # (adj_profits_df['date'] >= pd.to_datetime(config['training_data']['modeling_period_start'])) &\n",
    "    # (adj_profits_df['date'] <= pd.to_datetime(config['training_data']['modeling_period_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coin returns during modeling period\n",
    "coin_modeling_returns_df = start_prices[['price']].join(end_prices[['price']],lsuffix='_start',rsuffix='_end')\n",
    "coin_modeling_returns_df['coin_modeling_return'] = coin_modeling_returns_df['price_end']/coin_modeling_returns_df['price_start']\n",
    "coin_modeling_returns_df[\"coin_modeling_percentile_return\"] = coin_modeling_returns_df[\"coin_modeling_return\"].rank(ascending=True, pct=True)\n",
    "\n",
    "coin_modeling_returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate wallet ending balances\n",
    "min_end_balance = 1000\n",
    "\n",
    "# Calculate period end balance for each coin-wallet pair\n",
    "end_balances_df = adj_profits_df[adj_profits_df['date']==pd.to_datetime(config['training_data']['training_period_end'])]\n",
    "end_balances_df = end_balances_df[end_balances_df['usd_net_transfers']<=-min_end_balance]\n",
    "end_balances_df['usd_balance'] = end_balances_df['usd_net_transfers'].abs()\n",
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]\n",
    "end_balances_df = end_balances_df.set_index(['coin_id','wallet_address'])\n",
    "end_balances_df.head()\n",
    "\n",
    "# Add wallet performance metrics\n",
    "end_balances_df = end_balances_df.join(performance_df,on='wallet_address')\n",
    "end_balances_df = end_balances_df[end_balances_df['return_training'].notna()]\n",
    "\n",
    "end_balances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_return_column = 'return_training'\n",
    "\n",
    "# Assess average wallet return during training period\n",
    "coin_wallet_performance = pd.DataFrame(end_balances_df.reset_index().groupby('coin_id',observed=True)[wallet_return_column].mean())\n",
    "coin_wallet_performance.columns = ['avg_wallet_training_return']\n",
    "\n",
    "coin_wallet_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coin_return_column = 'coin_modeling_return'\n",
    "coin_return_column = 'coin_modeling_percentile_return'\n",
    "\n",
    "wallet_forecast_df = coin_modeling_returns_df[[coin_return_column]].join(coin_wallet_performance)\n",
    "wallet_forecast_df[coin_return_column].corr(wallet_forecast_df['avg_wallet_training_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_forecast_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_balances_df = end_balances_df[['coin_id','wallet_address','usd_balance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prices.loc['0037051e-677f-439f-9353-4dc896fe9ecd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_performance_df = aggregate_wallet_performance(training_df)\n",
    "training_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_performance_df = aggregate_wallet_performance(modeling_df)\n",
    "modeling_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training and modeling data\n",
    "performance_df = filtered_df[['return']].join(modeling_performance_df[['return']],lsuffix='_training',rsuffix='_modeling')\n",
    "performance_df.shape\n",
    "\n",
    "# Calculate percentiles\n",
    "performance_df[\"training_percentile\"] = performance_df[\"return_training\"].rank(ascending=True, pct=True)\n",
    "performance_df[\"modeling_percentile\"] = performance_df[\"return_modeling\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "performance_df['training_decile'] = np.ceil(performance_df['training_percentile']*10)\n",
    "performance_df['modeling_decile'] = np.ceil(performance_df['modeling_percentile']*10)\n",
    "\n",
    "# Check correlation\n",
    "performance_df['training_percentile'].corr(performance_df['modeling_percentile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    performance_df['training_decile'],\n",
    "    performance_df['modeling_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_performance_df = training_performance_df\n",
    "wallet_performance_df['return'] = wallet_performance_df['net_gain']/wallet_performance_df['invested']\n",
    "wallet_performance_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wallet_performance_df.sample(10)\n",
    "\n",
    "wallet_performance_df[wallet_performance_df['invested']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0xca6cfaa7d61371310d84b63a4ca90cbf7883a9db'\n",
    "\n",
    "df = wallets_df_filtered.loc[w]\n",
    "\n",
    "# print(xirr(df.index.get_level_values('date'), df['usd_net_transfers']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_profits_df[adj_profits_df['wallet_address']==w]\n",
    "profits_df[profits_df['wallet_address']==w].sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_profits_df[adj_profits_df['wallet_address']==w].sort_values('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wallets_xirr(profits_df, min_wallet_volume):\n",
    "    \"\"\"\n",
    "    Calculates the XIRR of each wallet based on their cash flows across all coins they've\n",
    "    interacted with in profits_df.\n",
    "\n",
    "    Parameters:\n",
    "    - profits_df (pd.DataFrame): shows daily coin-wallet transfers in USD\n",
    "    - min_wallet_volume (int): wallets with less than this total USD volume will be excluded\n",
    "\n",
    "    Returns:\n",
    "    - xirr_df (pd.DataFrame): shows the XIRR of each wallet over the provided transactions\n",
    "    \"\"\"\n",
    "    logger.info('Beginning XIRR calculation sequence...')\n",
    "\n",
    "    # 1. Summarize cash flows on a wallet level\n",
    "    # -----------------------------------------\n",
    "    # Sum cash flows on a wallet level\n",
    "    wallets_df = pd.DataFrame(profits_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "    # 2. Filter wallets on data quality\n",
    "    # ---------------------------------\n",
    "    # Identify wallets with no transactions\n",
    "    wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "    low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "    # Remove transactionless wallets\n",
    "    wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "    logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "    # Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "    wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "        lambda x: (x > 0).any() and (x < 0).any()\n",
    "    )\n",
    "    wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "    # Filter wallet addresses that do not have both positive and negative transfers\n",
    "    wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "    logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "    # 3. Calculate XIRR\n",
    "    # -----------------\n",
    "    # Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "    start_time = time.time()\n",
    "    logger.info('Calculating XIRR values...')\n",
    "    xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "        lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    "    )\n",
    "    logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    xirr_df = pd.DataFrame(xirr_results)\n",
    "    xirr_df.columns = ['xirr']\n",
    "\n",
    "    # Fill empty values with 0s\n",
    "    xirr_df = xirr_df.fillna(0)\n",
    "\n",
    "\n",
    "    return xirr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 1\n",
    "\n",
    "# 1. Summarize cash flows on a wallet level\n",
    "# -----------------------------------------\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(modeling_df.copy().groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "\n",
    "# 2. Filter wallets on data quality\n",
    "# ---------------------------------\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]\n",
    "logger.info('Removed %s wallets with volume below $%s.', len(low_volume_wallets), min_wallet_volume)\n",
    "\n",
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "\n",
    "# Filter wallet addresses that do not have both positive and negative transfers\n",
    "wallets_df_filtered = wallets_df_filtered[~wallets_df_filtered.index.get_level_values('wallet_address').isin(wallets_missing_both)]\n",
    "logger.info('Removed %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "# 3. Calculate XIRR\n",
    "# -----------------\n",
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\\\n",
    "start_time = time.time()\n",
    "logger.info('Calculating XIRR values...')\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "logger.info('XIRR calculations complete after %.2f seconds.', time.time() - start_time)\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Fill empty values with 0s\n",
    "xirr_df = xirr_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: -df['usd_net_transfers'].sum()/df['usd_net_transfers'].cumsum().max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df.loc[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wallet_metrics(group):\n",
    "    cumsum = group['usd_net_transfers'].cumsum()\n",
    "    invested = cumsum.max()\n",
    "    net_gain = group['usd_net_transfers'].sum()\n",
    "\n",
    "    return pd.Series({\n",
    "        'invested': invested,\n",
    "        'net_gain': net_gain,\n",
    "        'return': net_gain/invested if invested != 0 else np.nan\n",
    "    })\n",
    "\n",
    "# Calculate metrics for all wallets at once\n",
    "results = wallets_df_filtered.groupby(level='wallet_address').apply(wallet_metrics)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wallet_volume = 10000\n",
    "\n",
    "# Calculate XIRR\n",
    "training_xirr_df = calculate_wallets_xirr(training_df,min_wallet_volume)\n",
    "modeling_xirr_df = calculate_wallets_xirr(modeling_df,min_wallet_volume=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate percentiles\n",
    "xirr_df[\"training_xirr_percentile\"] = xirr_df[\"training_xirr\"].rank(ascending=True, pct=True)\n",
    "xirr_df[\"modeling_xirr_percentile\"] = xirr_df[\"modeling_xirr\"].rank(ascending=True, pct=True)\n",
    "\n",
    "# Calculate decile buckets\n",
    "xirr_df['training_xirr_decile'] = np.ceil(xirr_df['training_xirr_percentile']*10)\n",
    "xirr_df['modeling_xirr_decile'] = np.ceil(xirr_df['modeling_xirr_percentile']*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a cross-tabulation of the deciles\n",
    "heatmap_data = pd.crosstab(\n",
    "    xirr_df['training_xirr_decile'],\n",
    "    xirr_df['modeling_xirr_decile'],\n",
    "    normalize='index'\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"coolwarm\", cbar=True)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Percentage Allocation Heatmap: Training to Modeling Deciles')\n",
    "plt.xlabel('Modeling Decile')\n",
    "plt.ylabel('Training Decile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = training_xirr_df.rename(columns={'xirr': 'training_xirr'}).join(\n",
    "    modeling_xirr_df.rename(columns={'xirr': 'modeling_xirr'}),\n",
    "    how='inner'\n",
    ").fillna({'modeling_xirr': 0})\n",
    "\n",
    "\n",
    "\n",
    "xirr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df['training_xirr_percentile'].corr(xirr_df['modeling_xirr_percentile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sum cash flows on a wallet level\n",
    "wallets_df = pd.DataFrame(training_df.groupby(['wallet_address','date'])['usd_net_transfers'].sum())\n",
    "\n",
    "# Identify wallets with no transactions\n",
    "wallets_agg_df = wallets_df.groupby(level='wallet_address')['usd_net_transfers'].apply(lambda x: x.abs().sum())\n",
    "low_volume_wallets = wallets_agg_df[wallets_agg_df < min_wallet_volume].index\n",
    "\n",
    "# Remove transactionless wallets\n",
    "wallets_df_filtered = wallets_df[~wallets_df.index.get_level_values('wallet_address').isin(low_volume_wallets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address and check for both positive and negative usd_net_transfers\n",
    "wallet_check = wallets_df_filtered.groupby('wallet_address')['usd_net_transfers'].apply(\n",
    "    lambda x: (x > 0).any() and (x < 0).any()\n",
    ")\n",
    "\n",
    "# Filter wallet addresses that do not meet the condition\n",
    "wallets_missing_both = wallet_check[~wallet_check].index\n",
    "logger.info('Found %s wallets missing either a positive or negative transaction.', len(wallets_missing_both))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = '0x036783df7aec54b5dfca9e1f870577bbcca95481'\n",
    "# wallets_df.loc[w]\n",
    "\n",
    "# profits_df[profits_df['wallet_address']==w]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XIRR sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallets_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = '0x0000000000000000000000000000000000000014'\n",
    "\n",
    "dates = wallets_df.loc[w].index.values\n",
    "cash_flows = wallets_df.loc[w]['usd_net_transfers']\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by wallet_address (level of the MultiIndex) and calculate XIRR\n",
    "xirr_results = wallets_df_filtered.groupby(level='wallet_address').apply(\n",
    "    lambda df: xirr(df.index.get_level_values('date'), df['usd_net_transfers'])\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(xirr_results.shape)\n",
    "xirr_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xirr_df = pd.DataFrame(xirr_results)\n",
    "xirr_df.columns = ['xirr']\n",
    "xirr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xirr(dates,cash_flows)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '77e2cf4b-d18a-4026-a2f2-f083f48fe1be'\n",
    "w = '0xaff2943cfe3e95f66142a1729079418d78e42236'\n",
    "\n",
    "# u.cw_filter_df(training_df,c,w)\n",
    "\n",
    "df = u.cw_filter_df(training_df,c,w)\n",
    "df = df.sort_values('date')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['date']\n",
    "cash_flows = df['usd_net_transfers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxirr import xirr\n",
    "\n",
    "xirr(dates,cash_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate year fractions from the first date\n",
    "start_date = dates.min()  # Use the earliest date as the reference\n",
    "date_fractions = (dates - start_date).dt.days / 365.0\n",
    "date_fractions = date_fractions.values\n",
    "\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fractions = (np.datetime64(dates) - np.datetime64(dates[0])).astype('timedelta64[D]') / np.timedelta64(1, 'Y')\n",
    "date_fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     select cwp.wallet_address\n",
    "#     ,cwp.coin_id\n",
    "#     ,cwp.date\n",
    "#     ,round(cwp.usd_net_transfers) as usd_net_transfers\n",
    "#     ,round(cwp.usd_balance) as usd_balance\n",
    "#     ,round(cwp.usd_net_transfers/cmd.price) as token_transfers\n",
    "#     ,round(cwp.usd_balance/cmd.price) as token_balance\n",
    "#     ,cmd.price\n",
    "#     from wallets w\n",
    "#     join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#     join core.coin_wallet_profits cwp on cwp.wallet_address = wc.wallet_address\n",
    "#         and cwp.coin_id = wc.coin_id\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = cwp.coin_id\n",
    "#         and cmd.date = cwp.date\n",
    "#     order by 1,2,3\n",
    "#     '''\n",
    "# transfers_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert wallet_address to categorical, store the mapping, and convert the column to int32\n",
    "# wallet_address_categorical = transfers_df['wallet_address'].astype('category')\n",
    "# # wallet_address_mapping = wallet_address_categorical.cat.categories\n",
    "# # transfers_df['wallet_address'] = wallet_address_categorical.cat.codes.astype('uint32')\n",
    "\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# transfers_df['coin_id'] = transfers_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_net_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'usd_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_transfers', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'token_balance', 'float32')\n",
    "# transfers_df = u.safe_downcast(transfers_df, 'price', 'float32')\n",
    "\n",
    "# print(transfers_df.info())\n",
    "# print(u.df_mem(transfers_df))\n",
    "# transfers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sql = '''\n",
    "#     with wallet_coins as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,coin_id\n",
    "#             ,max(usd_inflows_cumulative) as coin_inflows\n",
    "#             from core.coin_wallet_profits\n",
    "#             group by 1,2\n",
    "#         )\n",
    "#         where coin_inflows > 500\n",
    "#     )\n",
    "\n",
    "#     ,wallets as (\n",
    "#         select *\n",
    "#         from (\n",
    "#             select wallet_address\n",
    "#             ,count(coin_id) as total_tokens\n",
    "#             ,sum(coin_inflows) as total_inflows\n",
    "#             from wallet_coins wti\n",
    "#             group by 1\n",
    "#         )\n",
    "#         where total_tokens between 3 and 50\n",
    "#         and total_inflows < 20000000\n",
    "#     )\n",
    "\n",
    "#     ,coins as (\n",
    "#         select wc.coin_id\n",
    "#         from wallets w\n",
    "#         join wallet_coins wc on wc.wallet_address = w.wallet_address\n",
    "#         group by 1\n",
    "#     )\n",
    "\n",
    "#     select cmd.coin_id\n",
    "#     ,cmd.date\n",
    "#     ,cmd.price\n",
    "#     ,cmd.market_cap\n",
    "#     from coins c\n",
    "#     join core.coin_market_data cmd on cmd.coin_id = c.coin_id\n",
    "#     order by 1,2\n",
    "#     '''\n",
    "# prices_df = dgc().run_sql(query_sql)\n",
    "\n",
    "# # Convert coin_id to categorical (original strings are preserved)\n",
    "# prices_df['coin_id'] = prices_df['coin_id'].astype('category')\n",
    "\n",
    "# # Convert all numerical columns to 32 bit, using safe_downcast to avoid overflow\n",
    "# prices_df = u.safe_downcast(prices_df, 'price', 'float32')\n",
    "# prices_df = u.safe_downcast(prices_df, 'market_cap', 'int32')\n",
    "\n",
    "# print(prices_df.info())\n",
    "# print(u.df_mem(prices_df))\n",
    "# prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_multiple_coins_per_wallet():\n",
    "\"\"\"\n",
    "Test scenario where wallets own multiple coins, some exceeding thresholds when aggregated.\n",
    "Checks filtering based on specified date range.\n",
    "\"\"\"\n",
    "# Create test data\n",
    "sample_profits_df = pd.DataFrame({\n",
    "    'coin_id': ['BTC', 'ETH', 'BTC', 'ETH', 'LTC', 'BTC', 'ETH'],\n",
    "    'wallet_address': ['wallet1', 'wallet1', 'wallet2', 'wallet2', 'wallet2',\n",
    "                        'wallet3', 'wallet3'],\n",
    "    'date': pd.date_range(start='2023-01-01', periods=7),\n",
    "    'profits_cumulative': [5000, 3000, 1000, 500, 500, 100, 50],\n",
    "    'usd_inflows_cumulative': [10000, 8000, 2000, 1500, 1500, 500, 250]\n",
    "})\n",
    "\n",
    "config = {\n",
    "    'profitability_filter': 7500,\n",
    "    'inflows_filter': 15000,\n",
    "    'date_range': {\n",
    "        'start': '2023-01-02',\n",
    "        'end': '2023-01-05'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the function with date range\n",
    "cleaned_df, exclusions_logs_df = dr.clean_profits_df(\n",
    "    sample_profits_df,\n",
    "    config,\n",
    "    earliest_date=config['date_range']['start'],\n",
    "    latest_date=config['date_range']['end']\n",
    ")\n",
    "\n",
    "# Expected results - only checking within date window but removing all records\n",
    "expected_cleaned_df = sample_profits_df[\n",
    "    sample_profits_df['wallet_address'].isin(['wallet2', 'wallet3'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "expected_exclusions = pd.DataFrame({\n",
    "    'wallet_address': ['wallet1'],\n",
    "    'profits_exclusion': [True],\n",
    "    'inflows_exclusion': [True]\n",
    "})\n",
    "\n",
    "# Assertions\n",
    "assert len(cleaned_df) == 5  # wallet2 (3 records) and wallet3 (2 records) should remain\n",
    "assert np.array_equal(cleaned_df.values, expected_cleaned_df.values)\n",
    "assert np.array_equal(exclusions_logs_df.values, expected_exclusions.values)\n",
    "\n",
    "# Check if profits and inflows are approximately correct for the remaining wallets\n",
    "# Should include ALL records for passing wallets (1000 + 500 + 500 + 100 + 50)\n",
    "assert pytest.approx(cleaned_df['profits_cumulative'].sum(), abs=1e-4) == 2150\n",
    "# Should include ALL records for passing wallets (2000 + 1500 + 1500 + 500 + 250)\n",
    "assert pytest.approx(cleaned_df['usd_inflows_cumulative'].sum(), abs=1e-4) == 5750\n",
    "\n",
    "# Additional date-specific checks\n",
    "date_mask = ((cleaned_df['date'] >= config['date_range']['start']) &\n",
    "                (cleaned_df['date'] <= config['date_range']['end']))\n",
    "date_filtered = cleaned_df[date_mask]\n",
    "\n",
    "# Verify we have the expected number of records in the date range\n",
    "assert len(date_filtered) == 3  # Should only have records between Jan 2-5 for remaining wallets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
