{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingModuleSource=false\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "from utils import load_config, cw_filter_df, create_progress_bar\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "# load configs\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "# generate prices metrics metrics\n",
    "prices_metrics_df,partial_prices_metrics_df = cwm.generate_time_series_metrics(prices_df, config, metrics_config, dataset_key='prices', colname='price')\n",
    "\n",
    "print(buysell_metrics_df.shape)\n",
    "print(prices_metrics_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# retrieve transfers data\n",
    "transfers_df = td.retrieve_transfers_data(\n",
    "    config['training_data']['training_period_start'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end']\n",
    "    )\n",
    "\n",
    "# retrieve and clean prices data\n",
    "prices_df = td.retrieve_prices_data()\n",
    "prices_df,prices_log = td.fill_prices_gaps(prices_df,config['data_cleaning']['max_gap_days'])\n",
    "\n",
    "# compile profits_df\n",
    "profits_df = td.prepare_profits_data(transfers_df, prices_df)\n",
    "profits_df = td.calculate_wallet_profitability(profits_df)\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "\n",
    "# generate prices metrics metrics\n",
    "prices_metrics_df,partial_prices_metrics_df = cwm.generate_time_series_metrics(prices_df, config, metrics_config, dataset_key='prices', colname='price')\n",
    "\n",
    "print(prices_metrics_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prices_metrics_df['coin_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated time series metrics. Out of 427 total coins, 230 had complete period coverage, 88 had partial coverage, and 109 had no coverage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_period_start = pd.to_datetime(config['training_data']['training_period_start'])\n",
    "training_period_end = pd.to_datetime(config['training_data']['training_period_end'])\n",
    "\n",
    "# Get the difference in days as an integer\n",
    "training_period_duration = (training_period_end - training_period_start).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"prices_df: {len(prices_df['coin_id'].unique())}\")\n",
    "print(f\"prices_metrics_df: {len(prices_metrics_df['coin_id'].unique())}\")\n",
    "print(f\"partial_prices_metrics_df: {len(partial_prices_metrics_df['coin_id'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prices_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prices_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prices_metrics_df['coin_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prices_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices preprocessing\n",
    "prices_metrics_config = metrics_config['time_series']['prices']\n",
    "prices_description = 'prices_timeseries'\n",
    "\n",
    "# flatten, save, and preprocess the flattened df\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "\n",
    "flattened_prices_metrics_df = fe.flatten_coin_date_df(\n",
    "    prices_metrics_df,\n",
    "    prices_metrics_config,\n",
    "    config['training_data']['training_period_end']\n",
    ")\n",
    "flattened_prices_metrics_df, flattened_prices_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_prices_metrics_df,\n",
    "    flattened_output_directory,\n",
    "    prices_description,\n",
    "    config['training_data']['modeling_period_start']\n",
    ")\n",
    "prices_preprocessed_df, prices_preprocessed_filepath = fe.preprocess_coin_df(flattened_prices_metrics_filepath, modeling_config, prices_metrics_config)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the training data df\n",
    "input_filenames = [\n",
    "    (buysell_preprocessed_filepath.split('preprocessed_outputs/')[1], 'drop_records'),\n",
    "    (prices_preprocessed_filepath.split('preprocessed_outputs/')[1], 'drop_records')\n",
    "]\n",
    "training_data_df, merge_logs_df = fe.create_training_data_df(modeling_config['modeling']['modeling_folder'], input_filenames)\n",
    "\n",
    "# create the target variable df\n",
    "target_variable_df,_ = fe.create_target_variables_mooncrater(prices_df, config['training_data'], modeling_config)\n",
    "\n",
    "# merge the two into the final model input df\n",
    "model_input_df = fe.prepare_model_input_df(training_data_df, target_variable_df, modeling_config['modeling']['target_column'])\n",
    "\n",
    "# split the df into train and test sets\n",
    "X_train, X_test, y_train, y_test = m.split_model_input(\n",
    "    model_input_df,\n",
    "    modeling_config['modeling']['target_column'],\n",
    "    modeling_config['modeling']['train_test_split'],\n",
    "    modeling_config['modeling']['random_state']\n",
    ")\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate the model's performance on the test set\n",
    "metrics = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "\n",
    "# 3.6 Log the experiment results for this configuration\n",
    "m.log_trial_results(modeling_folder, model_id)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_input_files_colnames(tmpdir):\n",
    "    \"\"\"\n",
    "    Unit test data for scenario with many duplicate columns and similar filenames.\n",
    "    \"\"\"\n",
    "    # Create mock filenames and corresponding DataFrames\n",
    "    filenames = [\n",
    "        'buysell_metrics_2024-09-13_14-44_model_period_2024-05-01_v0.1.csv',\n",
    "        'buysell_metrics_2024-09-13_14-45_model_period_2024-05-01_v0.1.csv',\n",
    "        'buysell_metrics_megasharks_2024-09-13_14-45_model_period_2024-05-01_v0.1.csv',\n",
    "        'buysell_metrics_megasharks_2024-09-13_14-45_model_period_2024-05-01_v0.2.csv',\n",
    "        'price_metrics_2024-09-13_14-45_model_period_2024-05-01_v0.1.csv'\n",
    "    ]\n",
    "\n",
    "    # Create mock DataFrames for each file\n",
    "    df1 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [100, 200]})\n",
    "    df2 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [150, 250]})\n",
    "    df3 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [110, 210]})\n",
    "    df4 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [120, 220]})\n",
    "    df5 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [130, 230]})\n",
    "\n",
    "    # Save each DataFrame as a CSV\n",
    "    for i, df in enumerate([df1, df2, df3, df4, df5]):\n",
    "        df.to_csv(os.path.join(tmpdir, filenames[i]), index=False)\n",
    "\n",
    "    # Create a tuple list with filenames and 'fill_zeros' strategy\n",
    "    input_files = [(filenames[i], 'fill_zeros') for i in range(len(filenames))]\n",
    "\n",
    "    return tmpdir, input_files\n",
    "\n",
    "tmpdir, input_files = mock_input_files_colnames('temp/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "# def test_create_training_data_df(mock_input_files_colnames):\n",
    "\"\"\"\n",
    "Test column renaming logic for clarity when merging multiple files with similar filenames.\n",
    "\"\"\"\n",
    "# tmpdir, input_files = mock_input_files_colnames\n",
    "print(\"Input files:\", input_files)\n",
    "\n",
    "# Call the function\n",
    "merged_df, _ = fe.create_training_data_df(tmpdir, input_files)\n",
    "\n",
    "# Check if the columns have the correct suffixes\n",
    "expected_columns = [\n",
    "    'coin_id',\n",
    "    'buyers_new_buysell_metrics_2024-09-13_14-44',\n",
    "    'buyers_new_buysell_metrics_2024-09-13_14-45',\n",
    "    'buyers_new_buysell_metrics_megasharks_2024-09-13_14-45',\n",
    "    'buyers_new_buysell_metrics_megasharks_2024-09-13_14-45_2',\n",
    "    'buyers_new_price_metrics'\n",
    "]\n",
    "\n",
    "assert list(merged_df.columns) == expected_columns, \\\n",
    "    f\"Expected columns: {expected_columns}, but got: {list(merged_df.columns)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input files:\", input_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "\n",
    "def sample_time_series_df():\n",
    "    \"\"\"Fixture that provides a sample DataFrame for the time series with multiple coin_ids.\"\"\"\n",
    "    data = {\n",
    "        'coin_id': [1, 1, 1, 2, 2, 2],\n",
    "        'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-01', '2023-01-02', '2023-01-03'],\n",
    "        'price': [100, 110, 120, 200, 210, 220]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sample_metrics_config():\n",
    "    \"\"\"Fixture that provides a sample metrics configuration for time series analysis.\"\"\"\n",
    "    return {\n",
    "        'time_series': {\n",
    "            'prices': {\n",
    "                'sma': {\n",
    "                    'parameters': {\n",
    "                        'period': 2\n",
    "                    }\n",
    "                },\n",
    "                'ema': {\n",
    "                    'parameters': {\n",
    "                        'period': 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "sample_time_series_df = sample_time_series_df()\n",
    "sample_metrics_config = sample_metrics_config()\n",
    "\n",
    "# def test_generate_time_series_metrics_basic_functionality(sample_time_series_df, sample_metrics_config):\n",
    "\"\"\"\n",
    "Test the basic functionality of generate_time_series_metrics to ensure that SMA and EMA\n",
    "are calculated correctly for a simple DataFrame with multiple coin_ids.\n",
    "\"\"\"\n",
    "# Convert the date to datetime in the sample data\n",
    "sample_time_series_df['date'] = pd.to_datetime(sample_time_series_df['date'])\n",
    "\n",
    "# Mock any necessary file or folder dependencies if needed\n",
    "# (none in this specific case)\n",
    "\n",
    "# Run the generate_time_series_metrics function\n",
    "result_df,_ = cwm.generate_time_series_metrics(\n",
    "    time_series_df=sample_time_series_df,\n",
    "    metrics_config=sample_metrics_config,\n",
    "    dataset_key='prices',\n",
    "    colname='price'\n",
    ")\n",
    "\n",
    "# Expected columns in the result\n",
    "expected_columns = ['coin_id', 'date', 'price', 'sma', 'ema']\n",
    "\n",
    "# Assert that the columns exist in the result\n",
    "assert all(col in result_df.columns for col in expected_columns), \"Missing expected columns in the result.\"\n",
    "\n",
    "# Assert that SMA and EMA are calculated correctly\n",
    "expected_sma_1 = [100.0, 105.0, 115.0]  # SMA for coin_id=1 with period=2\n",
    "expected_ema_1 = [100.0, 106.666667, 115.555556]  # EMA for coin_id=1 with period=2\n",
    "\n",
    "# Confirm that the SMA result matches the expected, with special logic to handle NaNs\n",
    "for i, (expected, actual) in enumerate(zip(\n",
    "    expected_sma_1,\n",
    "    result_df[result_df['coin_id'] == 1]['sma'].tolist()\n",
    ")):\n",
    "    if np.isnan(expected) and np.isnan(actual):\n",
    "        continue  # Both values are NaN, so this is considered equal\n",
    "    assert expected == actual, f\"Mismatch at index {i}: expected {expected}, got {actual}\"\n",
    "\n",
    "# Confirm that the EMA result matches the expected\n",
    "assert result_df[result_df['coin_id'] == 1]['ema'].tolist() == pytest.approx(\n",
    "    expected_ema_1,\n",
    "    abs=1e-2\n",
    "), \"EMA calculation incorrect for coin_id=1\"\n",
    "\n",
    "# Check for another coin_id\n",
    "expected_sma_2 = [200.0, 205.0, 215.0]  # SMA for coin_id=2 with period=2\n",
    "expected_ema_2 = [200.0, 206.666667, 215.555556]  # EMA for coin_id=2 with period=2\n",
    "\n",
    "# Confirm that the SMA result matches the expected, with special logic to handle NaNs\n",
    "for i, (expected, actual) in enumerate(zip(\n",
    "    expected_sma_2,\n",
    "    result_df[result_df['coin_id'] == 2]['sma'].tolist()\n",
    ")):\n",
    "    if np.isnan(expected) and np.isnan(actual):\n",
    "        continue  # Both values are NaN, so this is considered equal\n",
    "    assert expected == actual, f\"Mismatch at index {i}: expected {expected}, got {actual}\"\n",
    "\n",
    "# Confirm that the EMA result matches the expected\n",
    "assert result_df[result_df['coin_id'] == 2]['ema'].tolist() == pytest.approx(\n",
    "    expected_ema_2,\n",
    "    abs=1e-2\n",
    "), \"EMA calculation incorrect for coin_id=2\"\n",
    "\n",
    "# Confirm that the output df has the same number of rows as the input df\n",
    "assert len(result_df) == len(sample_time_series_df), \"Output row count does not match input row count\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mock DataFrames\n",
    "df1 = pd.DataFrame({'coin_id': [1, 2, 3], 'metric_1': [10, 20, 30]})\n",
    "df2 = pd.DataFrame({'coin_id': [2, 3], 'metric_2': [200, 300]})\n",
    "\n",
    "# List of input DataFrames with fill strategies\n",
    "df_list = [(df1, 'fill_zeros', 'df1'), (df2, 'fill_zeros', 'df2')]\n",
    "\n",
    "# Call the function\n",
    "merged_df, _ = fe.merge_and_fill_training_data(df_list)\n",
    "\n",
    "# Define the expected output\n",
    "expected_df = pd.DataFrame({\n",
    "    'coin_id': [1, 2, 3],\n",
    "    'metric_1': [10, 20, 30],\n",
    "    'metric_2': [0, 200, 300]  # Coin_id 1 should have metric_2 filled with 0\n",
    "})\n",
    "\n",
    "# Compare DataFrames\n",
    "pd.testing.assert_frame_equal(merged_df, expected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mock DataFrames\n",
    "df1 = pd.DataFrame({'coin_id': [1, 2, 3], 'metric_1': [10, 20, 30]})\n",
    "df2 = pd.DataFrame({'coin_id': [2, 3], 'metric_2': [200, 300]})\n",
    "\n",
    "# List of input DataFrames with fill strategies\n",
    "df_list = [(df1, 'fill_zeros', 'df1'), (df2, 'drop_records', 'df2')]\n",
    "\n",
    "# Call the function\n",
    "merged_df, merge_logs = fe.merge_and_fill_training_data(df_list)\n",
    "\n",
    "# Define the expected output\n",
    "expected_df = pd.DataFrame({\n",
    "    'coin_id': [2, 3],\n",
    "    'metric_1': [20, 30],\n",
    "    'metric_2': [200, 300]  # Coin_id 1 should be dropped\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mock DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'coin_id': [1, 2, 3],\n",
    "    'metric_1': [10, 20, 30]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'coin_id': [2, 3],\n",
    "    'metric_2': [200, 300]\n",
    "})\n",
    "\n",
    "# Expected output when drop_records is applied: rows for coin 1 should be dropped\n",
    "expected_df = pd.DataFrame({\n",
    "    'coin_id': [2, 3],\n",
    "    'metric_1': [20, 30],\n",
    "    'metric_2': [200, 300]\n",
    "})\n",
    "\n",
    "# Run the function\n",
    "merged_df, logs_df = fe.merge_and_fill_training_data([\n",
    "    (df1, 'drop_records', 'df1'),\n",
    "    (df2, 'drop_records', 'df2')\n",
    "])\n",
    "\n",
    "# Assert the merged DataFrame is correct\n",
    "assert np.array_equal(merged_df.values, expected_df.values), \"Merged DataFrame values do not match the expected DataFrame.\"\n",
    "\n",
    "\n",
    "# Assert the logs are correct\n",
    "# df1 should have no filled rows, and df2 should also have no filled rows (since we used drop_records)\n",
    "expected_logs = pd.DataFrame({\n",
    "    'file': ['df1', 'df2'],\n",
    "    'original_count': [3, 2],\n",
    "    'filled_count': [0, 0]\n",
    "})\n",
    "\n",
    "pd.testing.assert_frame_equal(logs_df.reset_index(drop=True), expected_logs.reset_index(drop=True))\n",
    "\n",
    "\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(merged_df.values,expected_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(merged_df, expected_df, check_dtype=False, , check_like=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
