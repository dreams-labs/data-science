{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# import local files if necessary\n",
    "sys.path.append('..//src')\n",
    "from utils import load_config, cw_filter_df, create_progress_bar\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "# load configs\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "importlib.reload(u)\n",
    "from utils import timing_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data (profits_df) Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# retrieve and clean prices data\n",
    "prices_df = td.retrieve_prices_data()\n",
    "prices_df,_ = td.fill_prices_gaps(prices_df,config['data_cleaning']['max_gap_days'])\n",
    "\n",
    "# retrieve transfers data\n",
    "transfers_df = td.retrieve_transfers_data(\n",
    "    config['training_data']['training_period_start'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end']\n",
    "    )\n",
    "\n",
    "# compile profits_df\n",
    "profits_df = td.prepare_profits_data(transfers_df, prices_df)\n",
    "profits_df = td.calculate_wallet_profitability(profits_df)\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "\n",
    "\n",
    "# identify cohort\n",
    "cohort_summary_df = td.classify_wallet_cohort(profits_df, config['wallet_cohorts']['sharks'])\n",
    "\n",
    "# generate and flatten buysell_metrics\n",
    "cohort_wallets = cohort_summary_df[cohort_summary_df['in_cohort']==True]['wallet_address']\n",
    "buysell_metrics_df = cwm.generate_buysell_metrics_df(profits_df,config['training_data']['training_period_end'],cohort_wallets)\n",
    "\n",
    "# flatten, save, and preprocess the flattened df\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "cohort_name = list(config['wallet_cohorts'].keys())[0]\n",
    "metric_description = f\"{cohort_name}_cohort\"\n",
    "\n",
    "flattened_buysell_metrics_df = fe.flatten_coin_date_df(buysell_metrics_df,metrics_config,config['training_data']['training_period_end'])\n",
    "flattened_df, flattened_filepath = fe.save_flattened_outputs(flattened_buysell_metrics_df, flattened_output_directory, metric_description, config['training_data']['modeling_period_start'])\n",
    "preprocessed_df, preprocessed_filepath = fe.preprocess_coin_df(flattened_filepath, modeling_config, metrics_config)\n",
    "\n",
    "# create the training data df\n",
    "input_directory = f\"{preprocessed_filepath.split('preprocessed_outputs/')[0]}preprocessed_outputs/\"\n",
    "input_filenames = [\n",
    "    preprocessed_filepath.split('preprocessed_outputs/')[1]\n",
    "]\n",
    "training_data_df = fe.create_training_data_df(input_directory, input_filenames)\n",
    "\n",
    "# create the target variable df\n",
    "target_variable_df,_ = fe.create_target_variables_mooncrater(prices_df, config['training_data'], modeling_config)\n",
    "\n",
    "# merge the two into the final model input df\n",
    "model_input_df = fe.prepare_model_input_df(training_data_df, target_variable_df, modeling_config['modeling']['target_column'])\n",
    "\n",
    "# split the df into train and test sets\n",
    "X_train, X_test, y_train, y_test = m.split_model_input(\n",
    "    model_input_df,\n",
    "    modeling_config['modeling']['target_column'],\n",
    "    modeling_config['modeling']['train_test_split'],\n",
    "    modeling_config['modeling']['random_state']\n",
    ")\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate the model's performance on the test set\n",
    "metrics = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "\n",
    "# 3.6 Log the experiment results for this configuration\n",
    "m.log_trial_results(modeling_folder, model_id, experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "\n",
    "\n",
    "\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "i.run_experiment(modeling_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "\n",
    "\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "experiment_id = \"more_sharks_131bff29-dafa-4e56-ae14-2862a22b5304\"\n",
    "\n",
    "# 1. Construct the path to the experiment metadata file\n",
    "experiment_metadata_path = os.path.join(modeling_folder, \"experiment_metadata\", f\"{experiment_id}.json\")\n",
    "with open(experiment_metadata_path, 'r') as f:\n",
    "    experiment_metadata = json.load(f)\n",
    "\n",
    "# Retrieve trial log filenames\n",
    "trial_logs = experiment_metadata['trial_logs']\n",
    "\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_feature_importances = []\n",
    "\n",
    "# Loop through each trial log and process feature importance\n",
    "for trial_log_path in trial_logs:\n",
    "    with open(trial_log_path, 'r') as f:\n",
    "        trial_log_data = json.load(f)\n",
    "\n",
    "    # Extract feature importance and convert to DataFrame\n",
    "    feature_importance = trial_log_data['feature_importance']\n",
    "    feature_importance_df = pd.DataFrame(list(feature_importance.items()), columns=['feature', 'importance'])\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    all_feature_importances.append(feature_importance_df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_feature_importance_df = pd.concat(all_feature_importances)\n",
    "\n",
    "# Group by feature and calculate mean importance\n",
    "feature_stats = combined_feature_importance_df.groupby('feature')['importance'].agg(['mean', 'var', 'std']).reset_index()\n",
    "\n",
    "# Sort by mean importance and display top features\n",
    "sorted_features = feature_stats.sort_values(by='mean', ascending=False)\n",
    "print(sorted_features.head())\n",
    "\n",
    "# Plot the top features by importance\n",
    "sorted_features.head(10).plot(kind='barh', x='feature', y='mean', title='Top Features by Mean Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store the data\n",
    "trial_data = []\n",
    "\n",
    "# Loop through each trial log and extract trial_overrides and performance metrics\n",
    "for trial_log_path in trial_logs:\n",
    "    with open(trial_log_path, 'r') as f:\n",
    "        trial_log_data = json.load(f)\n",
    "\n",
    "    # Extract trial_overrides and performance metrics\n",
    "    trial_overrides = trial_log_data.get('trial_overrides', {})\n",
    "    performance_metrics = trial_log_data.get('metrics', {})\n",
    "\n",
    "    # Merge trial_overrides and performance metrics into a single dictionary\n",
    "    trial_info = {**trial_overrides, **performance_metrics}\n",
    "\n",
    "    # Append trial info to the list\n",
    "    trial_data.append(trial_info)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "trial_df = pd.DataFrame(trial_data)\n",
    "\n",
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that are numeric for correlation analysis\n",
    "numeric_cols = [\n",
    "    'modeling_config.target_variables.moon_threshold',\n",
    "    'modeling_config.modeling.model_params.n_estimators',\n",
    "    'config.wallet_cohorts.sharks.wallet_minimum_inflows',\n",
    "    'config.wallet_cohorts.sharks.wallet_min_coin_wins',\n",
    "    'config.wallet_cohorts.sharks.coin_return_win_threshold',\n",
    "    'config.wallet_cohorts.sharks.coin_profits_win_threshold',\n",
    "    'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss'\n",
    "]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = trial_df[numeric_cols].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by key parameter columns and calculate mean performance metrics\n",
    "group_by_params = [\n",
    "    'config.wallet_cohorts.sharks.wallet_minimum_inflows',\n",
    "    'config.wallet_cohorts.sharks.wallet_min_coin_wins',\n",
    "    'modeling_config.target_variables.moon_threshold'\n",
    "]\n",
    "\n",
    "# Calculate mean of performance metrics for each group\n",
    "grouped_metrics = trial_df.groupby(group_by_params)[['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']].mean()\n",
    "\n",
    "# Display the grouped metrics\n",
    "grouped_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Construct the path to the experiment metadata file\n",
    "experiment_metadata_path = os.path.join(modeling_folder, \"experiment_metadata\", f\"{experiment_id}.json\")\n",
    "\n",
    "# Load the experiment metadata to retrieve trial logs\n",
    "with open(experiment_metadata_path, 'r') as f:\n",
    "    experiment_metadata = json.load(f)\n",
    "\n",
    "# Retrieve trial log filenames\n",
    "trial_logs = experiment_metadata['trial_logs']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_feature_importances = []\n",
    "\n",
    "# Loop through each trial log and process feature importance\n",
    "for trial_log_path in trial_logs:\n",
    "    with open(trial_log_path, 'r') as f:\n",
    "        trial_log_data = json.load(f)\n",
    "\n",
    "    # Extract feature importance and convert to DataFrame\n",
    "    feature_importance = trial_log_data['feature_importance']\n",
    "    feature_importance_df = pd.DataFrame(list(feature_importance.items()), columns=['feature', 'importance'])\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    all_feature_importances.append(feature_importance_df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_feature_importance_df = pd.concat(all_feature_importances)\n",
    "\n",
    "# Group by feature and calculate mean importance\n",
    "feature_stats = combined_feature_importance_df.groupby('feature')['importance'].agg(['mean', 'var', 'std']).reset_index()\n",
    "\n",
    "# Sort by mean importance\n",
    "sorted_features = feature_stats.sort_values(by='mean', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_feature_importance(modeling_folder, experiment_id, top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the top features by mean importance from an experiment's feature importance logs.\n",
    "\n",
    "    Parameters:\n",
    "    - modeling_folder: str, path to the folder where the experiment data is stored.\n",
    "    - experiment_id: str, unique identifier for the experiment to retrieve the logs.\n",
    "    - top_n: int, number of top features to display in the bar chart (default: 10).\n",
    "\n",
    "    This function retrieves trial logs from an experiment's metadata, extracts feature importance\n",
    "    data, calculates the mean importance across all trials, and displays a bar chart of the top_n\n",
    "    most important features.\n",
    "    \"\"\"\n",
    "    # 1. Construct the path to the experiment metadata file\n",
    "    experiment_metadata_path = os.path.join(modeling_folder, \"experiment_metadata\", f\"{experiment_id}.json\")\n",
    "\n",
    "    # Load the experiment metadata to retrieve trial logs\n",
    "    with open(experiment_metadata_path, 'r') as f:\n",
    "        experiment_metadata = json.load(f)\n",
    "\n",
    "    # Retrieve trial log filenames\n",
    "    trial_logs = experiment_metadata['trial_logs']\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    all_feature_importances = []\n",
    "\n",
    "    # Loop through each trial log and process feature importance\n",
    "    for trial_log_path in trial_logs:\n",
    "        with open(trial_log_path, 'r') as f:\n",
    "            trial_log_data = json.load(f)\n",
    "\n",
    "        # Extract feature importance and convert to DataFrame\n",
    "        feature_importance = trial_log_data['feature_importance']\n",
    "        feature_importance_df = pd.DataFrame(list(feature_importance.items()), columns=['feature', 'importance'])\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        all_feature_importances.append(feature_importance_df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    combined_feature_importance_df = pd.concat(all_feature_importances)\n",
    "\n",
    "    # Group by feature and calculate mean importance\n",
    "    feature_stats = combined_feature_importance_df.groupby('feature')['importance'].agg(['mean', 'var', 'std']).reset_index()\n",
    "\n",
    "    # Sort by mean importance\n",
    "    sorted_features = feature_stats.sort_values(by='mean', ascending=False)\n",
    "\n",
    "    # Plot the top features by importance\n",
    "    sorted_features.head(top_n).sort_values(by='mean',ascending=True).plot(kind='barh', x='feature', y='mean', title=f'Top {top_n} Features by Mean Importance')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.xlabel('Mean Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_top_feature_importance(modeling_folder, experiment_id, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def generate_trial_df(modeling_folder, experiment_id):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame by loading and processing trial logs from a specified modeling folder and experiment ID.\n",
    "\n",
    "    Parameters:\n",
    "    - modeling_folder: The path to the folder where model data is stored.\n",
    "    - experiment_id: The ID of the experiment to retrieve trial logs from.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame (trial_df) containing the processed trial logs.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Construct the path to the experiment metadata file\n",
    "    experiment_metadata_path = os.path.join(modeling_folder, \"experiment_metadata\", f\"{experiment_id}.json\")\n",
    "\n",
    "    # 2. Load the experiment metadata\n",
    "    with open(experiment_metadata_path, 'r') as f:\n",
    "        experiment_metadata = json.load(f)\n",
    "\n",
    "    # Retrieve trial log filenames\n",
    "    trial_logs = experiment_metadata['trial_logs']\n",
    "\n",
    "    # 3. Initialize lists to store trial data\n",
    "    trial_data = []\n",
    "\n",
    "    # 4. Loop through each trial log and extract relevant information\n",
    "    for trial_log_path in trial_logs:\n",
    "        with open(trial_log_path, 'r') as f:\n",
    "            trial_log_data = json.load(f)\n",
    "\n",
    "        # Extract trial_overrides and performance metrics\n",
    "        trial_overrides = trial_log_data.get('trial_overrides', {})\n",
    "        performance_metrics = trial_log_data.get('metrics', {})\n",
    "\n",
    "        # Merge trial_overrides and performance metrics into a single dictionary\n",
    "        trial_info = {**trial_overrides, **performance_metrics}\n",
    "\n",
    "        # Append trial info to the list\n",
    "        trial_data.append(trial_info)\n",
    "\n",
    "    # 5. Convert the list of dictionaries to a pandas DataFrame\n",
    "    trial_df = pd.DataFrame(trial_data)\n",
    "\n",
    "    return trial_df\n",
    "\n",
    "\n",
    "def plot_roc_auc_performance(trial_df, top_n):\n",
    "    \"\"\"\n",
    "    Plot the average ROC AUC performance for the top N features in the trial_df.\n",
    "\n",
    "    Parameters:\n",
    "    - trial_df: DataFrame containing trial data with columns to be grouped and evaluated.\n",
    "    - top_n: The number of top features based on average ROC AUC to plot.\n",
    "    \"\"\"\n",
    "    # Ensure all columns are converted to strings to handle arrays, lists, and other non-numeric data types\n",
    "    trial_df = trial_df.applymap(lambda x: str(x) if isinstance(x, (list, dict, tuple)) else x)\n",
    "\n",
    "    # Calculate mean roc_auc for each category in the relevant columns\n",
    "    roc_auc_means = pd.DataFrame()\n",
    "\n",
    "    for column in trial_df.columns:\n",
    "        if column not in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss', 'confusion_matrix']:\n",
    "            grouped = trial_df.groupby(column)['roc_auc'].mean().reset_index()\n",
    "            grouped['feature'] = column\n",
    "            roc_auc_means = pd.concat([roc_auc_means, grouped], ignore_index=True)\n",
    "\n",
    "    # Sort by mean ROC AUC and select the top N\n",
    "    roc_auc_means = roc_auc_means.sort_values(by='roc_auc', ascending=False).head(top_n)\n",
    "\n",
    "    # Plot the results in a single bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='roc_auc', y='feature', data=roc_auc_means)\n",
    "    plt.title(f'Top {top_n} Features by ROC AUC Performance')\n",
    "    plt.xlabel('Average ROC AUC')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_top_feature_importance(modeling_folder, experiment_id, top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the top features by mean importance from an experiment's feature importance logs.\n",
    "\n",
    "    Parameters:\n",
    "    - modeling_folder: str, path to the folder where the experiment data is stored.\n",
    "    - experiment_id: str, unique identifier for the experiment to retrieve the logs.\n",
    "    - top_n: int, number of top features to display in the bar chart (default: 10).\n",
    "\n",
    "    This function retrieves trial logs from an experiment's metadata, extracts feature importance\n",
    "    data, calculates the mean importance across all trials, and displays a bar chart of the top_n\n",
    "    most important features.\n",
    "    \"\"\"\n",
    "    # 1. Construct the path to the experiment metadata file\n",
    "    experiment_metadata_path = os.path.join(modeling_folder, \"experiment_metadata\", f\"{experiment_id}.json\")\n",
    "\n",
    "    # Load the experiment metadata to retrieve trial logs\n",
    "    with open(experiment_metadata_path, 'r') as f:\n",
    "        experiment_metadata = json.load(f)\n",
    "\n",
    "    # Retrieve trial log filenames\n",
    "    trial_logs = experiment_metadata['trial_logs']\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    all_feature_importances = []\n",
    "\n",
    "    # Loop through each trial log and process feature importance\n",
    "    for trial_log_path in trial_logs:\n",
    "        with open(trial_log_path, 'r') as f:\n",
    "            trial_log_data = json.load(f)\n",
    "\n",
    "        # Extract feature importance and convert to DataFrame\n",
    "        feature_importance = trial_log_data['feature_importance']\n",
    "        feature_importance_df = pd.DataFrame(list(feature_importance.items()), columns=['feature', 'importance'])\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        all_feature_importances.append(feature_importance_df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    combined_feature_importance_df = pd.concat(all_feature_importances)\n",
    "\n",
    "    # Group by feature and calculate mean importance\n",
    "    feature_stats = combined_feature_importance_df.groupby('feature')['importance'].agg(['mean', 'var', 'std']).reset_index()\n",
    "\n",
    "    # Sort by mean importance\n",
    "    sorted_features = feature_stats.sort_values(by='mean', ascending=False)\n",
    "\n",
    "    # Plot the top features by importance\n",
    "    sorted_features.head(top_n).sort_values(by='mean',ascending=True).plot(kind='barh', x='feature', y='mean', title=f'Top {top_n} Features by Mean Importance')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.xlabel('Mean Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def analyze_experiment(modeling_folder, experiment_id, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze experiment results by generating two visualizations:\n",
    "    1. A bar chart of the top N features by ROC AUC performance.\n",
    "    2. A bar chart of the top N features by mean importance from the trial logs.\n",
    "\n",
    "    Parameters:\n",
    "    - modeling_folder: str, path to the folder where the experiment data is stored.\n",
    "    - experiment_id: str, unique identifier for the experiment to retrieve the logs.\n",
    "    - top_n: int, number of top features to display in the bar charts (default: 10).\n",
    "    \"\"\"\n",
    "    # Generate trial DataFrame from the experiment logs\n",
    "    trial_df = generate_trial_df(modeling_folder, experiment_id)\n",
    "\n",
    "    # Plot ROC AUC performance for the top N features\n",
    "    plot_roc_auc_performance(trial_df, top_n)\n",
    "\n",
    "    # Plot top feature importance based on the trial logs\n",
    "    plot_top_feature_importance(modeling_folder, experiment_id, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_experiment(modeling_folder, experiment_id, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df = generate_trial_df(modeling_folder, experiment_id)\n",
    "trial_df\n",
    "\n",
    "plot_roc_auc_performance(trial_df, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load necessary libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Load experiment metadata\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "experiment_id = \"more_sharks_131bff29-dafa-4e56-ae14-2862a22b5304\"\n",
    "\n",
    "experiment_metadata_path = os.path.join(modeling_folder, \"experiment_metadata\", f\"{experiment_id}.json\")\n",
    "with open(experiment_metadata_path, 'r') as f:\n",
    "    experiment_metadata = json.load(f)\n",
    "\n",
    "# 3. Retrieve trial logs\n",
    "trial_logs = experiment_metadata['trial_logs']\n",
    "\n",
    "# Initialize lists to store both trial data and feature importances\n",
    "trial_data = []\n",
    "all_feature_importances = []\n",
    "\n",
    "# 4. Loop through each trial log once, processing both feature importance and trial metrics\n",
    "for trial_log_path in trial_logs:\n",
    "    with open(trial_log_path, 'r') as f:\n",
    "        trial_log_data = json.load(f)\n",
    "\n",
    "    # Process feature importance\n",
    "    feature_importance = trial_log_data['feature_importance']\n",
    "    feature_importance_df = pd.DataFrame(list(feature_importance.items()), columns=['feature', 'importance'])\n",
    "    all_feature_importances.append(feature_importance_df)\n",
    "\n",
    "    # Process trial data (trial_overrides and performance metrics)\n",
    "    trial_overrides = trial_log_data.get('trial_overrides', {})\n",
    "    performance_metrics = trial_log_data.get('metrics', {})\n",
    "    trial_info = {**trial_overrides, **performance_metrics}\n",
    "    trial_data.append(trial_info)\n",
    "\n",
    "# 5. Concatenate feature importance data\n",
    "combined_feature_importance_df = pd.concat(all_feature_importances)\n",
    "\n",
    "# 6. Calculate mean feature importance and plot top 15\n",
    "feature_stats = combined_feature_importance_df.groupby('feature')['importance'].agg(['mean', 'var', 'std']).reset_index()\n",
    "sorted_features = feature_stats.sort_values(by='mean', ascending=False).head(15)\n",
    "# sorted_features.plot(kind='barh', x='feature', y='mean', title='Top 15 Features by Mean Importance')\n",
    "\n",
    "# 7. Convert trial data into a DataFrame\n",
    "trial_df = pd.DataFrame(trial_data)\n",
    "\n",
    "# 8. Filter trial_df to match top 15 features\n",
    "top_features = sorted_features['feature'].tolist()\n",
    "trial_df_top_features = trial_df[top_features + ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss']]\n",
    "\n",
    "# 9. Correlation matrix of top 15 features and performance metrics\n",
    "numeric_columns = trial_df_top_features.applymap(lambda x: isinstance(x, (int, float))).all(0)\n",
    "numeric_trial_df = trial_df_top_features.loc[:, numeric_columns]\n",
    "\n",
    "corr_matrix = numeric_trial_df.corr()\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix of Top 15 Features and Performance Metrics')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot ROC AUC performance for top_n features\n",
    "def plot_roc_auc_performance(trial_df, top_n):\n",
    "    \"\"\"\n",
    "    Plot the average ROC AUC performance for the top N features in the trial_df.\n",
    "\n",
    "    Parameters:\n",
    "    - trial_df: DataFrame containing trial data with columns to be grouped and evaluated.\n",
    "    - top_n: The number of top features based on average ROC AUC to plot.\n",
    "    \"\"\"\n",
    "    # Calculate mean roc_auc for each category in the relevant columns\n",
    "    roc_auc_means = pd.DataFrame()\n",
    "\n",
    "    for column in trial_df.columns:\n",
    "        if column not in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'log_loss', 'confusion_matrix']:\n",
    "            grouped = trial_df.groupby(column)['roc_auc'].mean().reset_index()\n",
    "            grouped['feature'] = column\n",
    "            roc_auc_means = pd.concat([roc_auc_means, grouped], ignore_index=True)\n",
    "\n",
    "    # Sort by mean ROC AUC and select the top N\n",
    "    roc_auc_means = roc_auc_means.sort_values(by='roc_auc', ascending=False).head(top_n)\n",
    "\n",
    "    # Plot the results in a single bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='roc_auc', y='feature', data=roc_auc_means, palette='Blues_d')\n",
    "    plt.title(f'Top {top_n} Features by ROC AUC Performance')\n",
    "    plt.xlabel('Average ROC AUC')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_roc_auc_performance(trial_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for metrics and their average ROC AUC scores (sorted)\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'total_bought_sum_sharks_cohort', 'buyers_repeat_mean_sharks_cohort',\n",
    "        'total_buyers_sum_sharks_cohort', 'buyers_repeat_sum_sharks_cohort',\n",
    "        'total_bought_sum_7d_period_2_sharks_cohort', 'sellers_new_mean_sharks_cohort',\n",
    "        'total_sold_sum_sharks_cohort', 'buyers_new_std_sharks_cohort',\n",
    "        'buyers_new_mean_sharks_cohort', 'total_bought_sum_7d_period_1_sharks_cohort',\n",
    "        'total_bought_sum_7d_period_8_sharks_cohort', 'total_bought_sum_7d_period_5_sharks_cohort',\n",
    "        'buyers_new_sum_sharks_cohort', 'total_bought_change_7d_period_4_sharks_cohort',\n",
    "        'total_bought_sum_7d_period_4_sharks_cohort'\n",
    "    ],\n",
    "    'Avg_ROC_AUC': [0.85, 0.83, 0.81, 0.80, 0.79, 0.78, 0.77, 0.76, 0.75, 0.74, 0.73, 0.72, 0.71, 0.70, 0.69]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame by ROC AUC scores in descending order\n",
    "df_sorted = df.sort_values(by='Avg_ROC_AUC', ascending=False)\n",
    "\n",
    "# Plot a horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_sorted['Metric'], df_sorted['Avg_ROC_AUC'])\n",
    "plt.xlabel('Average ROC AUC Score')\n",
    "plt.ylabel('Metric')\n",
    "plt.title('Top Metrics by Average ROC AUC')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in trial_df.columns:\n",
    "    if column != 'roc_auc':\n",
    "        # Group by column and calculate mean roc_auc for each group\n",
    "        grouped = trial_df.groupby(column)['roc_auc'].mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
