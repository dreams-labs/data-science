{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "from urllib.parse import urlencode\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import importlib\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import flask\n",
    "\n",
    "sys.path.append('../GitHub/core-functions/src/dreams_core')\n",
    "import googlecloud as dgc2\n",
    "importlib.reload(dgc2)\n",
    "\n",
    "\n",
    "# sys.path.append('../GitHub/etl-pipelines/cloud_functions/core_chains')\n",
    "# import main as wip\n",
    "# importlib.reload(wip)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.15g}')\n",
    "# pd.reset_option('display.float_format')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax to trigger google cloud functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries with notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- successfully retrieves metadata for MILKBAG\n",
    "\n",
    "SELECT\n",
    "*\n",
    "FROM\n",
    "  -- `bigquery-public-data.crypto_solana_mainnet_us.Token Transfers` t\n",
    "  `bigquery-public-data.crypto_solana_mainnet_us.Tokens` t\n",
    "WHERE\n",
    "  t.block_timestamp between '2024-03-01' and '2024-04-30'\n",
    " and mint='2ubuHGFS4VJVxSEpvV3kDwz6JiuXdaAoGMwrwYC87tp8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "[{\n",
    "  \"block_slot\": \"255513963\",\n",
    "  \"block_hash\": \"3JxMLj4B5WoGPYhM6oEQR2jZ3qytWuUQtyKZALZtNwZD\",\n",
    "  \"block_timestamp\": \"2024-03-21 08:51:38.000000 UTC\",\n",
    "  \"tx_signature\": \"5R6bGVgcByPEz6qeX5MhJCBreiMAQBz4NSSRrTBDJzqow4RQP3DHsTZMZAvTfFuFYVcBx2XNp2TWdjVfedN7dj7k\",\n",
    "  \"retrieval_timestamp\": \"2024-03-21 22:12:36.000000 UTC\",\n",
    "  \"is_nft\": \"false\",\n",
    "  \"mint\": \"2ubuHGFS4VJVxSEpvV3kDwz6JiuXdaAoGMwrwYC87tp8\",\n",
    "  \"update_authority\": \"FkNS1zBQrsb9a4PAvfBArGNcjzwZjEqEnLm7YMr5xqY8\",\n",
    "  \"name\": \"MILKBAG\",\n",
    "  \"symbol\": \"MILKBAG\",\n",
    "  \"uri\": \"https://bafkreibmwrmuk4x4gawfluqfvmjjfvsq5gwblyn3cnwknmnmcw5l32etsy.ipfs.nftstorage.link/\",\n",
    "  \"seller_fee_basis_points\": \"0\",\n",
    "  \"creators\": [{\n",
    "    \"address\": null,\n",
    "    \"verified\": null,\n",
    "    \"share\": null\n",
    "  }],\n",
    "  \"primary_sale_happened\": \"false\",\n",
    "  \"is_mutable\": \"false\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "count(*)\n",
    "FROM\n",
    "  `bigquery-public-data.crypto_solana_mainnet_us.Token Transfers` t\n",
    "  -- `bigquery-public-data.crypto_solana_mainnet_us.Tokens` t\n",
    "WHERE\n",
    "  t.block_timestamp between '2024-03-01' and '2024-03-31'\n",
    " and mint='2ubuHGFS4VJVxSEpvV3kDwz6JiuXdaAoGMwrwYC87tp8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transaction types data assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level assessment\n",
    "\n",
    "A pipeline to retrieve accurate transfers data should be buildable using the bigquery components, but will require different treatments for transfer, burn, and mint transactions. A key concept to understand is that solana differentiates accounts (token-specific adddresses) from owners (general addresses that own an account for their token transactions). One owner can have many accounts, and all accounts have an owner. \n",
    "\n",
    "Transaction success/failure is only available in the bigquery Transactions table which is 346 billion records. It is unclear how failed transactions could be filtered from the dataset without querying this table, or querying dune for every single transaction. \n",
    "\n",
    "#### Transfers\n",
    "\n",
    "* Sender: account address is available\n",
    "* Receiver: account address is available\n",
    "* Token Address: available\n",
    "* Amount: available\n",
    "* Timestamp: available\n",
    "\n",
    "A crosswalk from account to owner would need to be made. \n",
    "\n",
    "\n",
    "#### Burns\n",
    "\n",
    "* Sender: owner address is available\n",
    "* Receiver: not applicable\n",
    "* Token Address: available\n",
    "* Amount: available\n",
    "* Timestamp: available\n",
    "\n",
    "#### Mints\n",
    "\n",
    "* Sender: not applicable\n",
    "* Receiver: no information available\n",
    "* Token Address: available\n",
    "* Amount: available\n",
    "* Timestamp: available\n",
    "\n",
    "the receiver info would have to be extracted from dune. it could potentially be in the bigquery transactions table but that is massive and would be expensive to incorporate. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spl-transfer\n",
    "\n",
    "34649/65166 records\n",
    "\n",
    "Key Fields\n",
    "\n",
    "* sender address: source\n",
    "* receiver address: destination\n",
    "* token address: mint\n",
    "* amount: value\n",
    "\n",
    "Note that these refer to the address rather than the owner. Burn records show the owner. A crosswalk should be able to be created via either the bigquery Accounts table or via Dune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # or specify a number for a custom limit\n",
    "\n",
    "query_sql = \"\"\"\n",
    "    select *\n",
    "    from `sandbox.solana_transfers_2023_05`\n",
    "    where transfer_type = 'spl-transfer'\n",
    "\"\"\"\n",
    "\n",
    "transfer_df = dgc().run_sql(query_sql)\n",
    "transfer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## burn\n",
    "\n",
    "30426/65166 records\n",
    "\n",
    "### Key Fields\n",
    "* sender account owner: authority\n",
    "* token address: mint\n",
    "* amount: value\n",
    "\n",
    "### ETL Feasibility\n",
    "we should have enough data in this table to generate transfers calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sql = \"\"\"\n",
    "    select *\n",
    "    from `sandbox.solana_transfers_2023_05`\n",
    "    where transfer_type = 'burn'\n",
    "\"\"\"\n",
    "\n",
    "burn_df = dgc().run_sql(query_sql)\n",
    "burn_df.head()\n",
    "# burn_df['tx_signature'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mintTo \n",
    "\n",
    "91/65166 records\n",
    "\n",
    "the data from bigquery does not include anything about who received the tokens, and will need to be matched to another source in order to obtain the receiving wallet address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sql = \"\"\"\n",
    "    select *\n",
    "    from `sandbox.solana_transfers_2023_05`\n",
    "    where transfer_type = 'mintTo'\n",
    "\"\"\"\n",
    "\n",
    "mint_df = dgc().run_sql(query_sql)\n",
    "mint_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL feasibility\n",
    "\n",
    "the biggest risk is that there is a problem with either bigquery or dune records, or that the joins between the two are somehow imperfect. as of now there are 7347 total mint transactions that would need to be retrieved for all solana core.coins which means that the total volume of extractions would be very low compared to transfers pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dune count query\n",
    "\n",
    "dune_sql = \"\"\"\n",
    "    with dreams_tokens as (\n",
    "        select token_address\n",
    "        from dune.dreamslabs.etl_net_transfers_freshness t\n",
    "        where chain = 'solana'\n",
    "        group by 1\n",
    "    )\n",
    "\n",
    "    select count(*)\n",
    "    from tokens_solana.transfers t\n",
    "    join dreams_tokens dt on dt.token_address = t.token_mint_address\n",
    "    where action = 'mint'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matching mint transfers to dune\n",
    "\n",
    "the bigquery table does not provide information about the token receiver. we have the transaction signature, token address (mint), and amount (value) but there is no information about who receives the tokens as the destination and authority fields are all empty. \n",
    "\n",
    "the transaction signature can be matched to dune where the receiving account owner data is available. \n",
    "\n",
    "match fields:\n",
    "* bq.mint = dune.token_mint_address\n",
    "* bq.tx_signature = dune.tx_id\n",
    "* bq.value = dune.amount\n",
    "\n",
    "wallet_address_receiver\n",
    "* dune.to_owner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example record match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigquery data\n",
    "\n",
    "tx = '3RhHy1p2p5uAw7QDFjuNwWUQqtuCwfwFJ9BQLNE3UBHmZgpTdWCECGzzFJ8qkGnuoZvQpNZNiSoeXz97DRm64azq'\n",
    "token = 'hntyVP6YFm1Hg25TN9WGLqM12b8TQmcknKrdu1oxWux'\n",
    "mint_df[\n",
    "    (mint_df['tx_signature']==tx)\n",
    "    & (mint_df['mint']==token)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dune data\n",
    "\n",
    "dune_query = f\"\"\"\n",
    "    select *\n",
    "    from tokens_solana.transfers t\n",
    "    where block_date = cast('2023-05-01' as date)\n",
    "    and tx_id = {tx}\n",
    "    and token_mint_address = {token}\n",
    "    and action = 'mint'\n",
    "\"\"\"\n",
    "\n",
    "pd.read_csv('solana_transfer_analysis/mintTo_dune_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export data from public bigquery to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "EXPORT DATA\n",
    "OPTIONS(\n",
    "  uri='gs://dreams-labs-storage/data_lake/solana_transfers/dev_2023_05/2023_05_*.csv',\n",
    "  format='CSV',\n",
    "  overwrite=true\n",
    ") AS\n",
    "\n",
    "-- SELECT tx_signature\n",
    "-- ,block_timestamp\n",
    "-- ,source\n",
    "-- ,destination\n",
    "-- ,mint\n",
    "-- ,value\n",
    "select *\n",
    "FROM\n",
    "  `bigquery-public-data.crypto_solana_mainnet_us.Token Transfers` t\n",
    "WHERE\n",
    "  t.block_timestamp between '2023-05-01' and '2023-05-31'\n",
    "and t.mint in (\n",
    "'Cbet5zMSzpCUKmVfy4nSkBjGPSy7WoUEwqLcQ6HhrHVA'\n",
    ",'HovGjrBGTfna4dvg6exkMxXuexB3tUfEZKcut8AWowXj','7SdFACfxmg2eetZEhEYZhsNMVAu84USVtfJ64jFDCg9Y','25hAyBQfoDhfWx9ay6rarbgvWGwDdNqcHsXS3jQ3mTDJ','D1nj2nyuVLHtL1Fd96hXzhUgaet9c9LTvXRs7E2Rpump','4XQvdipJBdrb5hUgUrbZPPFmp6BCav41n55dc7KDYW3m','A3eME5CetyZPBoWbRUwY3tSe25S6tb18ba9ZPbWk9eFJ','2ez1pFrqmsXa4688qMQezgxGq7cDWhKPebJTHPSdUXDY','5mbK36SZ7J19An8jFochhQS4of8g6BwUjbeCSxBSoWdp','orcaEKTdK7LKz57vaAYr9QeNsVEPfiu6QeMU1kektZE','ZhiSHHegARupUWNj8nNoV9Q8CgTpYLz79U3m3UF3r9b','8mq2np5SgMpJxZeNXjeoVYvjNWAnGKhYJU2Xj4GxFz5Q','HZ1JovNiVvGrGNiiYvEozEVgZ58xaU3RKwX8eACQBCt3','45EgCwcPXYagBC7KqBin4nCFgEZWN7f3Y6nACwxqMCWX','26KMQVgDUoB6rEfnJ51yAABWWJND8uMtpnQgsHQ64Udr','4G3kNxwaA2UQHDpaQtJWQm1SReXcUD7LkT14v2oEs7rV','EjErrBoCw7eWYkMfimhPckaPSuBukyhUYwv2dLJYDWB7','EHCwJQi8dSpZfKm4LJypzozEj5vAN7pESRXJGpESKMfJ','6yjNqPzTSanBWSa6dxVEgTjePXBrZ2FoHLDQwYwEsyM6','FYa25XnBsXQXAdTnsyKBKd5gZ1VZhChBRF57CqfRxJZX','6tWuipcDv4CiHtXid7JctDbtLLA1VdkTwSDupK6UxzJL','8doS8nzmgVZEaACxALkbK5fZtw4UuoRp4Yt8NEaXfDMb','9BBd5VJUPK41ntmdEvBMdGg2aXSnDpgVYxcDwP5c78Ym','69kdRLyP5DTRkpHraaSZAQbWmAwzF9guKjZfzMXzcbAs','C1kzNkFfgdtP8VF1pFYA4S32RLPqk5KPaurCaQJwxfWb','E2BGnzHdJNUBtAVR7EyQMuEMHqgv65JL8J9ZyqyXUVvA','EKpQGSJtjMFqKZ9KQanSqYXRcF8fBopzLHYxdM65zcjm','5LafQUrVco6o7KMz42eqVEJ9LW31StPyGjeeu5sKoMtA','4pb6mymm9hYQN6og9uF24eyZ2qwXCWCwGvcR1DkCgeEr','9XRpjZjhJPeWtUymiEWn3FW7uAnMeQca14ucTWWWyP2g','7D7BRcBYepfi77vxySapmeqRNN1wsBBxnFPJGbH5pump','8m9fjYycXAFva1kScttQgsESVZT7yELhjZASqfHBuMa5','ukHH6c7mMyiWCf1b9pnWe25TSpkDDt3H5pQZgZ74J82','3de2yRhtD4VbJBb8EQAQffYMPLU4EnSHT1eveBwiL3tn','AVLhahDcDQ4m4vHM4ug63oh7xc8Jtk49Dm5hoe9Sazqr','SNApmcWQqj3Ny2YFkQmkELQnNgaXRu6KmnYSPiFZcLn','GEdBv2DnES89DvasmZ35TaxP9kBibznYKbacXpoGTEBU','pawSXHWsonrTey4SX7tz1fM9ksuLpE13Y54K57ym4Rg','nosXBVoaCTtYdLvKY6Csb4AC8JCdQKKAaWYtx2ZMoo7','7vuhsRQ2gE4WPv37qegBKu8PcWHxDb5rQ6fQKkDfUghF','BNT4uhSStq1beFADv3cq4wQAVfWB392PjAaxTBpNeWxu','B5LMXiuvbB5jN3auECUtdfyeFWm27krgFinrBrqJGFRM','Adq3wnAvtaXBNfy63xGV1YNkDiPKadDT469xF9uZPrqE','7iT1GRYYhEop2nV1dyCwK2MGyLmPHq47WhPGSwiqcUg5','hntyVP6YFm1Hg25TN9WGLqM12b8TQmcknKrdu1oxWux','HuPspkki5Qdnf5WAU7jtEThkeMhni6XQ23tunZRkZWUi','754Ry9yULcPgSHcmfLSQDiihZgs7917dJUZ9513FLkg7','DeoP2swMNa9d4SGcQkR82j4RYYeNhDjcTCwyzEhKwfAf','SHDWyBxihqiCj6YekG2GUr7wqKLeLAMK1gHZck9pL6y','9V4x6ikFm9XKsnh3TiYJWPwQfFkJZDjifu7VSUqg3es1','BWXWbFu8bYtJRrDb4bRpaSPz8PQZvTG8ZK5bwkPWhgcJ','SNSNkV9zfG5ZKWQs6x4hxvBRV6s8SqMfSGCtECDvdMd','BGyjasmSzYM9hHiZ1LBU4EJ7KCtRjMSpbN4zTru3W5vf','CdZaJzc2BdmHhbr3LTP4DCPKyBu4zJrfB2mQKCgURUgp','NeonTjSjsuo3rexg9o6vHuMXw62f9V7zvmu8M8Zut44','8wXtPeU6557ETkp9WHFY1n1EcU6NxDvbAggHGsMYiHsB','jtojtomepa8beP8AuQc6eXt5FriJwfFMwQx2v2f9mCL','G33s1LiUADEBLzN5jL6ocSXqrT2wsUq9W6nZ8o4k1b4L','7GCihgDB8fe6KNjn2MYtkzZcRjQy3t9GHdC8uHYmW2hr','octo82drBEdm8CSDaEKBymVn86TBtgmPnDdmE64PTqJ','3ag1Mj9AKz9FAkCQ6gAEhpLSX8B2pUbPdkb9iBsDLZNB','MNDEFzGvMt87ueuHvVU9VcTqsAP5b3fTGPsHuuPA5ey','FoVeWwe6H6hWEa1cQfZeNSGsGSDyKz57CZT49BusdshW','DcUoGUeNTLhhzyrcz49LE7z3MEFwca2N9uSw1xbVi1gm','PUPS8ZgJ5po4UmNDfqtDMCPP6M1KP3EEzG9Zufcwzrg','5z3EqYQo9HiCEs3R84RCDMu2n7anpDMxRhdK8PSWmrRC','GtDZKAqvMZMnti46ZewMiXCa4oXF4bZxwQPoKzXPFxZn','52DfsNknorxogkjqecCTT3Vk2pUwZ3eMnsYKVm4z3yWy','FU1q8vJpZNUrmqsciSjp8bAKKidGsLmouB8CBdf8TKQv','BSHanq7NmdY6j8u5YE9A3SUygj1bhavFqb73vadspkL3','Avp2VDgnQqxsnrjtq3ynNhKCfWGEGj1PmGLY5ZmgonjH','7njsg9BA1xvXX9DNpe5fERHK4zb7MbCHKZ6zsx5k3adr','DdqUGjhtZ8uNU7YHRsNFwXL5qM8Dbyiuzm22DRheN3aK','2ubuHGFS4VJVxSEpvV3kDwz6JiuXdaAoGMwrwYC87tp8','ULwSJmmpxmnRfpu6BjnK6rprKXqD5jXUmPpS1FxHXFy','4tJZhSdGePuMEfZQ3h5LaHjTPsw1iWTRFTojnZcwsAU6','GDfnEsia2WLAW5t8yx2X5j2mkfA74i5kwGdDuZHt7XmG','EJPtJEDogxzDbvM8qvAsqYbLmPj5n1vQeqoAzj9Yfv3q','Hf5gAgohzfUyjytaF5aUSMDwsPAbdThQJNnqw97reGMw','EsirN3orp85uyvZyDrZnbe9cyo7N1114ynLFdwMPCQce','3bRTivrVsitbmCTGtqwp7hxXPsybkjn4XLNtPsHqa3zR','BiDB55p4G3n1fGhwKFpxsokBMqgctL4qnZpDH1bVQxMD','947tEoG318GUmyjVYhraNRvWpMX7fpBTDQFBoJvSkSG3','AfcvNFud8cQPKpCZtW8GBsJqi2LJNztFPu8d4vciveL3','5ritAPtFPqQtEFHcHVqNjR5oFNUJqcmgKtZyPd2AyLLy','9niFQK8MsParjSxhYQ3Ys2a6zHmsEuKSB1M7wwdvZ7bj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract GCS data to project bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "def combine_csv_files(bucket_name, prefix):\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # List all blobs with the given prefix\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    # Column names\n",
    "    colnames = [\n",
    "        'block_slot', 'block_hash', 'block_timestamp', 'tx_signature',\n",
    "        'source', 'destination', 'authority', 'value', 'decimals',\n",
    "        'mint', 'mint_authority', 'fee', 'fee_decimals', 'memo', 'transfer_type'\n",
    "    ]\n",
    "\n",
    "    # List to store all dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through all blobs\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.csv'):\n",
    "            # Download the content of the blob\n",
    "            content = blob.download_as_text()\n",
    "\n",
    "            # Create a DataFrame from the content\n",
    "            df = pd.read_csv(StringIO(content), names=colnames, header=None)\n",
    "\n",
    "            # Only append if the DataFrame is not empty\n",
    "            if not df.empty:\n",
    "                dfs.append(df)\n",
    "\n",
    "            print(f\"Processed {blob.name}\")\n",
    "\n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Usage\n",
    "bucket_name = \"dreams-labs-storage\"\n",
    "prefix = \"data_lake/solana_transfers/dev_2023_05/\"\n",
    "\n",
    "result_df = combine_csv_files(bucket_name, prefix)\n",
    "\n",
    "# # Optional: Save the combined DataFrame to a local CSV file\n",
    "# result_df.to_csv(\"combined_solana_transfers.csv\", index=False)\n",
    "\n",
    "print(f\"Combined DataFrame shape: {result_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def upload_to_bigquery(df, project_id, dataset_id, table_id):\n",
    "    # Initialize BigQuery client\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Define the table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "    # Define the job config\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        write_disposition=\"WRITE_TRUNCATE\",  # This will overwrite the table if it exists\n",
    "    )\n",
    "\n",
    "    # Load the dataframe into BigQuery\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Loaded {job.output_rows} rows into {project_id}:{dataset_id}.{table_id}\")\n",
    "\n",
    "\n",
    "# Upload to BigQuery\n",
    "project_id = \"western-verve-411004\"  # Replace with your Google Cloud project ID\n",
    "dataset_id = \"sandbox\"  # Replace with your BigQuery dataset ID\n",
    "table_id = \"solana_transfers_2023_05\"   # Replace with your desired table name\n",
    "\n",
    "upload_to_bigquery(result_df, project_id, dataset_id, table_id)\n",
    "\n",
    "# Optionally, you can still save the combined DataFrame to a local CSV file\n",
    "# result_df.to_csv(\"combined_solana_transfers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
