{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as ri\n",
    "import feature_engineering as fe\n",
    "import coin_wallet_metrics as cwm\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.model_input_flows as mf\n",
    "import utils as u\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [dr, ri, fe, cwm, m, ia, mf, u]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Initialize empty lists to hold concatenated data\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "returns_test_list = []\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mf.generate_time_windows(config)\n",
    "\n",
    "for n, window in enumerate(time_windows):\n",
    "\n",
    "    model_data = mf.build_time_window_model_input(n, window, config, metrics_config, modeling_config)\n",
    "\n",
    "    # Append the current window's data to the lists\n",
    "    X_train_list.append(model_data['X_train'])\n",
    "    X_test_list.append(model_data['X_test'])\n",
    "    y_train_list.append(model_data['y_train'])\n",
    "    y_test_list.append(model_data['y_test'])\n",
    "    returns_test_list.append(model_data['returns_test'])\n",
    "\n",
    "\n",
    "# Concatenate all the data for each part\n",
    "X_train = pd.concat(X_train_list, axis=0)\n",
    "X_test = pd.concat(X_test_list, axis=0)\n",
    "y_train = pd.concat(y_train_list, axis=0)\n",
    "y_test = pd.concat(y_test_list, axis=0)\n",
    "returns_test = pd.concat(returns_test_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict, y_pred, y_pred_prob = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns  # Feature names\n",
    "\n",
    "# Create a DataFrame with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob or y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob or y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### global_market_data from coingecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/jeremymeadow/DreamsData/Local/datasets/macro_trends/CoinGecko-GlobalCryptoMktCap-2024-10-04.csv\")\n",
    "df['date'] = pd.to_datetime(df['snapped_at'], unit='ms')\n",
    "df = df.drop(columns='snapped_at')\n",
    "df = df[['date','market_cap','total_volume']]\n",
    "\n",
    "df.to_csv(\"/Users/jeremymeadow/DreamsData/Local/datasets/macro_trends/formatted/crypto_global_market.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bitcoin indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_process_bitcoin_data(directory_path):\n",
    "    # List to store individual dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate through CSV files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Convert 'DateTime' column to datetime type\n",
    "            df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "            # Set 'DateTime' as the index\n",
    "            df.set_index('DateTime', inplace=True)\n",
    "\n",
    "            # Append to the list of dataframes\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Join all dataframes\n",
    "    combined_df = pd.concat(dfs, axis=1, join='outer')\n",
    "\n",
    "    # Remove duplicate 'BTC price' columns\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "\n",
    "    # Define a dictionary for column renaming\n",
    "    rename_dict = {\n",
    "        'BTC price': 'btc_price',\n",
    "        'CDD Terminal Ajusted 90dma': 'cdd_terminal_adjusted_90dma',\n",
    "        'Fear and Greed': 'fear_and_greed',\n",
    "        'MVRV Z-Score': 'mvrv_z_score',\n",
    "        'VDD Multiple': 'vdd_multiple'\n",
    "    }\n",
    "\n",
    "    # Rename columns using the dictionary\n",
    "    combined_df = combined_df.rename(columns=rename_dict)\n",
    "\n",
    "    # Rename the index (DateTime column)\n",
    "    combined_df.index.name = 'date'\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "# df = load_and_process_bitcoin_data('/path/to/csv/directory')\n",
    "# print(df.head())\n",
    "\n",
    "df = load_and_process_bitcoin_data('/Users/jeremymeadow/DreamsData/Local/datasets/macro_trends/bitcoin_macro_indicators')\n",
    "\n",
    "# remove partial recent records\n",
    "df = df.loc[:'2024-10-02']\n",
    "df.to_csv(\"/Users/jeremymeadow/DreamsData/Local/datasets/macro_trends/formatted/bitcoin_indicators.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "\n",
    "# Retrieve data\n",
    "macro_trends_df = dr.retrieve_macro_trends_data()\n",
    "macro_trends_df = cwm.generate_macro_trends_features(macro_trends_df, config)\n",
    "macro_trends_tuples, _ = fe.generate_macro_trends_features(\n",
    "        macro_trends_df,\n",
    "        config,\n",
    "        metrics_config,\n",
    "        modeling_config\n",
    "    )\n",
    "\n",
    "macro_trends_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "dataset_df = macro_trends_df.copy().reset_index()\n",
    "\n",
    "\n",
    "# function variables we want to reference\n",
    "dataset_category = 'macro_trends'\n",
    "id_column=None\n",
    "dataset_metrics_config = metrics_config[dataset_category]\n",
    "\n",
    "# Model-ready tuples and dfs will go here\n",
    "training_data_tuples = []\n",
    "training_data_dfs = []\n",
    "\n",
    "\n",
    "# calculate metrics for each value column\n",
    "for value_column in list(dataset_metrics_config.keys()):\n",
    "\n",
    "    # a value_column-specific df will be used for feature generation\n",
    "    value_column_config = config['datasets'][dataset_category][value_column]\n",
    "    value_column_metrics_config = dataset_metrics_config[value_column]\n",
    "    value_column_df = dataset_df[['date',value_column]].copy()\n",
    "    # check if there are any time series indicators to add, e.g. sma, ema, etc\n",
    "    if 'indicators' in value_column_metrics_config:\n",
    "        value_column_metrics_df, _ = cwm.generate_time_series_indicators(\n",
    "            value_column_df,\n",
    "            config,\n",
    "            value_column_metrics_config['indicators'],\n",
    "            value_column,\n",
    "            id_column\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # if no indicators are needed, pass through coins with complete date coverage\n",
    "        logging.getLogger().setLevel(logging.WARNING) # suppress INFO logs about splits\n",
    "        value_column_metrics_df, _ = cwm.split_dataframe_by_coverage(\n",
    "            value_column_df,\n",
    "            config['training_data']['training_period_start'],\n",
    "            config['training_data']['training_period_end'],\n",
    "            id_column\n",
    "        )\n",
    "        logging.getLogger().setLevel(logging.INFO) # could be updated to use original level\n",
    "\n",
    "\n",
    "    # flatten metrics\n",
    "    flattened_features = fe.flatten_date_features(value_column_metrics_df,dataset_metrics_config)\n",
    "    flattened_macro_trends_df = pd.DataFrame([flattened_features])\n",
    "\n",
    "    # save flattened metrics\n",
    "    flattened_macro_trends_df, flattened_macro_trends_filepath = fe.save_flattened_outputs(\n",
    "        flattened_macro_trends_df,\n",
    "        os.path.join(\n",
    "            modeling_config['modeling']['modeling_folder'],  # Folder to store flattened outputs\n",
    "            'outputs/flattened_outputs'\n",
    "        ),\n",
    "        value_column_config['description'],  # Descriptive metadata for the dataset\n",
    "        config['training_data']['modeling_period_start']  # Ensure data starts from modeling period\n",
    "    )\n",
    "\n",
    "    # preprocess metrics\n",
    "    macro_trends_preprocessed_df, macro_trends_preprocessed_filepath = fe.preprocess_coin_df(\n",
    "        flattened_macro_trends_filepath\n",
    "        ,modeling_config\n",
    "        ,value_column_config\n",
    "        ,value_column_metrics_config\n",
    "    )\n",
    "\n",
    "    macro_trends_tuple = (macro_trends_preprocessed_filepath.split('preprocessed_outputs/')[1],\n",
    "                            value_column_config['fill_method'])\n",
    "    logger.info('Generated features for %s.%s',\n",
    "                dataset_category, value_column)\n",
    "\n",
    "    training_data_tuples.append(macro_trends_tuple)\n",
    "    training_data_dfs.append(macro_trends_preprocessed_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_column_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "training_data_tuples = []\n",
    "\n",
    "# 1. Generate and merge features for all datasets\n",
    "# -------------------------------------\n",
    "\n",
    "# Macro trends features\n",
    "macro_trends_tuples, _ = fe.generate_macro_trends_features(\n",
    "        macro_trends_df,\n",
    "        config,\n",
    "        metrics_config,\n",
    "        modeling_config\n",
    "    )\n",
    "training_data_tuples.extend(macro_trends_tuples)\n",
    "\n",
    "# Merge all the features\n",
    "training_data_df, _ = fe.create_training_data_df(\n",
    "                        modeling_config['modeling']['modeling_folder'],\n",
    "                        training_data_tuples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = config['datasets']['macro_trends'][dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metrics_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# set parameters\n",
    "dataset_name = 'google_trends'\n",
    "dataset_df = google_trends_df\n",
    "config,\n",
    "metrics_config,\n",
    "modeling_config\n",
    "\n",
    "training_data_tuples, training_data_dfs = fe.generate_macro_trends_features(\n",
    "        dataset_name,\n",
    "        dataset_df,\n",
    "        config,\n",
    "        metrics_config,\n",
    "        modeling_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config['macro_trends'][dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = config['datasets']['macro_trends'][dataset_name]\n",
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten metrics\n",
    "flattened_features = fe.flatten_date_features(value_column_metrics_df,dataset_metrics_config)\n",
    "flattened_google_trends_df = pd.DataFrame([flattened_features])\n",
    "\n",
    "# save flattened metrics\n",
    "flattened_google_trends_df, flattened_google_trends_filepath = fe.save_flattened_outputs(\n",
    "    flattened_google_trends_df,\n",
    "    os.path.join(\n",
    "        modeling_config['modeling']['modeling_folder'],  # Folder to store flattened outputs\n",
    "        'outputs/flattened_outputs'\n",
    "    ),\n",
    "    value_column_config['description'],  # Descriptive metadata for the dataset\n",
    "    config['training_data']['modeling_period_start']  # Ensure data starts from modeling period\n",
    ")\n",
    "\n",
    "# preprocess metrics\n",
    "google_trends_preprocessed_df, google_trends_preprocessed_filepath = fe.preprocess_coin_df(\n",
    "    flattened_google_trends_filepath\n",
    "    ,modeling_config\n",
    "    ,value_column_config\n",
    "    ,value_column_metrics_config\n",
    ")\n",
    "\n",
    "google_trends_tuple = (google_trends_preprocessed_filepath.split('preprocessed_outputs/')[1], value_column_config['fill_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['datasets']['macro_trends'][dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# set up config variables\n",
    "dataset_category = 'macro_trends'\n",
    "dataset_name = 'google_trends'\n",
    "dataset_config = config['datasets'][dataset_category][dataset_name]\n",
    "dataset_metrics_config = metrics_config[dataset_category][dataset_name]\n",
    "\n",
    "# load dataset\n",
    "google_trends_df = td.retrieve_google_trends_data()\n",
    "\n",
    "\n",
    "# calculate and merge all metrics in the config\n",
    "all_metrics = []\n",
    "for key in list(dataset_metrics_config.keys()):\n",
    "    value_column_metrics_config = metrics_config[dataset_category][dataset_name][key]\n",
    "    metric_df = google_trends_df[['date',key]]\n",
    "\n",
    "    # check if there are any time series indicators to add, e.g. sma, ema, etc\n",
    "    if 'indicators' in value_column_metrics_config:\n",
    "        value_column_metrics_df, _ = cwm.generate_time_series_indicators(\n",
    "            metric_df,\n",
    "            config,\n",
    "            value_column_metrics_config,\n",
    "            key,\n",
    "            id_column=None\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # if no indicators are needed, pass through coins with complete date coverage\n",
    "        logging.getLogger().setLevel(logging.WARNING)\n",
    "        value_column_metrics_df, _ = cwm.split_dataframe_by_coverage(\n",
    "            value_column_df,\n",
    "            config['training_data']['training_period_start'],\n",
    "            config['training_data']['training_period_end'],\n",
    "            id_column='coin_id'\n",
    "        )\n",
    "        logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    all_metrics.append(metric_df)\n",
    "\n",
    "all_metrics_df = all_metrics[0]\n",
    "for metrics_df in all_metrics[1:]:\n",
    "    all_metrics_df = pd.merge(all_metrics_df, metrics_df, on='date', how='outer')\n",
    "\n",
    "\n",
    "# flatten metrics\n",
    "flattened_features = fe.flatten_date_features(all_metrics_df,dataset_metrics_config)\n",
    "flattened_google_trends_df = pd.DataFrame([flattened_features])\n",
    "\n",
    "# save flattened metrics\n",
    "flattened_google_trends_df, flattened_google_trends_filepath = fe.save_flattened_outputs(\n",
    "    flattened_google_trends_df,\n",
    "    os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs'),\n",
    "    dataset_config['description'],\n",
    "    config['training_data']['modeling_period_start']\n",
    ")\n",
    "\n",
    "# preprocess metrics\n",
    "google_trends_preprocessed_df, google_trends_preprocessed_filepath = fe.preprocess_coin_df(\n",
    "    flattened_google_trends_filepath\n",
    "    ,modeling_config\n",
    "    ,dataset_config\n",
    "    ,dataset_metrics_config\n",
    ")\n",
    "\n",
    "google_trends_tuple = (google_trends_preprocessed_filepath.split('preprocessed_outputs/')[1], dataset_config['fill_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "importlib.reload(u)\n",
    "config = u.load_config('../config/config.yaml')\n",
    "metrics_config = u.load_config('../config/metrics_config.yaml')\n",
    "modeling_config = u.load_config('../config/modeling_config.yaml')\n",
    "experiments_config = u.load_config('../config/experiments_config.yaml')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Retrieve market data\n",
    "market_data_df = td.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# Retrieve profits data if necessary\n",
    "if 'profits_df' not in globals():\n",
    "    profits_df = None\n",
    "profits_df = i.rebuild_profits_df_if_necessary(\n",
    "                config,\n",
    "                modeling_folder,\n",
    "                prices_df,\n",
    "                profits_df)\n",
    "\n",
    "# Filter market_data rows without transfers if configured to do so\n",
    "if config['data_cleaning']['exclude_coins_without_transfers']:\n",
    "    market_data_df = market_data_df[market_data_df['coin_id'].isin(profits_df['coin_id'])]\n",
    "    prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
