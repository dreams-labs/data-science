{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingModuleSource=false\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import argrelextrema\n",
    "import progressbar\n",
    "\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# import local files if necessary\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "from utils import load_config, cw_filter_df, create_progress_bar\n",
    "import training_data as td\n",
    "importlib.reload(td)\n",
    "import feature_engineering as fe\n",
    "importlib.reload(fe)\n",
    "import coin_wallet_metrics as cwm\n",
    "importlib.reload(cwm)\n",
    "import modeling as m\n",
    "importlib.reload(m)\n",
    "import insights as i\n",
    "importlib.reload(i)\n",
    "import utils as u\n",
    "importlib.reload(u)\n",
    "\n",
    "# load configs\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Custom format function for displaying numbers\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "# retrieve transfers data\n",
    "transfers_df = td.retrieve_transfers_data(\n",
    "    config['training_data']['training_period_start'],\n",
    "    config['training_data']['modeling_period_start'],\n",
    "    config['training_data']['modeling_period_end']\n",
    "    )\n",
    "\n",
    "# retrieve and clean prices data\n",
    "prices_df = td.retrieve_prices_data()\n",
    "prices_df,prices_log = td.fill_prices_gaps(prices_df,config['data_cleaning']['max_gap_days'])\n",
    "\n",
    "# compile profits_df\n",
    "profits_df = td.prepare_profits_data(transfers_df, prices_df)\n",
    "profits_df = td.calculate_wallet_profitability(profits_df)\n",
    "profits_df,_ = td.clean_profits_df(profits_df, config['data_cleaning'])\n",
    "\n",
    "\n",
    "# cohort configurations\n",
    "cohort_name = list(config['wallet_cohorts'].keys())[0]\n",
    "cohort_description = f\"{cohort_name}_cohort\"\n",
    "cohort_metrics_config = metrics_config['wallet_cohorts'][cohort_name]\n",
    "\n",
    "# identify wallets in the cohort\n",
    "cohort_summary_df = td.classify_wallet_cohort(profits_df, config['wallet_cohorts'][cohort_name])\n",
    "cohort_wallets = cohort_summary_df[cohort_summary_df['in_cohort']==True]['wallet_address']\n",
    "\n",
    "# generate cohort buysell_metrics\n",
    "buysell_metrics_df = cwm.generate_buysell_metrics_df(profits_df,config['training_data']['training_period_end'],cohort_wallets)\n",
    "\n",
    "# generate prices metrics metrics\n",
    "prices_metrics_df,x = cwm.generate_time_series_metrics(prices_df, metrics_config, dataset_key='prices', colname='price')\n",
    "\n",
    "print(buysell_metrics_df.shape)\n",
    "print(prices_metrics_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices preprocessing\n",
    "prices_metrics_config = metrics_config['time_series']['prices']\n",
    "prices_description = 'prices_timeseries'\n",
    "\n",
    "# flatten, save, and preprocess the flattened df\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "\n",
    "flattened_prices_metrics_df = fe.flatten_coin_date_df(\n",
    "    prices_metrics_df,\n",
    "    prices_metrics_config,\n",
    "    config['training_data']['training_period_end']\n",
    ")\n",
    "flattened_prices_metrics_df, flattened_prices_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_prices_metrics_df,\n",
    "    flattened_output_directory,\n",
    "    prices_description,\n",
    "    config['training_data']['modeling_period_start']\n",
    ")\n",
    "prices_preprocessed_df, prices_preprocessed_filepath = fe.preprocess_coin_df(flattened_prices_metrics_filepath, modeling_config, prices_metrics_config)\n",
    "\n",
    "\n",
    "\n",
    "# buysell metrics df processing\n",
    "cohort_metrics_config = metrics_config['wallet_cohorts'][cohort_name]\n",
    "cohort_name = list(config['wallet_cohorts'].keys())[0]\n",
    "cohort_description = f\"{cohort_name}_cohort\"\n",
    "\n",
    "# flatten, save, and preprocess the flattened df\n",
    "flattened_output_directory = os.path.join(modeling_config['modeling']['modeling_folder'],'outputs/flattened_outputs')\n",
    "\n",
    "flattened_buysell_metrics_df = fe.flatten_coin_date_df(\n",
    "    buysell_metrics_df,\n",
    "    cohort_metrics_config,\n",
    "    config['training_data']['training_period_end']\n",
    ")\n",
    "flattened_buysell_metrics_df, flattened_buysell_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_buysell_metrics_df,\n",
    "    flattened_output_directory,\n",
    "    cohort_description,\n",
    "    config['training_data']['modeling_period_start']\n",
    "    )\n",
    "buysell_preprocessed_df, buysell_preprocessed_filepath = fe.preprocess_coin_df(flattened_buysell_metrics_filepath, modeling_config, cohort_metrics_config)\n",
    "\n",
    "print(buysell_preprocessed_df.shape)\n",
    "print(prices_preprocessed_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create the training data df\n",
    "input_filenames = [\n",
    "    (buysell_preprocessed_filepath.split('preprocessed_outputs/')[1], 'drop_records'),\n",
    "    (prices_preprocessed_filepath.split('preprocessed_outputs/')[1], 'drop_records')\n",
    "]\n",
    "training_data_df, merge_logs_df = fe.create_training_data_df(modeling_config['modeling']['modeling_folder'], input_filenames)\n",
    "\n",
    "# create the target variable df\n",
    "target_variable_df,_ = fe.create_target_variables_mooncrater(prices_df, config['training_data'], modeling_config)\n",
    "\n",
    "# merge the two into the final model input df\n",
    "model_input_df = fe.prepare_model_input_df(training_data_df, target_variable_df, modeling_config['modeling']['target_column'])\n",
    "\n",
    "# split the df into train and test sets\n",
    "X_train, X_test, y_train, y_test = m.split_model_input(\n",
    "    model_input_df,\n",
    "    modeling_config['modeling']['target_column'],\n",
    "    modeling_config['modeling']['train_test_split'],\n",
    "    modeling_config['modeling']['random_state']\n",
    ")\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "modeling_folder = modeling_config['modeling']['modeling_folder']\n",
    "model, model_id = m.train_model(X_train, y_train, modeling_folder, modeling_config['modeling']['model_params'])\n",
    "\n",
    "# 3.5 Evaluate the model's performance on the test set\n",
    "metrics = m.evaluate_model(model, X_test, y_test, model_id, modeling_folder)\n",
    "\n",
    "# 3.6 Log the experiment results for this configuration\n",
    "m.log_trial_results(modeling_folder, model_id)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_input_files_colnames(tmpdir):\n",
    "    \"\"\"\n",
    "    Unit test data for scenario with many duplicate columns and similar filenames.\n",
    "    \"\"\"\n",
    "    # Create mock filenames and corresponding DataFrames\n",
    "    filenames = [\n",
    "        'buysell_metrics_2024-09-13_14-44_model_period_2024-05-01_v0.1.csv',\n",
    "        'buysell_metrics_2024-09-13_14-45_model_period_2024-05-01_v0.1.csv',\n",
    "        'buysell_metrics_megasharks_2024-09-13_14-45_model_period_2024-05-01_v0.1.csv',\n",
    "        'buysell_metrics_megasharks_2024-09-13_14-45_model_period_2024-05-01_v0.2.csv',\n",
    "        'price_metrics_2024-09-13_14-45_model_period_2024-05-01_v0.1.csv'\n",
    "    ]\n",
    "\n",
    "    # Create mock DataFrames for each file\n",
    "    df1 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [100, 200]})\n",
    "    df2 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [150, 250]})\n",
    "    df3 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [110, 210]})\n",
    "    df4 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [120, 220]})\n",
    "    df5 = pd.DataFrame({'coin_id': [1, 2], 'buyers_new': [130, 230]})\n",
    "\n",
    "    # Save each DataFrame as a CSV\n",
    "    for i, df in enumerate([df1, df2, df3, df4, df5]):\n",
    "        df.to_csv(os.path.join(tmpdir, filenames[i]), index=False)\n",
    "\n",
    "    # Create a tuple list with filenames and 'fill_zeros' strategy\n",
    "    input_files = [(filenames[i], 'fill_zeros') for i in range(len(filenames))]\n",
    "\n",
    "    return tmpdir, input_files\n",
    "\n",
    "tmpdir, input_files = mock_input_files_colnames('temp/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(td)\n",
    "importlib.reload(cwm)\n",
    "importlib.reload(fe)\n",
    "importlib.reload(m)\n",
    "importlib.reload(i)\n",
    "config = load_config('../config/config.yaml')\n",
    "metrics_config = load_config('../config/metrics_config.yaml')\n",
    "modeling_config = load_config('../config/modeling_config.yaml')\n",
    "experiments_config = load_config('../config/experiments_config.yaml')\n",
    "\n",
    "\n",
    "# def test_create_training_data_df(mock_input_files_colnames):\n",
    "\"\"\"\n",
    "Test column renaming logic for clarity when merging multiple files with similar filenames.\n",
    "\"\"\"\n",
    "# tmpdir, input_files = mock_input_files_colnames\n",
    "print(\"Input files:\", input_files)\n",
    "\n",
    "# Call the function\n",
    "merged_df, _ = fe.create_training_data_df(tmpdir, input_files)\n",
    "\n",
    "# Check if the columns have the correct suffixes\n",
    "expected_columns = [\n",
    "    'coin_id',\n",
    "    'buyers_new_buysell_metrics_2024-09-13_14-44',\n",
    "    'buyers_new_buysell_metrics_2024-09-13_14-45',\n",
    "    'buyers_new_buysell_metrics_megasharks_2024-09-13_14-45',\n",
    "    'buyers_new_buysell_metrics_megasharks_2024-09-13_14-45_2',\n",
    "    'buyers_new_price_metrics'\n",
    "]\n",
    "\n",
    "assert list(merged_df.columns) == expected_columns, \\\n",
    "    f\"Expected columns: {expected_columns}, but got: {list(merged_df.columns)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input files:\", input_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_input_dfs():\n",
    "    \"\"\"\n",
    "    Mock input DataFrames for merging with different coin_ids and fill strategies.\n",
    "    \"\"\"\n",
    "    # Create mock DataFrames\n",
    "    df1 = pd.DataFrame({'coin_id': [1, 2], 'metric_a': [100, 200]})\n",
    "    df2 = pd.DataFrame({'coin_id': [2, 3], 'metric_b': [150, 250]})\n",
    "\n",
    "    # Return a list of tuples (df, fill_strategy, filename)\n",
    "    input_files = [\n",
    "        (df1, 'fill_zeros', 'file1'),\n",
    "        (df2, 'drop_records', 'file2')\n",
    "    ]\n",
    "\n",
    "    return input_files\n",
    "\n",
    "mock_input_dfs = mock_input_dfs()\n",
    "\n",
    "\n",
    "# def test_merge_and_fill_basic_functionality(mock_input_dfs):\n",
    "\"\"\"\n",
    "Test the basic functionality of merge_and_fill_training_data by ensuring that DataFrames\n",
    "are merged correctly and the appropriate fill strategies are applied.\n",
    "\"\"\"\n",
    "input_dfs = mock_input_dfs\n",
    "\n",
    "# Call the function under test\n",
    "merged_df, merge_logs_df = fe.merge_and_fill_training_data(input_dfs)\n",
    "\n",
    "# Check that the merged DataFrame has the correct coin_ids\n",
    "expected_coin_ids = {2, 3}  # coin_id 1 from df1 is dropped, coin_id 2 and coin_id 3 remain\n",
    "assert set(merged_df['coin_id'].unique()) == expected_coin_ids, f\"Expected coin_ids: {expected_coin_ids}, but got: {set(merged_df['coin_id'].unique())}\"\n",
    "\n",
    "# Check the merge logs for correct filled and dropped counts\n",
    "assert len(merge_logs_df) == 2  # Two DataFrames should be logged\n",
    "assert merge_logs_df.iloc[0]['filled_count'] == 1  # One coin_id (3) should be filled from df1\n",
    "assert merge_logs_df.iloc[1]['dropped_count'] == 1  # One coin_id (1) should be dropped from df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mock DataFrames\n",
    "df1 = pd.DataFrame({'coin_id': [1, 2, 3], 'metric_1': [10, 20, 30]})\n",
    "df2 = pd.DataFrame({'coin_id': [2, 3], 'metric_2': [200, 300]})\n",
    "\n",
    "# List of input DataFrames with fill strategies\n",
    "df_list = [(df1, 'fill_zeros', 'df1'), (df2, 'fill_zeros', 'df2')]\n",
    "\n",
    "# Call the function\n",
    "merged_df, _ = fe.merge_and_fill_training_data(df_list)\n",
    "\n",
    "# Define the expected output\n",
    "expected_df = pd.DataFrame({\n",
    "    'coin_id': [1, 2, 3],\n",
    "    'metric_1': [10, 20, 30],\n",
    "    'metric_2': [0, 200, 300]  # Coin_id 1 should have metric_2 filled with 0\n",
    "})\n",
    "\n",
    "# Compare DataFrames\n",
    "pd.testing.assert_frame_equal(merged_df, expected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mock DataFrames\n",
    "df1 = pd.DataFrame({'coin_id': [1, 2, 3], 'metric_1': [10, 20, 30]})\n",
    "df2 = pd.DataFrame({'coin_id': [2, 3], 'metric_2': [200, 300]})\n",
    "\n",
    "# List of input DataFrames with fill strategies\n",
    "df_list = [(df1, 'fill_zeros', 'df1'), (df2, 'drop_records', 'df2')]\n",
    "\n",
    "# Call the function\n",
    "merged_df, merge_logs = fe.merge_and_fill_training_data(df_list)\n",
    "\n",
    "# Define the expected output\n",
    "expected_df = pd.DataFrame({\n",
    "    'coin_id': [2, 3],\n",
    "    'metric_1': [20, 30],\n",
    "    'metric_2': [200, 300]  # Coin_id 1 should be dropped\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mock DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'coin_id': [1, 2, 3],\n",
    "    'metric_1': [10, 20, 30]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'coin_id': [2, 3],\n",
    "    'metric_2': [200, 300]\n",
    "})\n",
    "\n",
    "# Expected output when drop_records is applied: rows for coin 1 should be dropped\n",
    "expected_df = pd.DataFrame({\n",
    "    'coin_id': [2, 3],\n",
    "    'metric_1': [20, 30],\n",
    "    'metric_2': [200, 300]\n",
    "})\n",
    "\n",
    "# Run the function\n",
    "merged_df, logs_df = fe.merge_and_fill_training_data([\n",
    "    (df1, 'drop_records', 'df1'),\n",
    "    (df2, 'drop_records', 'df2')\n",
    "])\n",
    "\n",
    "# Assert the merged DataFrame is correct\n",
    "assert np.array_equal(merged_df.values, expected_df.values), \"Merged DataFrame values do not match the expected DataFrame.\"\n",
    "\n",
    "\n",
    "# Assert the logs are correct\n",
    "# df1 should have no filled rows, and df2 should also have no filled rows (since we used drop_records)\n",
    "expected_logs = pd.DataFrame({\n",
    "    'file': ['df1', 'df2'],\n",
    "    'original_count': [3, 2],\n",
    "    'filled_count': [0, 0]\n",
    "})\n",
    "\n",
    "pd.testing.assert_frame_equal(logs_df.reset_index(drop=True), expected_logs.reset_index(drop=True))\n",
    "\n",
    "\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(merged_df.values,expected_df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(merged_df, expected_df, check_dtype=False, , check_like=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
