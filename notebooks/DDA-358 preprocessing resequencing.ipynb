{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.model_input_flows as mif\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [dr, pri, fg, flt, prp, cwm, ind, m, ia, mif, u]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All windows data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Retrieve all windows datasets\n",
    "macro_trends_df, market_data_df, profits_df, prices_df = mif.retrieve_all_windows_training_data(config,\n",
    "                                                                                            metrics_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "\n",
    "all_flattened_dfs = []\n",
    "all_flattened_filepaths = []\n",
    "\n",
    "for n, time_window in enumerate(time_windows):\n",
    "\n",
    "    # Prepare time window config files\n",
    "    window_config, window_metrics_config, window_modeling_config = (\n",
    "        mif.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "    # Generate flattened feature dfs for all datasets for the window\n",
    "    window_flattened_dfs, window_flattened_filepaths = mif.generate_window_flattened_dfs(\n",
    "        market_data_df,\n",
    "        macro_trends_df,\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        window_config,\n",
    "        window_metrics_config,\n",
    "        window_modeling_config\n",
    "    )\n",
    "\n",
    "    # Store window's flattened features\n",
    "    all_flattened_dfs.extend(window_flattened_dfs)\n",
    "    all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n",
    "all_flattened_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flattened_filepaths[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Identify filepath of parent directory\n",
    "parent_directory = os.path.join(modeling_config['modeling']['modeling_folder'],\n",
    "                               'outputs/preprocessed_outputs/')\n",
    "\n",
    "# for filepath in all_flattened_filepaths:\n",
    "filepath = all_flattened_filepaths[2]\n",
    "\n",
    "dataset = prp.extract_dataset_key_from_filepath(filepath, parent_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flattened_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = all_flattened_filepaths[0]\n",
    "print(filepath)\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold DataFrames\n",
    "df_list = []\n",
    "missing_files = []\n",
    "\n",
    "# Regex to extract the date pattern %Y-%m-%d_%H-%M from the filename\n",
    "date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}')\n",
    "\n",
    "# Loop through the input_file_tuples (filename, fill_strategy)\n",
    "for filepath in all_flattened_filepaths:\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Check if coin_id column exists if the fill strategy requires it\n",
    "        if fill_strategy in ['drop_records','fill_zeros']:\n",
    "            if 'coin_id' not in df.columns:\n",
    "                raise ValueError(f\"coin_id column is missing in {filename}\")\n",
    "\n",
    "            # Raise an error if there are duplicate coin_ids in the file\n",
    "            if df['coin_id'].duplicated().any():\n",
    "                raise ValueError(f\"Duplicate coin_ids found in file: {filename}\")\n",
    "\n",
    "        # Extract the date string from the filename\n",
    "        match = date_pattern.search(filename)\n",
    "        if not match:\n",
    "            raise ValueError(f\"No valid date string found in the filename: {filename}\")\n",
    "        date_string = match.group()  # e.g., '2024-09-13_14-44'\n",
    "        metric_string = filename.split(date_string)[0].rstrip('_')\n",
    "\n",
    "        # Add column suffixes based on the count of metric_string\n",
    "        if metric_string_count[metric_string] > 1:\n",
    "            suffix = f\"{metric_string}_{date_string}\"\n",
    "        else:\n",
    "            suffix = metric_string\n",
    "\n",
    "        # Check if this suffix has been used before and append a numerical suffix if necessary\n",
    "        for column in df.columns:\n",
    "            if column != 'coin_id':\n",
    "                column_with_suffix = f\"{column}_{suffix}\"\n",
    "\n",
    "                # Check if suffix exists in the count, increment if necessary\n",
    "                if column_with_suffix in column_suffix_count:\n",
    "                    column_suffix_count[column_with_suffix] += 1\n",
    "                    suffix_count = column_suffix_count[column_with_suffix]\n",
    "                    column_with_suffix = f\"{column_with_suffix}_{suffix_count}\"\n",
    "                else:\n",
    "                    column_suffix_count[column_with_suffix] = 1\n",
    "\n",
    "                df = df.rename(columns={column: column_with_suffix})\n",
    "\n",
    "        # Append DataFrame and fill strategy to the list for processing\n",
    "        df_list.append((df, fill_strategy, filename))\n",
    "    else:\n",
    "        missing_files.append(filename)\n",
    "\n",
    "# Merge the output DataFrames based on their fill strategies\n",
    "training_data_df, merge_logs_df = merge_and_fill_training_data(df_list)\n",
    "\n",
    "# Log the results\n",
    "logger.debug(\"%d files were successfully merged into training_data_df.\", len(df_list))\n",
    "if missing_files:\n",
    "    logger.warning(\"%d files could not be found: %s\",\n",
    "                    len(missing_files), ', '.join(missing_files))\n",
    "else:\n",
    "    logger.debug(\"All specified files were found and merged into training_data_df.\")\n",
    "\n",
    "return training_data_df, merge_logs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_and_fill_training_data(\n",
    "    df_list: list[tuple[pd.DataFrame, str, str]]\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Merges a list of DataFrames on 'coin_id' and applies specified fill strategies for missing\n",
    "    values. Generates a log of the merging process with details on record counts and modifications.\n",
    "\n",
    "    Params:\n",
    "    - df_list (list of tuples): Each tuple contains:\n",
    "        - df (pd.DataFrame): A DataFrame to merge.\n",
    "        - fill_strategy (str): The strategy to handle missing values ('fill_zeros', 'drop_records').\n",
    "        - filename (str): The name of the input file, used for logging.\n",
    "\n",
    "    Returns:\n",
    "    - training_data_df (pd.DataFrame): Merged DataFrame with applied fill strategies.\n",
    "    - merge_logs_df (pd.DataFrame): Log DataFrame detailing record counts for each input DataFrame.\n",
    "    \"\"\"\n",
    "    if not df_list:\n",
    "        raise ValueError(\"No DataFrames to merge.\")\n",
    "\n",
    "    # Initialize the log DataFrame\n",
    "    merge_logs = []\n",
    "\n",
    "    # Pull a unique set of all coin_ids across all DataFrames\n",
    "    all_coin_ids = set()\n",
    "    for df, _, _ in df_list:\n",
    "        if 'coin_id' in df.columns:\n",
    "            all_coin_ids.update(df['coin_id'].unique())\n",
    "\n",
    "    # Makes a new df with all coin_ids in a column\n",
    "    training_data_df = pd.DataFrame(all_coin_ids, columns=['coin_id'])\n",
    "\n",
    "    # Iterate through df_list and merge each one\n",
    "    for df, fill_strategy, filename in df_list:\n",
    "\n",
    "        # if the df is a macro_series without a coin_id, cross join it to all coin_ids\n",
    "        if fill_strategy == 'extend':\n",
    "            original_coin_ids = set()\n",
    "            training_data_df = training_data_df.merge(df, how='cross')\n",
    "\n",
    "        else:\n",
    "            # Merge with the full coin_id set (outer join)\n",
    "            original_coin_ids = set(df['coin_id'].unique())  # Track original coin_ids\n",
    "            training_data_df = pd.merge(training_data_df, df, on='coin_id', how='outer')\n",
    "\n",
    "            # Apply the fill strategy\n",
    "            if fill_strategy == 'fill_zeros':\n",
    "                # Fill missing values with 0\n",
    "                training_data_df.fillna(0, inplace=True)\n",
    "            elif fill_strategy == 'drop_records':\n",
    "                # Drop rows with missing values for this DataFrame's columns\n",
    "                training_data_df.dropna(inplace=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid fill strategy '{fill_strategy}' found in config.yaml.\")\n",
    "\n",
    "        # Calculate log details\n",
    "        final_coin_ids = set(training_data_df['coin_id'].unique())\n",
    "        filled_ids = final_coin_ids - original_coin_ids  # Present in final, missing in original\n",
    "\n",
    "        # Add log information for this DataFrame\n",
    "        merge_logs.append({\n",
    "            'file': filename,  # Use filename for logging\n",
    "            'original_count': len(original_coin_ids),\n",
    "            'filled_count': len(filled_ids)\n",
    "        })\n",
    "\n",
    "    # Ensure no duplicate columns after merging\n",
    "    if training_data_df.columns.duplicated().any():\n",
    "        raise ValueError(\"Duplicate columns found after merging.\")\n",
    "\n",
    "    # Raise an error if there are any null values in the final DataFrame\n",
    "    if training_data_df.isnull().any().any():\n",
    "        raise ValueError(\"Null values detected in the final merged DataFrame.\")\n",
    "\n",
    "    # Convert logs to a DataFrame\n",
    "    merge_logs_df = pd.DataFrame(merge_logs)\n",
    "\n",
    "    return training_data_df, merge_logs_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window-specific metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Prepare time window config files\n",
    "config, metrics_config, modeling_config = mif.prepare_configs(modeling_config['modeling']['config_folder'], time_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flattened_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_flattened_dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flattened_filepaths.extend([flattened_market_data_filepath])\n",
    "\n",
    "all_flattened_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "def generate_wallet_cohort_features(\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        config,\n",
    "        metrics_config,\n",
    "        modeling_config\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generates a window-specific feature set from the full all windows dataset. The window-specific\n",
    "    features are saved as a csv and returned, along with the csv filepath.\n",
    "\n",
    "    This function differs from the time_series set because it only flattens on date, since this\n",
    "    dataset doesn't have coin_id.\n",
    "\n",
    "    Params:\n",
    "    - all_windows_time_series_df (DataFrame): df containing all metrics and indicators for a time\n",
    "        series dataset.\n",
    "    - config: config.yaml that has the dates for the specific time window\n",
    "    - metrics_config: metrics_config.yaml\n",
    "    - modeling_config: modeling_config.yaml\n",
    "\n",
    "    Returns:\n",
    "    - flattened_metrics_df (DataFrame): the flattened version of the original df, with columns for\n",
    "        the configured aggregations and rolling metrics for all value columns and indicators.\n",
    "    - flattened_metrics_filepath (string): the filepath to where the flattened_metrics_df is saved\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Impute all required dates\n",
    "    # ----------------------------\n",
    "    # Identify all required imputation dates\n",
    "    imputation_dates = pri.identify_imputation_dates(config)\n",
    "\n",
    "    # Impute all required dates\n",
    "    window_profits_df = pri.impute_profits_for_multiple_dates(profits_df, prices_df, imputation_dates, n_threads=24)\n",
    "    window_profits_df = (window_profits_df[(window_profits_df['date'] >= pd.to_datetime(min(imputation_dates))) &\n",
    "                                        (window_profits_df['date'] <= pd.to_datetime(max(imputation_dates)))])\n",
    "\n",
    "\n",
    "    # 2. Generate metrics and indicators for all cohorts\n",
    "    # --------------------------------------------------\n",
    "    for cohort_name in metrics_config['wallet_cohorts']:\n",
    "\n",
    "        # load configs\n",
    "        dataset_metrics_config = metrics_config['wallet_cohorts'][cohort_name]\n",
    "        dataset_config = config['datasets']['wallet_cohorts'][cohort_name]\n",
    "\n",
    "        # identify wallets in the cohort based on the full lookback period\n",
    "        cohort_summary_df = cwm.classify_wallet_cohort(window_profits_df, dataset_config, cohort_name)\n",
    "        cohort_wallets = cohort_summary_df[cohort_summary_df['in_cohort']]['wallet_address']\n",
    "\n",
    "        # If no cohort members were identified, continue\n",
    "        if len(cohort_wallets) == 0:\n",
    "            logger.info(\"No wallets identified as members of cohort '%s'\", cohort_name)\n",
    "            continue\n",
    "\n",
    "        # Generate cohort buysell_metrics\n",
    "        cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,\n",
    "                                                            config['training_data']['training_period_end'],\n",
    "                                                            cohort_wallets)\n",
    "\n",
    "        # Generate cohort indicator metrics\n",
    "        cohort_metrics_df = ind.generate_time_series_indicators(cohort_metrics_df,\n",
    "                                                                metrics_config['wallet_cohorts'][cohort_name],\n",
    "                                                                'coin_id')\n",
    "\n",
    "        # Flatten cohort metrics\n",
    "        flattened_cohort_df, flattened_cohort_filepath = fe.generate_window_time_series_features(\n",
    "            cohort_metrics_df,\n",
    "            config,\n",
    "            dataset_metrics_config,\n",
    "            modeling_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the metrics DataFrame to be keyed only on coin_id\n",
    "flattened_metrics_df = fe.flatten_coin_date_df(\n",
    "    cohort_metrics_df,\n",
    "    metrics_config['wallet_cohorts'][cohort_name],\n",
    "    config['training_data']['training_period_end']  # Ensure data is up to training period end\n",
    ")\n",
    "flattened_metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time window modeling period start\n",
    "flattened_metrics_df.loc[:,'time_window'] = config['training_data']['modeling_period_start']\n",
    "\n",
    "# Save the flattened output and retrieve the file path\n",
    "_, flattened_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_metrics_df,\n",
    "    os.path.join(\n",
    "        modeling_config['modeling']['modeling_folder'],  # Folder to store flattened outputs\n",
    "        'outputs/flattened_outputs'\n",
    "    ),\n",
    "    'market_data',  # Descriptive metadata for the dataset\n",
    "    config['training_data']['modeling_period_start']  # Ensure data starts from modeling period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cohort buysell_metrics\n",
    "cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,\n",
    "                                                    config['training_data']['training_period_end'],\n",
    "                                                    cohort_wallets)\n",
    "\n",
    "\n",
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate cohort indicator metrics\n",
    "cohort_metrics_df = ind.generate_time_series_indicators(cohort_metrics_df,\n",
    "                                                        metrics_config['wallet_cohorts'][cohort_name],\n",
    "                                                        'coin_id')\n",
    "\n",
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = cohort_metrics_df\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['training_period_end']\n",
    "id_column='coin_id'\n",
    "drop_outside_date_range=True\n",
    "\n",
    "# Convert params to datetime\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "\n",
    "# Create copy of df\n",
    "time_series_df = time_series_df.copy()\n",
    "\n",
    "# Drop all rows with any NaN values\n",
    "time_series_df = time_series_df.dropna()\n",
    "\n",
    "# Define a function to check if a date range has full coverage\n",
    "def has_full_coverage(min_date, max_date):\n",
    "    return (min_date <= start_date) and (max_date >= end_date)\n",
    "\n",
    "if id_column:\n",
    "    # Multi-series data\n",
    "    series_data_range = time_series_df.groupby(id_column, observed=True)['date'].agg(['min', 'max'])\n",
    "    full_duration_series = series_data_range[series_data_range.apply(lambda x: has_full_coverage(x['min'], x['max']), axis=1)].index\n",
    "else:\n",
    "    # Single-series data\n",
    "    series_data_range = time_series_df['date'].agg(['min', 'max'])\n",
    "    full_duration_series = [0] if has_full_coverage(series_data_range['min'], series_data_range['max']) else []\n",
    "\n",
    "# Calculate coverage statistics\n",
    "full_coverage_count = len(full_duration_series)\n",
    "\n",
    "# # Split the dataframe\n",
    "# if id_column:\n",
    "#     # Convert id column to categorical to reduce memory usage\n",
    "#     time_series_df[id_column] = time_series_df[id_column].astype('category')\n",
    "#     full_coverage_df = time_series_df[time_series_df[id_column].isin(full_duration_series)]\n",
    "#     partial_coverage_df = time_series_df[~time_series_df[id_column].isin(full_duration_series)]\n",
    "# else:\n",
    "#     full_coverage_df = time_series_df if full_coverage_count else pd.DataFrame(columns=time_series_df.columns)\n",
    "#     partial_coverage_df = time_series_df if not full_coverage_count else pd.DataFrame(columns=time_series_df.columns)\n",
    "\n",
    "# logger.info(\"Split df with dimensions %s into %s full coverage records and %s partial coverage records.\",\n",
    "#             time_series_df.shape,\n",
    "#             len(full_coverage_df),\n",
    "#             len(partial_coverage_df))\n",
    "\n",
    "# if drop_outside_date_range:\n",
    "#     # Remove rows outside the date range for both dataframes\n",
    "#     full_coverage_df = (full_coverage_df[(full_coverage_df['date'] >= start_date) &\n",
    "#                                             (full_coverage_df['date'] <= end_date)])\n",
    "#     partial_coverage_df = (partial_coverage_df[(partial_coverage_df['date'] >= start_date) &\n",
    "#                                                 (partial_coverage_df['date'] <= end_date)])\n",
    "\n",
    "#     # Log the number of remaining records\n",
    "#     total_remaining = len(full_coverage_df) + len(partial_coverage_df)\n",
    "#     logger.info(\"After removing records outside the date range, %s records remain.\",\n",
    "#                 total_remaining)\n",
    "\n",
    "# # return full_coverage_df, partial_coverage_df\n",
    "\n",
    "time_series_df.shape\n",
    "full_coverage_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data_range = time_series_df.groupby(id_column, observed=True)['date'].agg(['min', 'max'])\n",
    "series_data_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cohort_metrics_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If no cohort members were identified, continue\n",
    "if len(cohort_wallets) == 0:\n",
    "    logger.info(\"No wallets identified as members of cohort '%s'\", cohort_name)\n",
    "    continue\n",
    "\n",
    "# Generate cohort buysell_metrics\n",
    "cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,\n",
    "                                                    config['training_data']['training_period_end'],\n",
    "                                                    cohort_wallets)\n",
    "\n",
    "# Generate cohort indicator metrics\n",
    "cohort_metrics_df = ind.generate_time_series_indicators(cohort_metrics_df,\n",
    "                                                        metrics_config['wallet_cohorts'][cohort_name],\n",
    "                                                        'coin_id')\n",
    "\n",
    "# Split df to only training_period\n",
    "cohort_metrics_df,_ = cwm.split_dataframe_by_coverage(cohort_metrics_df,\n",
    "                                                    config['training_data']['training_period_start'],\n",
    "                                                    config['training_data']['modeling_period_end'],\n",
    "                                                    id_column='coin_id',\n",
    "                                                    drop_outside_date_range=True)\n",
    "\n",
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Add time window modeling period start\n",
    "# cohort_metrics_df.loc[:,'time_window'] = config['training_data']['modeling_period_start']\n",
    "\n",
    "cohort_metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market data: generate window-specific flattened metrics\n",
    "flattened_market_data_df, flattened_market_data_filepath = fe.generate_window_time_series_features(\n",
    "    market_data_df,\n",
    "    config,\n",
    "    metrics_config,\n",
    "    modeling_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metrics_config['total_bought']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config['wallet_cohorts']['whales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.df_mem(profits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IN WINDOW FUNCTIONS\n",
    "\n",
    "market_data_df: just filter to window\n",
    "macro_trends_df: just filter to window\n",
    "\n",
    "profits_df\n",
    "1. identify all dates needed\n",
    "    all cohort lookback window starts\n",
    "    training_period_start\n",
    "    training_period_end\n",
    "    modeling_period_start\n",
    "    modeling_period_end\n",
    "2. impute them\n",
    "3. filter df to only dates between earliest and latest dates\n",
    "\n",
    "\n",
    "4. wallet cohorts and buysell metrics\n",
    "5. indicators\n",
    "6. filter to window\n",
    "\"\"\"\n",
    "\n",
    "# def build_time_window_model_input(n, window, config, metrics_config, modeling_config):\n",
    "#     \"\"\"\n",
    "#     Generates training data for each of the config.training_data.additional_windows.\n",
    "\n",
    "#     Params:\n",
    "#         n (int): The lookback number of the time window (e.g 0,1,2)\n",
    "#         window (Dict): The config override dict with the window's modeling_period_start\n",
    "#         config: config.yaml\n",
    "#         metrics_config: metrics_config.yaml\n",
    "#         modeling_config: modeling_config.yaml\n",
    "\n",
    "#     Returns:\n",
    "#         model_data (Dict): Dictionary containing all of the modeling features and variables:\n",
    "#             X_train, X_test (DataFrame): Model training features\n",
    "#             y_train, y_test (pd.Series): Model target variables\n",
    "#             returns_test (DataFrame): The actual returns of each coin_id in each time_window.\n",
    "#                 - coin_id: Index (str)\n",
    "#                 - time_window: Index (int)\n",
    "#                 - returns: value column (float)\n",
    "#     \"\"\"\n",
    "\n",
    "# Prepare the full configuration by applying overrides from the current trial config\n",
    "config, metrics_config, modeling_config = prepare_configs(modeling_config['modeling']['config_folder'], window)\n",
    "\n",
    "# Define window start and end dates\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Rebuild market data\n",
    "market_data_df = dr.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# Retrieve macro trends data\n",
    "macro_trends_df = dr.retrieve_macro_trends_data()\n",
    "macro_trends_df = cwm.generate_macro_trends_features(macro_trends_df, config)\n",
    "\n",
    "# Rebuild profits_df\n",
    "if 'profits_df' not in locals():\n",
    "    profits_df = None\n",
    "profits_df = rebuild_profits_df_if_necessary(config, prices_df, profits_df)\n",
    "\n",
    "# Build the configured model input data for the nth window\n",
    "X_train, X_test, y_train, y_test, returns_test = build_configured_model_input(\n",
    "                                    profits_df,\n",
    "                                    market_data_df,\n",
    "                                    macro_trends_df,\n",
    "                                    config,\n",
    "                                    metrics_config,\n",
    "                                    modeling_config)\n",
    "\n",
    "# Add time window indices to dfs with coin_ids\n",
    "X_train['time_window'] = n\n",
    "X_train.set_index('time_window', append=True, inplace=True)\n",
    "X_test['time_window'] = n\n",
    "X_test.set_index('time_window', append=True, inplace=True)\n",
    "returns_test['time_window'] = n\n",
    "returns_test.set_index('time_window', append=True, inplace=True)\n",
    "\n",
    "model_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'returns_test': returns_test\n",
    "}\n",
    "\n",
    "# return model_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "\n",
    "# Initialize empty lists to hold concatenated data\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "returns_test_list = []\n",
    "\n",
    "for n, window in enumerate(time_windows):\n",
    "\n",
    "    model_data = mif.build_time_window_model_input(n, window, config, metrics_config, modeling_config)\n",
    "\n",
    "    # Append the current window's data to the lists\n",
    "    X_train_list.append(model_data['X_train'])\n",
    "    X_test_list.append(model_data['X_test'])\n",
    "    y_train_list.append(model_data['y_train'])\n",
    "    y_test_list.append(model_data['y_test'])\n",
    "    returns_test_list.append(model_data['returns_test'])\n",
    "\n",
    "\n",
    "# Concatenate all the data for each part\n",
    "X_train = pd.concat(X_train_list, axis=0)\n",
    "X_test = pd.concat(X_test_list, axis=0)\n",
    "y_train = pd.concat(y_train_list, axis=0)\n",
    "y_test = pd.concat(y_test_list, axis=0)\n",
    "returns_test = pd.concat(returns_test_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict, y_pred, y_pred_prob = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns  # Feature names\n",
    "\n",
    "# Create a DataFrame with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob or y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Window Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking profits_df downcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = config['training_data']['earliest_window_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "minimum_wallet_inflows = config['data_cleaning']['minimum_wallet_inflows']\n",
    "\n",
    "# SQL query to retrieve profits data\n",
    "query_sql = f\"\"\"\n",
    "    -- STEP 1: retrieve profits data and apply USD inflows filter\n",
    "    -------------------------------------------------------------\n",
    "    with profits_base as (\n",
    "        select coin_id\n",
    "        ,date\n",
    "        ,wallet_address\n",
    "        ,profits_cumulative\n",
    "        ,usd_balance\n",
    "        ,usd_net_transfers\n",
    "        ,usd_inflows\n",
    "        ,usd_inflows_cumulative\n",
    "        from core.coin_wallet_profits\n",
    "        where date <= '{end_date}'\n",
    "    ),\n",
    "\n",
    "    usd_inflows_filter as (\n",
    "        select coin_id\n",
    "        ,wallet_address\n",
    "        ,max(usd_inflows_cumulative) as total_usd_inflows\n",
    "        from profits_base\n",
    "        -- we don't need to include coin-wallet pairs that have no transactions between\n",
    "        -- the start and end dates\n",
    "        group by 1,2\n",
    "    ),\n",
    "\n",
    "    profits_base_filtered as (\n",
    "        select pb.*\n",
    "        from profits_base pb\n",
    "        join usd_inflows_filter f on f.coin_id = pb.coin_id\n",
    "            and f.wallet_address = pb.wallet_address\n",
    "        where f.total_usd_inflows >= {minimum_wallet_inflows}\n",
    "    ),\n",
    "\n",
    "\n",
    "    -- STEP 2: create new records for all coin-wallet pairs as of the training_period_start\n",
    "    ---------------------------------------------------------------------------------------\n",
    "    -- compute the starting profits and balances as of the training_period_start\n",
    "    training_start_existing_rows as (\n",
    "        -- identify coin-wallet pairs that already have a balance as of the period end\n",
    "        select *\n",
    "        from profits_base_filtered\n",
    "        where date = '{start_date}'\n",
    "    ),\n",
    "    training_start_needs_rows as (\n",
    "        -- for coin-wallet pairs that don't have existing records, identify the row closest to the period end date\n",
    "        select t.*\n",
    "        ,cmd_previous.price as price_previous\n",
    "        ,cmd_training.price as price_current\n",
    "        ,row_number() over (partition by t.coin_id,t.wallet_address order by t.date desc) as rn\n",
    "        from profits_base_filtered t\n",
    "        left join training_start_existing_rows e on e.coin_id = t.coin_id\n",
    "            and e.wallet_address = t.wallet_address\n",
    "\n",
    "        -- obtain the last price used to compute the balance and profits data\n",
    "        join core.coin_market_data cmd_previous on cmd_previous.coin_id = t.coin_id and cmd_previous.date = t.date\n",
    "\n",
    "        -- obtain the training_period_start price so we can update the calculations\n",
    "        join core.coin_market_data cmd_training on cmd_training.coin_id = t.coin_id and cmd_training.date = '{start_date}'\n",
    "        where t.date < '{start_date}'\n",
    "        and e.coin_id is null\n",
    "    ),\n",
    "    training_start_new_rows as (\n",
    "        -- create a new row for the period end date by carrying the balance from the closest existing record\n",
    "        select t.coin_id\n",
    "        ,cast('{start_date}' as datetime) as date\n",
    "        ,t.wallet_address\n",
    "        -- profits_cumulative is the previous profits_cumulative + the change in profits up to the start_date\n",
    "        ,((t.price_current / t.price_previous) - 1) * t.usd_balance + t.profits_cumulative as profits_cumulative\n",
    "        -- usd_balance is previous balance * (1 + % change in price)\n",
    "        ,(t.price_current / t.price_previous) * t.usd_balance as usd_balance\n",
    "        -- there were no transfers\n",
    "        ,0 as usd_net_transfers\n",
    "        -- there were no inflows\n",
    "        ,0 as usd_inflows\n",
    "        -- no change since there were no inflows\n",
    "        ,usd_inflows_cumulative as usd_inflows_cumulative\n",
    "\n",
    "        from training_start_needs_rows t\n",
    "        where rn=1\n",
    "\n",
    "    ),\n",
    "\n",
    "    -- STEP 3: merge all records together\n",
    "    -------------------------------------\n",
    "    profits_merged as (\n",
    "        select * from profits_base_filtered\n",
    "        -- transfers prior to the training period are summarized in training_start_new_rows\n",
    "        where date >= '{start_date}'\n",
    "\n",
    "        union all\n",
    "\n",
    "        select * from training_start_new_rows\n",
    "    )\n",
    "\n",
    "    select coin_id\n",
    "    ,date\n",
    "\n",
    "    -- replace the memory-intensive address strings with integers\n",
    "    ,DENSE_RANK() OVER (ORDER BY wallet_address) as wallet_address\n",
    "\n",
    "    ,profits_cumulative\n",
    "    ,usd_balance\n",
    "    ,usd_net_transfers\n",
    "    ,usd_inflows\n",
    "    -- set a floor of $0.01 to avoid divide by 0 errors caused by rounding\n",
    "    ,greatest(0.01,usd_inflows_cumulative) as usd_inflows_cumulative\n",
    "    from profits_merged\n",
    "\"\"\"\n",
    "\n",
    "# Run the SQL query using dgc's run_sql method\n",
    "profits_df = dgc().run_sql(query_sql)\n",
    "\n",
    "logger.info('Converting columns to memory-optimized formats...')\n",
    "\n",
    "# Convert coin_id to categorical and date to date\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "profits_df['date'] = pd.to_datetime(profits_df['date'])\n",
    "\n",
    "# Add total_return column\n",
    "profits_df['total_return'] = (profits_df['profits_cumulative']\n",
    "                                / profits_df['usd_inflows_cumulative'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df = safe_downcast(profits_df, 'wallet_address', 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.can_cast(market_data_df['market_cap'].dtype, 'int32', casting='safe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df = safe_downcast(market_data_df, 'volume', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Data resequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "n = 0\n",
    "window = time_windows[n]\n",
    "\n",
    "# Prepare the full configuration by applying overrides from the current trial config\n",
    "config, metrics_config, modeling_config = mif.prepare_configs(modeling_config['modeling']['config_folder'], window)\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "n = 0\n",
    "window = time_windows[n]\n",
    "\n",
    "\n",
    "# market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "# prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "market_data_df_full = market_data_df.copy()\n",
    "market_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "market_data_df = market_data_df_full.copy()\n",
    "print(market_data_df.columns)\n",
    "market_data_df = ind.generate_time_series_indicators('market_data', market_data_df, metrics_config)\n",
    "print(market_data_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(time_series_df.index, pd.RangeIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "market_data_df = market_data_df_full.copy()\n",
    "value_column = 'price'\n",
    "value_column_indicators_config = metrics_config['time_series']['market_data'][value_column]['indicators']\n",
    "id_column = 'coin_id'\n",
    "market_data_df = ind.generate_column_time_series_indicators(\n",
    "    market_data_df,\n",
    "    value_column,\n",
    "    value_column_indicators_config,\n",
    "    id_column\n",
    ")\n",
    "\n",
    "market_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = None\n",
    "if not id_column:\n",
    "    print('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = time_series_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# time_series_df = market_data_df[['date','coin_id','price']].copy()\n",
    "time_series_df = market_data_df_full.copy()\n",
    "config = config\n",
    "value_column_indicators_config = metrics_config['time_series']['market_data']['price']['indicators']\n",
    "value_column = 'price'\n",
    "id_column='coin_id'\n",
    "\n",
    "time_series_df = time_series_df.set_index(['coin_id','date'])\n",
    "\n",
    "# Data Quality Checks and Formatting\n",
    "if value_column not in time_series_df.columns:\n",
    "    raise KeyError(f\"Input DataFrame does not include column '{value_column}'.\")\n",
    "\n",
    "if time_series_df[value_column].isnull().any():\n",
    "    raise ValueError(f\"The '{value_column}' column contains null values, which are not allowed.\")\n",
    "\n",
    "# Indicator Calculations\n",
    "# ----------------------\n",
    "# If there is an id_column, group on it\n",
    "if id_column:\n",
    "    groupby_column = id_column\n",
    "# If there isn't, create a dummy_column for grouping and remove it later\n",
    "else:\n",
    "    time_series_df['dummy_group'] = 1\n",
    "    groupby_column = 'dummy_group'\n",
    "\n",
    "# For each indicator, loop through all options and add the appropriate column\n",
    "for indicator, indicator_config in value_column_indicators_config.items():\n",
    "    if indicator == 'sma':\n",
    "        windows = indicator_config['parameters']['window']\n",
    "        for w in windows:\n",
    "            ind_series = time_series_df.groupby(level=groupby_column, observed=True)[value_column].transform(\n",
    "                lambda x: ind.calculate_sma(x, w))\n",
    "            time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    elif indicator == 'ema':\n",
    "        windows = indicator_config['parameters']['window']\n",
    "        for w in windows:\n",
    "            ind_series = time_series_df.groupby(level=groupby_column, observed=True)[value_column].transform(\n",
    "                lambda x: ind.calculate_ema(x, w))\n",
    "            time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    # elif indicator == 'rsi':\n",
    "    #     windows = indicator_config['parameters']['window']\n",
    "    #     for w in windows:\n",
    "    #         ind_series = time_series_df.groupby(level=groupby_column, observed=True)['price'].transform(\n",
    "    #             lambda x: calculate_rsi(x, w))\n",
    "    #         time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    # elif indicator == 'bollinger_bands_upper':\n",
    "    #     windows = indicator_config['parameters']['window']\n",
    "    #     num_std = indicator_config['parameters'].get('num_std', None)\n",
    "    #     for w in windows:\n",
    "    #         ind_series = time_series_df.groupby(level=groupby_column, observed=True)['price'].transform(\n",
    "    #             lambda x: calculate_bollinger_bands(x, 'upper', w, num_std))\n",
    "    #         time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    # elif indicator == 'bollinger_bands_lower':\n",
    "    #     windows = indicator_config['parameters']['window']\n",
    "    #     num_std = indicator_config['parameters'].get('num_std', None)\n",
    "    #     for w in windows:\n",
    "    #         ind_series = time_series_df.groupby(level=groupby_column, observed=True)['price'].transform(\n",
    "    #             lambda x: calculate_bollinger_bands(x, 'lower', w, num_std))\n",
    "    #         time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "# Remove the dummy column if it was created\n",
    "if groupby_column == 'dummy_group':\n",
    "    time_series_df = time_series_df.drop('dummy_group', axis=1)\n",
    "\n",
    "logger.info(\"Generated indicators for column '%s' :%s\",\n",
    "            value_column,\n",
    "            list(value_column_indicators_config.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(value_column_indicators_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"%s\",value_column_indicators_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.xs('9d6619f4-b44b-4ff4-9f68-1f563f57e060',level='coin_id').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = indicator_config['parameters'].get('num_std', None)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.groupby(level='coin_id', observed=True)['price'].transform("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indicators implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "df = market_data_df.copy()\n",
    "df = market_data_df.set_index(['coin_id','date'])\n",
    "\n",
    "\n",
    "# Add Relative Strength Index (RSI)\n",
    "df['rsi'] = df.groupby(level='coin_id', observed=True)['price'].transform(\n",
    "    lambda x: ind.calculate_rsi(x, 14))\n",
    "# Add Money Flow Index (MFI)\n",
    "df = ind.add_mfi_column(df)\n",
    "\n",
    "# Calculate MACD with EMAs\n",
    "df['ema_12'] = df.groupby(level='coin_id', observed=True)['price'].transform(lambda x: ind.calculate_ema(x, 12))\n",
    "df['ema_26'] = df.groupby(level='coin_id', observed=True)['price'].transform(lambda x: ind.calculate_ema(x, 26))\n",
    "df = ind.add_crossover_column(df, 'ema_12', 'ema_26', drop_col1=True, drop_col2=True)\n",
    "\n",
    "# Add Bollinger Bands\n",
    "df = ind.add_bollinger_bands(df, include_middle=False)\n",
    "# Add crossover for price and upper band\n",
    "df = ind.add_crossover_column(df, 'price', 'bollinger_band_upper', drop_col1=False, drop_col2=True)\n",
    "# Add crossover for price and lower band\n",
    "df = ind.add_crossover_column(df, 'price', 'bollinger_band_lower', drop_col1=False, drop_col2=True)\n",
    "\n",
    "# Calculate OBV\n",
    "df['obv_price_volume'] = ind.generalized_obv(df['price'],df['volume'])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Define a function to calculate MFI within each group, similar to the crossovers function\n",
    "def apply_mfi(group):\n",
    "    # Reset index to avoid issues with the multi-index during group operations\n",
    "    group = group.reset_index()\n",
    "    group['mfi'] = ind.calculate_mfi(group['price'], group['volume'])\n",
    "\n",
    "    # Set index back to the original multi-index\n",
    "    return group.set_index(['coin_id', 'date'])\n",
    "\n",
    "# Apply the function within each 'coin_id' group\n",
    "df = df.groupby('coin_id', observed=True, group_keys=False).apply(apply_mfi)\n",
    "\n",
    "# Display the updated DataFrame with the MFI column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "df2 = df[['ema_12','ema_26']].copy()\n",
    "\n",
    "df2 = ind.add_crossover_column(df2, 'ema_12', 'ema_26', drop_col1=True, drop_col2=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_crossovers(series1, series2):\n",
    "    \"\"\"\n",
    "    Identify crossovers between two time series.\n",
    "\n",
    "    This function calculates the points where series1 crosses over series2.\n",
    "    It handles NaN values by converting them to 0.\n",
    "\n",
    "    Parameters:\n",
    "    series1 (array-like): The first time series\n",
    "    series2 (array-like): The second time series\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of the same length as the input series, where:\n",
    "        0 indicates no crossover\n",
    "        1 indicates an upward crossover (series1 crosses above series2)\n",
    "        -1 indicates a downward crossover (series1 crosses below series2)\n",
    "    \"\"\"\n",
    "    diff = series1 - series2\n",
    "\n",
    "    # Handle NaN values\n",
    "    diff = np.nan_to_num(diff, nan=0.0)\n",
    "\n",
    "    # Initialize crossovers array\n",
    "    crossovers = np.zeros(len(series1))\n",
    "\n",
    "    # Identify crossovers\n",
    "    signs = np.sign(diff)\n",
    "    sign_changes = signs[1:] != signs[:-1]\n",
    "    crossover_indices = np.where(sign_changes)[0] + 1\n",
    "\n",
    "    # Assign 1 for upward crossovers, -1 for downward crossovers\n",
    "    crossovers[crossover_indices] = np.where(signs[crossover_indices] > 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ema_12','ema_26']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `df` is your DataFrame with multi-index (coin_id, date) and ema_12, ema_26 columns\n",
    "\n",
    "# Define a function that applies identify_crossovers to a group\n",
    "def apply_crossovers(group):\n",
    "    group['crossovers'] = identify_crossovers(group['ema_12'], group['ema_26'])\n",
    "    return group\n",
    "\n",
    "# Apply the function within each 'coin_id' group\n",
    "df = df.groupby('coin_id', group_keys=False).apply(apply_crossovers)\n",
    "\n",
    "# Display the resulting DataFrame with the new 'crossovers' column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame with the new 'crossovers' column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "\n",
    "def sample_data():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample DataFrame for testing.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with sample data for testing indicators.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'coin_id': ['BTC', 'BTC', 'BTC', 'ETH', 'ETH', 'ETH'],\n",
    "        'date': pd.date_range(start='2023-01-01', periods=6),\n",
    "        'price': [100, 110, 105, 200, 220, 210]\n",
    "    })\n",
    "sample_data=sample_data()\n",
    "\n",
    "def sample_config():\n",
    "    \"\"\"\n",
    "    Fixture to create a sample configuration for testing.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A configuration dictionary for testing all supported indicators.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'time_series': {\n",
    "            'market_data': {\n",
    "                'price': {\n",
    "                    'indicators': {\n",
    "                        'sma': {'parameters': {'window': [2]}},\n",
    "                        'ema': {'parameters': {'window': [2]}},\n",
    "                        'rsi': {'parameters': {'window': [2]}},\n",
    "                        'bollinger_bands_upper': {'parameters': {'window': [2], 'num_std': 2}},\n",
    "                        'bollinger_bands_lower': {'parameters': {'window': [2], 'num_std': 2}}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "sample_config=sample_config()\n",
    "# @pytest.mark.unit\n",
    "# def test_all_supported_indicators(sample_data, sample_config):\n",
    "\"\"\"\n",
    "Test that all supported indicators are correctly calculated and added to the DataFrame.\n",
    "\n",
    "This test checks the calculation of SMA, EMA, RSI, and Bollinger Bands for the given sample data.\n",
    "\"\"\"\n",
    "result = ind.generate_time_series_indicators('market_data', sample_data, sample_config)\n",
    "\n",
    "# Calculate expected values\n",
    "# SMA (2-day)\n",
    "# For BTC: [None, 105, 107.5]\n",
    "# For ETH: [None, 210, 215]\n",
    "\n",
    "# EMA (2-day)\n",
    "# For BTC: [None, 106.67, 105.56]\n",
    "# For ETH: [None, 213.33, 211.11]\n",
    "# EMA = (Current * (2 / (1 + 2))) + (Previous EMA * (1 - (2 / (1 + 2))))\n",
    "\n",
    "# RSI (2-day)\n",
    "# For BTC: [None, 100, 33.33]\n",
    "# For ETH: [None, 100, 33.33]\n",
    "# RSI = 100 - (100 / (1 + (Average Gain / Average Loss)))\n",
    "\n",
    "# Bollinger Bands (2-day, 2 std dev)\n",
    "# Upper Band = SMA + (2 * std dev)\n",
    "# Lower Band = SMA - (2 * std dev)\n",
    "# For BTC: [None, 115, 112.5], [None, 95, 102.5]\n",
    "# For ETH: [None, 230, 225], [None, 190, 205]\n",
    "\n",
    "expected_columns = [\n",
    "    'coin_id', 'date', 'price',\n",
    "    'price_sma_2', 'price_ema_2', 'price_rsi_2',\n",
    "    'price_bollinger_bands_upper_2', 'price_bollinger_bands_lower_2'\n",
    "]\n",
    "\n",
    "assert list(result.columns) == expected_columns\n",
    "\n",
    "# Check SMA values\n",
    "expected_sma = [np.nan, 105, 107.5, np.nan, 210, 215]\n",
    "assert all(np.isclose(a, b, equal_nan=True) for a, b in zip(result['price_sma_2'], expected_sma))\n",
    "\n",
    "# Check EMA values\n",
    "expected_ema = [np.nan, 106.67, 105.56, np.nan, 213.33, 211.11]\n",
    "assert all(np.isclose(a, b, equal_nan=True, rtol=1e-2) for a, b in zip(result['price_ema_2'], expected_ema))\n",
    "\n",
    "# Check RSI values\n",
    "expected_rsi = [np.nan, 1.0, 0.6667, np.nan, 1.0, 0.6667]\n",
    "assert all(np.isclose(a, b, equal_nan=True, rtol=1e-2) for a, b in zip(result['price_rsi_2'], expected_rsi))\n",
    "\n",
    "# Check Bollinger Bands values\n",
    "expected_bb_upper = [np.nan, 115, 112.5, np.nan, 230, 225]\n",
    "expected_bb_lower = [np.nan, 95, 102.5, np.nan, 190, 205]\n",
    "assert all(np.isclose(a, b, equal_nan=True) for a, b in zip(result['price_bollinger_bands_upper_2'], expected_bb_upper))\n",
    "assert all(np.isclose(a, b, equal_nan=True) for a, b in zip(result['price_bollinger_bands_lower_2'], expected_bb_lower))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['price_bollinger_bands_upper_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'price': [100, 110, 105, 200, 220, 210]\n",
    "})\n",
    "\n",
    "upper_band = ind.calculate_bollinger_bands(sample_data['price'], 'upper', window=2, num_std=2)\n",
    "print(upper_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['price_rsi_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result['price_rsi_2'], expected_rsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(result_df.loc[result_df['coin_id'] == 'coin1', 'price_sma_2'].values) == list(expected_sma_2_coin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(expected_sma_2_coin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rsi.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
