{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false\n",
    "# pyright: reportMissingModuleSource=false\n",
    "\n",
    "import uuid\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import pdb\n",
    "import datetime\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "import pytest\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import argrelextrema\n",
    "from dreams_core.googlecloud import GoogleCloud as dgc\n",
    "from dreams_core import core as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import progressbar\n",
    "\n",
    "# load dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Custom format function for displaying |numbers/\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.12g}')\n",
    "# pd.reset_option('display.float_format')\n",
    "\n",
    "# Dark mode charts\n",
    "plt.rcParams['figure.facecolor'] = '#181818'  # Custom background color (dark gray in this case)\n",
    "plt.rcParams['axes.facecolor'] = '#181818'\n",
    "plt.rcParams['text.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.labelcolor'] = '#afc6ba'\n",
    "plt.rcParams['xtick.color'] = '#afc6ba'\n",
    "plt.rcParams['ytick.color'] = '#afc6ba'\n",
    "plt.rcParams['axes.titlecolor'] = '#afc6ba'\n",
    "\n",
    "# import local modules\n",
    "# pyright: reportMissingImports=false\n",
    "sys.path.append('..//src')\n",
    "import utils as u\n",
    "import training_data.data_retrieval as dr\n",
    "import training_data.profits_row_imputation as pri\n",
    "import coin_wallet_metrics.coin_wallet_metrics as cwm\n",
    "import coin_wallet_metrics.indicators as ind\n",
    "import feature_engineering.feature_generation as fg\n",
    "import feature_engineering.time_windows_orchestration as tw\n",
    "import feature_engineering.flattening as flt\n",
    "import feature_engineering.data_splitting as ds\n",
    "import feature_engineering.target_variables as tv\n",
    "import feature_engineering.preprocessing as prp\n",
    "import modeling as m\n",
    "import insights.analysis as ia\n",
    "import insights.experiments as exp\n",
    "\n",
    "\n",
    "# reload all modules\n",
    "modules = [u, dr, pri, cwm, ind, fg, tw, flt, ds, tv, prp, m, ia, exp]\n",
    "[importlib.reload(module) for module in modules]\n",
    "\n",
    "# load all configs\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')\n",
    "\n",
    "# configure logger\n",
    "logger = dc.setup_logger()\n",
    "logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "sets_X_y_dict, returns_df, join_logs_df = tw.generate_all_time_windows_model_inputs(config,metrics_config,modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sets_X_y_dict['train_set'][0]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Retrieve base datasets used by all windows\n",
    "# ---------------------------------------------\n",
    "macro_trends_df, market_data_df, profits_df, prices_df = tw.prepare_all_windows_base_data(config,\n",
    "                                                                                            metrics_config)\n",
    "\n",
    "\n",
    "# 2. Generate flattened features for each dataset in each window\n",
    "# --------------------------------------------------------------\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = tw.generate_time_windows(config)\n",
    "\n",
    "all_flattened_dfs = []\n",
    "all_flattened_filepaths = []\n",
    "\n",
    "for _, time_window in enumerate(time_windows):\n",
    "\n",
    "    # Prepare time window config files\n",
    "    window_config, window_metrics_config, window_modeling_config = (\n",
    "        exp.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "    # Generate flattened feature dfs for all datasets for the window\n",
    "    window_flattened_dfs, window_flattened_filepaths = tw.generate_window_flattened_dfs(\n",
    "        market_data_df,\n",
    "        macro_trends_df,\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        window_config,\n",
    "        window_metrics_config,\n",
    "        window_modeling_config\n",
    "    )\n",
    "\n",
    "    # Store window's flattened features\n",
    "    all_flattened_dfs.extend(window_flattened_dfs)\n",
    "    all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# 3. Combine features from all datasets in all time windows with target variables\n",
    "# -------------------------------------------------------------------------------\n",
    "# Combine all time windows for each dataset, the join the datasets together\n",
    "concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n",
    "\n",
    "# Create target variables for all time windows\n",
    "target_variable_df, returns_df, = tw.create_target_variables_for_all_time_windows(training_data_df,\n",
    "                                                                                    prices_df,\n",
    "                                                                                    config,\n",
    "                                                                                    modeling_config)\n",
    "\n",
    "# Split target variables into the train/test/validation/future sets\n",
    "sets_X_y_dict = ds.perform_train_test_validation_future_splits(training_data_df,\n",
    "                                                                target_variable_df,\n",
    "                                                                modeling_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sets_X_y_dict['train_set'][0]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Load your DataFrames (replace this with your actual data loading code)\n",
    "datasets = {\n",
    "    'train': df\n",
    "}\n",
    "\n",
    "# Initialize and run the preprocessor\n",
    "preprocessor = prp.DataPreprocessor(config, metrics_config, modeling_config)\n",
    "preprocessed_datasets = preprocessor.preprocess(datasets)\n",
    "\n",
    "# Print results\n",
    "for dataset_name, df in preprocessed_datasets.items():\n",
    "    print(f\"Columns in {dataset_name} set: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Confirm there are no null values\n",
    "if df.isnull().values.any():\n",
    "    raise ValueError(\"Missing values detected in the DataFrame.\")\n",
    "\n",
    "# Convert all columns to numeric\n",
    "df = prp.preprocess_categorical_and_boolean(df)\n",
    "\n",
    "# Feature Selection\n",
    "# Drop features specified in modeling_config['drop_features']\n",
    "drop_features = modeling_config['preprocessing'].get('drop_features', [])\n",
    "if drop_features:\n",
    "    df = df.drop(columns=drop_features, errors='warn')\n",
    "\n",
    "# # Apply feature selection based on sameness_threshold and retain_columns from dataset_config\n",
    "# sameness_threshold = dataset_config.get('sameness_threshold', 1.0)\n",
    "# retain_columns = dataset_config.get('retain_columns', [])\n",
    "\n",
    "# # Drop columns with more than `sameness_threshold` of the same value, unless in retain_columns\n",
    "# for column in df.columns:\n",
    "#     if column not in retain_columns:\n",
    "#         max_value_ratio = df[column].value_counts(normalize=True).max()\n",
    "#         if max_value_ratio > sameness_threshold:\n",
    "#             df = df.drop(columns=[column])\n",
    "#             logger.debug(\"Dropped column %s due to sameness_threshold\", column)\n",
    "\n",
    "\n",
    "# # Step 4: Scaling and Transformation\n",
    "# # ----------------------------------------------------\n",
    "# # Apply scaling if df_metrics_config is provided\n",
    "# if df_metrics_config:\n",
    "#     df = apply_scaling(df, df_metrics_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "\n",
    "def calculate_sameness_percentage(column: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of the most common value in a column.\n",
    "\n",
    "    Parameters:\n",
    "    column (pd.Series): The column to analyze.\n",
    "\n",
    "    Returns:\n",
    "    float: The percentage (0 to 1) of the most common value in the column.\n",
    "    \"\"\"\n",
    "    return column.value_counts().iloc[0] / len(column)\n",
    "\n",
    "def create_prefix_mapping(config: Dict[str, Any]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Create a mapping of column prefixes to their config paths and sameness thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    config (Dict[str, Any]): The configuration dictionary containing dataset information.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Dict[str, float]]: A dictionary where keys are column prefixes and values are\n",
    "    dictionaries containing 'path' (str) and 'threshold' (float) for each prefix.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    for dataset_type, dataset_config in config['datasets'].items():\n",
    "        for category, category_config in dataset_config.items():\n",
    "            if isinstance(category_config, dict) and 'sameness_threshold' in category_config:\n",
    "                prefix = f\"{category}_\"\n",
    "                mapping[prefix] = {\n",
    "                    'path': f\"datasets.{dataset_type}.{category}\",\n",
    "                    'threshold': category_config['sameness_threshold']\n",
    "                }\n",
    "            elif isinstance(category_config, dict):\n",
    "                for subcategory, subcategory_config in category_config.items():\n",
    "                    if 'sameness_threshold' in subcategory_config:\n",
    "                        prefix = f\"{subcategory}_\"\n",
    "                        mapping[prefix] = {\n",
    "                            'path': f\"datasets.{dataset_type}.{category}.{subcategory}\",\n",
    "                            'threshold': subcategory_config['sameness_threshold']\n",
    "                        }\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def check_and_drop_columns(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check column sameness and drop columns exceeding the threshold.\n",
    "\n",
    "    This function analyzes each column in the DataFrame, calculates its sameness percentage,\n",
    "    and drops columns that exceed the threshold specified in the configuration.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to process.\n",
    "    config (Dict[str, Any]): The configuration dictionary containing sameness thresholds.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with columns dropped based on the sameness criteria.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If any columns can't be mapped to a sameness threshold or if any config keys\n",
    "                can't be mapped to columns.\n",
    "    \"\"\"\n",
    "    prefix_mapping = create_prefix_mapping(config)\n",
    "    columns_to_drop = []\n",
    "    unmapped_columns = []\n",
    "    used_config_keys = set()\n",
    "\n",
    "    for column in df.columns:\n",
    "        mapped = False\n",
    "        for prefix, config_info in prefix_mapping.items():\n",
    "            if column.startswith(prefix):\n",
    "                mapped = True\n",
    "                used_config_keys.add(prefix)\n",
    "                sameness = calculate_sameness_percentage(df[column])\n",
    "                if sameness > config_info['threshold']:\n",
    "                    columns_to_drop.append(column)\n",
    "                break\n",
    "        if not mapped:\n",
    "            unmapped_columns.append(column)\n",
    "\n",
    "    unused_config_keys = set(prefix_mapping.keys()) - used_config_keys\n",
    "\n",
    "    if unmapped_columns:\n",
    "        raise ValueError(f\"The following columns could not be mapped to a sameness threshold: {unmapped_columns}\")\n",
    "\n",
    "    if unused_config_keys:\n",
    "        raise ValueError(f\"The following config keys could not be mapped to columns: {unused_config_keys}\")\n",
    "\n",
    "    # Drop the columns\n",
    "    df.drop(columns=columns_to_drop)\n",
    "    logger.info(\"Dropped %s columns %s due to sameness thresholds.\", len(columns_to_drop), columns_to_drop)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = check_and_drop_columns(df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert categorical and boolean columns to integers\n",
    "# ---------------------------------------------------------------\n",
    "# Convert categorical columns to one-hot encoding (get_dummies)\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_columns = [col for col in categorical_columns]\n",
    "for col in categorical_columns:\n",
    "    num_categories = df[col].nunique()\n",
    "    if num_categories > 8:\n",
    "        logger.warning(\"Column '%s' has %s categories, consider reducing categories.\",\n",
    "                        col, num_categories)\n",
    "    df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "df = df.apply(lambda col: col.astype(int) if col.dtype == bool else col)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All windows datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "def generate_all_time_windows_model_inputs(config,metrics_config,modeling_config):\n",
    "    \"\"\"\n",
    "    Generates the X and y splits for all sets across all time windows.\n",
    "\n",
    "    Sequence:\n",
    "    1. Retrieve the base datasets that contain records across all windows\n",
    "    2. Loop through each time window and generate flattened features for the window\n",
    "    3a. Concat each dataset's window dfs, then join all the dataset dfs with the target variable to\n",
    "        create a comprehensive feature set keyed on coin_id.\n",
    "    3b. Split the full feature set into train/test/validation/future sets.\n",
    "\n",
    "    Params:\n",
    "    - config, metrics_config, modeling_config: loaded config yaml files\n",
    "\n",
    "    Returns:\n",
    "    - sets_X_y_dict (dict[pd.DataFrame, pd.Series]): Dict with keys for each set type (e.g. train_set,\n",
    "        future_set, etc) that contains the X and y data for the set.\n",
    "    - returns_df (pd.DataFrame): DataFrame with MultiIndex on time_window,coin_id that contains a\n",
    "        'returns' column showing actual returns during the each time_window's modeling period.\n",
    "    - join_logs_df (pd.DataFrame): DataFrame showing the outcomes of each dataset's join and fill\n",
    "        methods\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Retrieve base datasets used by all windows\n",
    "    # ---------------------------------------------\n",
    "    macro_trends_df, market_data_df, profits_df, prices_df = tw.prepare_all_windows_base_data(config,\n",
    "                                                                                              metrics_config)\n",
    "\n",
    "\n",
    "    # 2. Generate flattened features for each dataset in each window\n",
    "    # --------------------------------------------------------------\n",
    "    # Generate time_windows config overrides that will modify each window's config settings\n",
    "    time_windows = tw.generate_time_windows(config)\n",
    "\n",
    "    all_flattened_dfs = []\n",
    "    all_flattened_filepaths = []\n",
    "\n",
    "    for n, time_window in enumerate(time_windows):\n",
    "\n",
    "        # Prepare time window config files\n",
    "        window_config, window_metrics_config, window_modeling_config = (\n",
    "            exp.prepare_configs(modeling_config['modeling']['config_folder'], time_window))\n",
    "\n",
    "        # Generate flattened feature dfs for all datasets for the window\n",
    "        window_flattened_dfs, window_flattened_filepaths = tw.generate_window_flattened_dfs(\n",
    "            market_data_df,\n",
    "            macro_trends_df,\n",
    "            profits_df,\n",
    "            prices_df,\n",
    "            window_config,\n",
    "            window_metrics_config,\n",
    "            window_modeling_config\n",
    "        )\n",
    "\n",
    "        # Store window's flattened features\n",
    "        all_flattened_dfs.extend(window_flattened_dfs)\n",
    "        all_flattened_filepaths.extend(window_flattened_filepaths)\n",
    "\n",
    "\n",
    "    # 3. Combine features from all datasets in all time windows with target variables\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Combine all time windows for each dataset, the join the datasets together\n",
    "    concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "    training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n",
    "\n",
    "    # Create target variables for all time windows\n",
    "    target_variable_df, returns_df, = tw.create_target_variables_for_all_time_windows(training_data_df,\n",
    "                                                                                        prices_df,\n",
    "                                                                                        config,\n",
    "                                                                                        modeling_config)\n",
    "\n",
    "    # Split target variables into the train/test/validation/future sets\n",
    "    sets_X_y_dict = prp.perform_train_test_validation_future_splits(training_data_df,\n",
    "                                                                    target_variable_df,\n",
    "                                                                    modeling_config)\n",
    "\n",
    "    return sets_X_y_dict, returns_df, join_logs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data, target vars, and split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Combine all time windows for each dataset, the join the datasets together\n",
    "concatenated_dfs = tw.concat_dataset_time_windows_dfs(all_flattened_filepaths,modeling_config)\n",
    "training_data_df, join_logs_df = tw.join_dataset_all_windows_dfs(concatenated_dfs)\n",
    "\n",
    "# Create target variables for all time windows\n",
    "target_variable_df, returns_df, = tw.create_target_variables_for_all_time_windows(training_data_df,\n",
    "                                                                                    prices_df,\n",
    "                                                                                    config,\n",
    "                                                                                    modeling_config)\n",
    "\n",
    "sets_X_y_dict = prp.perform_train_test_validation_future_splits(training_data_df, target_variable_df, modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "sets_X_y_dict = prp.perform_train_test_validation_future_splits(training_data_df, target_variable_df, modeling_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_time_windows = training_data_df.index.get_level_values('time_window').unique()\n",
    "\n",
    "# Select the first n windows, counting in reverse order\n",
    "if data_partitioning_config['future_set_time_windows'] == 0:\n",
    "    future_time_windows = []\n",
    "else:\n",
    "    future_time_windows = unique_time_windows[-data_partitioning_config['future_set_time_windows']:]\n",
    "future_mask = training_data_df.index.get_level_values('time_window').isin(future_time_windows)\n",
    "\n",
    "X_future = training_data_df[future_mask]\n",
    "y_future = target_variable_df[future_mask]\n",
    "temp_training_data_df = training_data_df[~future_mask]\n",
    "temp_target_variable_df = target_variable_df[~future_mask]\n",
    "\n",
    "# return X_future, y_future, temp_training_data_df, temp_target_variable_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_partitioning_config['future_set_time_windows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "original_row_count = len(training_data_df)\n",
    "\n",
    "# 2. Train Test Split\n",
    "# -------------------\n",
    "data_partitioning_config = modeling_config['preprocessing']['data_partitioning']\n",
    "np.random.seed(modeling_config['modeling']['random_seed'])\n",
    "\n",
    "# Split future set if specified\n",
    "X_future, y_future, temp_training_data_df, temp_target_variable_df = prp.split_future_set(\n",
    "    training_data_df,\n",
    "    target_variable_df,\n",
    "    data_partitioning_config)\n",
    "\n",
    "# Split validation set\n",
    "X_validation, y_validation, temp_training_data_df, temp_target_variable_df = prp.split_validation_set(\n",
    "    temp_training_data_df,\n",
    "    temp_target_variable_df,\n",
    "    data_partitioning_config,\n",
    "    training_data_df)\n",
    "\n",
    "# Split train and test sets\n",
    "X_train, X_test, y_train, y_test = prp.split_train_test_sets(\n",
    "    temp_training_data_df,\n",
    "    temp_target_variable_df,\n",
    "    data_partitioning_config,\n",
    "    training_data_df,\n",
    ")\n",
    "\n",
    "# Create the result dictionary\n",
    "sets_dict = {\n",
    "    'train_set': (X_train, y_train),\n",
    "    'test_set': (X_test, y_test),\n",
    "    'validation_set': (X_validation, y_validation),\n",
    "    'future_set': (X_future, y_future)\n",
    "}\n",
    "\n",
    "# 3. Logs and additional data quality checks\n",
    "# ------------------------------------------\n",
    "\n",
    "# Prepare log message\n",
    "target_column = modeling_config['modeling']['target_column']\n",
    "unique_values = target_variable_df[target_column].unique()\n",
    "is_binary = len(unique_values) == 2 and set(unique_values).issubset({0, 1})\n",
    "\n",
    "log_message = \"Data Partitioning Results:\\n\"\n",
    "total_partitioned_rows = 0\n",
    "for set_name, (X, y) in sets_dict.items():\n",
    "    row_count = len(X)\n",
    "    total_partitioned_rows += row_count\n",
    "    log_message += f\"- {set_name}: {row_count} rows\"\n",
    "    if is_binary:\n",
    "\n",
    "        positive_count = (y[target_column] == 1).sum()\n",
    "        total_count = len(y)\n",
    "        percentage = (positive_count / total_count) * 100 if total_count > 0 else 0\n",
    "        log_message += f\", Positive samples: {positive_count} ({percentage:.2f}%)\"\n",
    "    log_message += \"\\n\"\n",
    "\n",
    "# Check if total rows in all sets equals original row count\n",
    "if total_partitioned_rows != original_row_count:\n",
    "    raise ValueError(f\"Data partitioning error: Total rows in all sets ({total_partitioned_rows}) \"\n",
    "                        f\"does not match original row count ({original_row_count})\")\n",
    "\n",
    "# Log the consolidated message\n",
    "logger.info(log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\", Positive samples: {positive_count[0]} ({percentage[0]:.2f}%)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique coin_ids\n",
    "unique_coin_ids = temp_training_data_df.index.get_level_values('coin_id').unique()\n",
    "total_coin_ids = len(unique_coin_ids)\n",
    "\n",
    "# Calculate the number of coin_ids for the validation set\n",
    "num_validation_coins = int(np.round(data_partitioning_config['validation_set_share'] * total_coin_ids))\n",
    "\n",
    "# Randomly select coin_ids for the validation set\n",
    "validation_coin_ids = np.random.choice(unique_coin_ids, size=num_validation_coins, replace=False)\n",
    "\n",
    "# Create masks for the validation and training sets\n",
    "validation_mask = temp_training_data_df.index.get_level_values('coin_id').isin(validation_coin_ids)\n",
    "\n",
    "# # Split the data\n",
    "# X_val = temp_training_data_df[validation_mask]\n",
    "# y_val = temp_target_variable_df[validation_mask]\n",
    "# temp_training_data_df = temp_training_data_df[~validation_mask]\n",
    "# temp_target_variable_df = temp_target_variable_df[~validation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_coin_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df = training_data_df.sort_index()\n",
    "target_variable_df = target_variable_df.sort_index()\n",
    "training_data_df.index.equals(target_variable_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both DataFrames have their indices sorted\n",
    "df1_name = 'training_data'\n",
    "df2_name = 'target_var'\n",
    "\n",
    "df1 = training_data_df.sort_index()\n",
    "df2 = target_variable_df.sort_index()\n",
    "\n",
    "print(f\"Indices_match: {df1.index.equals(df2.index)}\")\n",
    "\n",
    "\n",
    "# Find indices unique to each DataFrame\n",
    "only_in_df1 = df1.index.difference(df2.index)\n",
    "only_in_df2 = df2.index.difference(df1.index)\n",
    "\n",
    "# Print results\n",
    "print(f\"Indices only in {df1_name}:\")\n",
    "if len(only_in_df1) > 0:\n",
    "    print(only_in_df1.to_frame(index=False))\n",
    "else:\n",
    "    print(\"None\")\n",
    "\n",
    "print(f\"\\nIndices only in {df2_name}:\")\n",
    "if len(only_in_df2) > 0:\n",
    "    print(only_in_df2.to_frame(index=False))\n",
    "else:\n",
    "    print(\"None\")\n",
    "\n",
    "# Check for any duplicates in either index\n",
    "duplicates_df1 = df1.index[df1.index.duplicated()].unique()\n",
    "duplicates_df2 = df2.index[df2.index.duplicated()].unique()\n",
    "\n",
    "print(f\"\\nDuplicate indices in {df1_name}:\")\n",
    "if len(duplicates_df1) > 0:\n",
    "    print(duplicates_df1.to_frame(index=False))\n",
    "else:\n",
    "    print(\"None\")\n",
    "\n",
    "print(f\"\\nDuplicate indices in {df2_name}:\")\n",
    "if len(duplicates_df2) > 0:\n",
    "    print(duplicates_df2.to_frame(index=False))\n",
    "else:\n",
    "    print(\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Detailed index comparison between {df1_name} and {df2_name}:\")\n",
    "\n",
    "# Check index types\n",
    "print(f\"\\nIndex types:\")\n",
    "print(f\"{df1_name}: {type(df1.index)}\")\n",
    "print(f\"{df2_name}: {type(df2.index)}\")\n",
    "\n",
    "# Check index names\n",
    "print(f\"\\nIndex names:\")\n",
    "print(f\"{df1_name}: {df1.index.names}\")\n",
    "print(f\"{df2_name}: {df2.index.names}\")\n",
    "\n",
    "# Check index lengths\n",
    "print(f\"\\nIndex lengths:\")\n",
    "print(f\"{df1_name}: {len(df1.index)}\")\n",
    "print(f\"{df2_name}: {len(df2.index)}\")\n",
    "\n",
    "# Check dtypes of each level\n",
    "print(f\"\\nDtypes of each level:\")\n",
    "for level in df1.index.names:\n",
    "    print(f\"Level '{level}':\")\n",
    "    print(f\"  {df1_name}: {df1.index.get_level_values(level).dtype}\")\n",
    "    print(f\"  {df2_name}: {df2.index.get_level_values(level).dtype}\")\n",
    "\n",
    "# Check for NaN values in index\n",
    "print(f\"\\nNaN values in index:\")\n",
    "for level in df1.index.names:\n",
    "    print(f\"Level '{level}':\")\n",
    "    print(f\"  {df1_name}: {df1.index.get_level_values(level).isnull().sum()} NaN values\")\n",
    "    print(f\"  {df2_name}: {df2.index.get_level_values(level).isnull().sum()} NaN values\")\n",
    "\n",
    "# Compare a sample of index values\n",
    "print(f\"\\nSample comparison of index values:\")\n",
    "sample_size = min(5, len(df1.index))\n",
    "sample_indices = np.random.choice(len(df1.index), sample_size, replace=False)\n",
    "for i in sample_indices:\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  {df1_name}: {df1.index[i]}\")\n",
    "    print(f\"  {df2_name}: {df2.index[i]}\")\n",
    "    print(f\"  Equal: {df1.index[i] == df2.index[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "def create_target_variables_for_all_time_windows(training_data_df, prices_df, config, modeling_config):\n",
    "    \"\"\"\n",
    "    Create target variables for all time windows in training_data_df.\n",
    "\n",
    "    Parameters:\n",
    "    - training_data_df: DataFrame with multi-index (time_window, coin_id) and 'modeling_period_end' column\n",
    "    - prices_df: DataFrame with 'coin_id', 'date', and 'price' columns\n",
    "    - config: config.yaml\n",
    "    - modeling_config: modeling_config.yaml\n",
    "\n",
    "    Returns:\n",
    "    - combined_target_variables: DataFrame with columns for 'time_window' and the configured\n",
    "        target variable\n",
    "    - combined_returns: DataFrame with columns 'returns' and 'time_window'\n",
    "    \"\"\"\n",
    "    all_target_variables = []\n",
    "    all_returns = []\n",
    "\n",
    "    for time_window in training_data_df.index.get_level_values('time_window').unique():\n",
    "        # Get the list of coin_ids for the current time_window\n",
    "        current_coins = training_data_df.loc[time_window].index.get_level_values('coin_id').tolist()\n",
    "\n",
    "        # Filter prices_df for the current coins\n",
    "        current_prices_df = prices_df[prices_df['coin_id'].isin(current_coins)]\n",
    "\n",
    "        # Create copy of config with time_window's modeling period dates\n",
    "        current_training_data_config = config['training_data'].copy()\n",
    "        current_training_data_config['modeling_period_start'] = time_window\n",
    "        current_training_data_config['modeling_period_end'] = (\n",
    "                pd.to_datetime(time_window) + timedelta(days=current_training_data_config['modeling_period_duration'])\n",
    "                ).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Call create_target_variables function\n",
    "        target_variables_df, returns_df = prp.create_target_variables(\n",
    "            current_prices_df,\n",
    "            current_training_data_config,\n",
    "            modeling_config\n",
    "        )\n",
    "\n",
    "        # Add time_window information to the results\n",
    "        target_variables_df['time_window'] = time_window\n",
    "        returns_df['time_window'] = time_window\n",
    "\n",
    "        # Store results\n",
    "        all_target_variables.append(target_variables_df)\n",
    "        all_returns.append(returns_df)\n",
    "\n",
    "    # Combine results\n",
    "    combined_target_variables = pd.concat(all_target_variables, ignore_index=True)\n",
    "    combined_returns = pd.concat(all_returns, ignore_index=False)\n",
    "\n",
    "    return combined_target_variables, combined_returns\n",
    "\n",
    "# Usage\n",
    "combined_target_variables, combined_returns = create_target_variables_for_all_time_windows(\n",
    "    training_data_df,\n",
    "    prices_df,\n",
    "    config,\n",
    "    modeling_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_target_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coin_returns(prices_df, training_data_config):\n",
    "    \"\"\"\n",
    "    Prepares the data and computes price returns for each coin.\n",
    "\n",
    "    Parameters:\n",
    "    - prices_df: DataFrame containing price data with columns 'coin_id', 'date', and 'price'.\n",
    "    - training_data_config: Configuration with modeling period dates.\n",
    "\n",
    "    Returns:\n",
    "    - returns_df: DataFrame with columns 'coin_id' and 'returns'.\n",
    "    \"\"\"\n",
    "    prices_df = prices_df.copy()\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    modeling_period_start = pd.to_datetime(training_data_config['modeling_period_start'])\n",
    "    modeling_period_end = pd.to_datetime(training_data_config['modeling_period_end'])\n",
    "\n",
    "    # Filter data for start and end dates\n",
    "    start_prices = prices_df[prices_df['date'] == modeling_period_start].set_index('coin_id')['price']\n",
    "    end_prices = prices_df[prices_df['date'] == modeling_period_end].set_index('coin_id')['price']\n",
    "\n",
    "    # Identify coins with both start and end prices\n",
    "    valid_coins = start_prices.index.intersection(end_prices.index)\n",
    "\n",
    "    # Check for missing data\n",
    "    all_coins = prices_df['coin_id'].unique()\n",
    "    coins_missing_price = set(all_coins) - set(valid_coins)\n",
    "\n",
    "    if coins_missing_price:\n",
    "        missing = ', '.join(map(str, coins_missing_price))\n",
    "        raise ValueError(f\"Missing price for coins at start or end date: {missing}\")\n",
    "\n",
    "    # Compute returns\n",
    "    returns = (end_prices[valid_coins] - start_prices[valid_coins]) / start_prices[valid_coins]\n",
    "    returns_df = pd.DataFrame({'returns': returns})\n",
    "\n",
    "    return returns_df\n",
    "\n",
    "returns_df = calculate_coin_returns(prices_df, config['training_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "def create_target_variables_for_all_time_windows(training_data_df, prices_df, config, modeling_config):\n",
    "    \"\"\"\n",
    "    Create target variables for all time windows in training_data_df.\n",
    "\n",
    "    Parameters:\n",
    "    - training_data_df: DataFrame with multi-index (time_window, coin_id) and 'modeling_period_end' column\n",
    "    - prices_df: DataFrame with 'coin_id', 'date', and 'price' columns\n",
    "    - config: config.yaml\n",
    "    - modeling_config: modeling_config.yaml\n",
    "\n",
    "    Returns:\n",
    "    - combined_target_variables: DataFrame with columns for 'time_window' and the configured\n",
    "        target variable\n",
    "    - combined_returns: DataFrame with columns 'returns' and 'time_window'\n",
    "    \"\"\"\n",
    "    all_target_variables = []\n",
    "    all_returns = []\n",
    "\n",
    "    for time_window in training_data_df.index.get_level_values('time_window').unique():\n",
    "        # Filter prices_df for the current time window\n",
    "        current_prices_df = prices_df[\n",
    "            (prices_df['date'] >= time_window) &\n",
    "            (prices_df['date'] <= training_data_df.loc[time_window, 'modeling_period_end'].iloc[0])\n",
    "        ]\n",
    "\n",
    "        # Update training_data_config for the current time window\n",
    "        current_training_data_config = config['training_data'].copy()\n",
    "        current_training_data_config['modeling_period_start'] = time_window\n",
    "        current_training_data_config['modeling_period_end'] = training_data_df.loc[time_window, 'modeling_period_end'].iloc[0]\n",
    "\n",
    "        # Call create_target_variables function\n",
    "        target_variables_df, returns_df = prp.create_target_variables(\n",
    "            current_prices_df,\n",
    "            current_training_data_config,\n",
    "            modeling_config\n",
    "        )\n",
    "\n",
    "        # Add time_window information to the results\n",
    "        target_variables_df['time_window'] = time_window\n",
    "        returns_df['time_window'] = time_window\n",
    "\n",
    "        # Store results\n",
    "        all_target_variables.append(target_variables_df)\n",
    "        all_returns.append(returns_df)\n",
    "\n",
    "    # Combine results\n",
    "    combined_target_variables = pd.concat(all_target_variables, ignore_index=True)\n",
    "    combined_returns = pd.concat(all_returns, ignore_index=True)\n",
    "\n",
    "    return combined_target_variables, combined_returns\n",
    "\n",
    "# Usage\n",
    "combined_target_variables, combined_returns = create_target_variables_for_all_time_windows(\n",
    "    training_data_df,\n",
    "    prices_df,\n",
    "    config,\n",
    "    modeling_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_target_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window specific metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window-specific metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Prepare time window config files\n",
    "config, metrics_config, modeling_config = mif.prepare_configs(modeling_config['modeling']['config_folder'], time_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flattened_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_flattened_dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flattened_filepaths.extend([flattened_market_data_filepath])\n",
    "\n",
    "all_flattened_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "def generate_wallet_cohort_features(\n",
    "        profits_df,\n",
    "        prices_df,\n",
    "        config,\n",
    "        metrics_config,\n",
    "        modeling_config\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generates a window-specific feature set from the full all windows dataset. The window-specific\n",
    "    features are saved as a csv and returned, along with the csv filepath.\n",
    "\n",
    "    This function differs from the time_series set because it only flattens on date, since this\n",
    "    dataset doesn't have coin_id.\n",
    "\n",
    "    Params:\n",
    "    - all_windows_time_series_df (DataFrame): df containing all metrics and indicators for a time\n",
    "        series dataset.\n",
    "    - config: config.yaml that has the dates for the specific time window\n",
    "    - metrics_config: metrics_config.yaml\n",
    "    - modeling_config: modeling_config.yaml\n",
    "\n",
    "    Returns:\n",
    "    - flattened_metrics_df (DataFrame): the flattened version of the original df, with columns for\n",
    "        the configured aggregations and rolling metrics for all value columns and indicators.\n",
    "    - flattened_metrics_filepath (string): the filepath to where the flattened_metrics_df is saved\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Impute all required dates\n",
    "    # ----------------------------\n",
    "    # Identify all required imputation dates\n",
    "    imputation_dates = pri.identify_imputation_dates(config)\n",
    "\n",
    "    # Impute all required dates\n",
    "    window_profits_df = pri.impute_profits_for_multiple_dates(profits_df, prices_df, imputation_dates, n_threads=24)\n",
    "    window_profits_df = (window_profits_df[(window_profits_df['date'] >= pd.to_datetime(min(imputation_dates))) &\n",
    "                                        (window_profits_df['date'] <= pd.to_datetime(max(imputation_dates)))])\n",
    "\n",
    "\n",
    "    # 2. Generate metrics and indicators for all cohorts\n",
    "    # --------------------------------------------------\n",
    "    for cohort_name in metrics_config['wallet_cohorts']:\n",
    "\n",
    "        # load configs\n",
    "        dataset_metrics_config = metrics_config['wallet_cohorts'][cohort_name]\n",
    "        dataset_config = config['datasets']['wallet_cohorts'][cohort_name]\n",
    "\n",
    "        # identify wallets in the cohort based on the full lookback period\n",
    "        cohort_summary_df = cwm.classify_wallet_cohort(window_profits_df, dataset_config, cohort_name)\n",
    "        cohort_wallets = cohort_summary_df[cohort_summary_df['in_cohort']]['wallet_address']\n",
    "\n",
    "        # If no cohort members were identified, continue\n",
    "        if len(cohort_wallets) == 0:\n",
    "            logger.info(\"No wallets identified as members of cohort '%s'\", cohort_name)\n",
    "            continue\n",
    "\n",
    "        # Generate cohort buysell_metrics\n",
    "        cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,\n",
    "                                                            config['training_data']['training_period_end'],\n",
    "                                                            cohort_wallets)\n",
    "\n",
    "        # Generate cohort indicator metrics\n",
    "        cohort_metrics_df = ind.generate_time_series_indicators(cohort_metrics_df,\n",
    "                                                                metrics_config['wallet_cohorts'][cohort_name],\n",
    "                                                                'coin_id')\n",
    "\n",
    "        # Flatten cohort metrics\n",
    "        flattened_cohort_df, flattened_cohort_filepath = fe.generate_window_time_series_features(\n",
    "            cohort_metrics_df,\n",
    "            config,\n",
    "            dataset_metrics_config,\n",
    "            modeling_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the metrics DataFrame to be keyed only on coin_id\n",
    "flattened_metrics_df = fe.flatten_coin_date_df(\n",
    "    cohort_metrics_df,\n",
    "    metrics_config['wallet_cohorts'][cohort_name],\n",
    "    config['training_data']['training_period_end']  # Ensure data is up to training period end\n",
    ")\n",
    "flattened_metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time window modeling period start\n",
    "flattened_metrics_df.loc[:,'time_window'] = config['training_data']['modeling_period_start']\n",
    "\n",
    "# Save the flattened output and retrieve the file path\n",
    "_, flattened_metrics_filepath = fe.save_flattened_outputs(\n",
    "    flattened_metrics_df,\n",
    "    os.path.join(\n",
    "        modeling_config['modeling']['modeling_folder'],  # Folder to store flattened outputs\n",
    "        'outputs/flattened_outputs'\n",
    "    ),\n",
    "    'market_data',  # Descriptive metadata for the dataset\n",
    "    config['training_data']['modeling_period_start']  # Ensure data starts from modeling period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cohort buysell_metrics\n",
    "cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,\n",
    "                                                    config['training_data']['training_period_end'],\n",
    "                                                    cohort_wallets)\n",
    "\n",
    "\n",
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate cohort indicator metrics\n",
    "cohort_metrics_df = ind.generate_time_series_indicators(cohort_metrics_df,\n",
    "                                                        metrics_config['wallet_cohorts'][cohort_name],\n",
    "                                                        'coin_id')\n",
    "\n",
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = cohort_metrics_df\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['training_period_end']\n",
    "id_column='coin_id'\n",
    "drop_outside_date_range=True\n",
    "\n",
    "# Convert params to datetime\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "\n",
    "# Create copy of df\n",
    "time_series_df = time_series_df.copy()\n",
    "\n",
    "# Drop all rows with any NaN values\n",
    "time_series_df = time_series_df.dropna()\n",
    "\n",
    "# Define a function to check if a date range has full coverage\n",
    "def has_full_coverage(min_date, max_date):\n",
    "    return (min_date <= start_date) and (max_date >= end_date)\n",
    "\n",
    "if id_column:\n",
    "    # Multi-series data\n",
    "    series_data_range = time_series_df.groupby(id_column, observed=True)['date'].agg(['min', 'max'])\n",
    "    full_duration_series = series_data_range[series_data_range.apply(lambda x: has_full_coverage(x['min'], x['max']), axis=1)].index\n",
    "else:\n",
    "    # Single-series data\n",
    "    series_data_range = time_series_df['date'].agg(['min', 'max'])\n",
    "    full_duration_series = [0] if has_full_coverage(series_data_range['min'], series_data_range['max']) else []\n",
    "\n",
    "# Calculate coverage statistics\n",
    "full_coverage_count = len(full_duration_series)\n",
    "\n",
    "# # Split the dataframe\n",
    "# if id_column:\n",
    "#     # Convert id column to categorical to reduce memory usage\n",
    "#     time_series_df[id_column] = time_series_df[id_column].astype('category')\n",
    "#     full_coverage_df = time_series_df[time_series_df[id_column].isin(full_duration_series)]\n",
    "#     partial_coverage_df = time_series_df[~time_series_df[id_column].isin(full_duration_series)]\n",
    "# else:\n",
    "#     full_coverage_df = time_series_df if full_coverage_count else pd.DataFrame(columns=time_series_df.columns)\n",
    "#     partial_coverage_df = time_series_df if not full_coverage_count else pd.DataFrame(columns=time_series_df.columns)\n",
    "\n",
    "# logger.info(\"Split df with dimensions %s into %s full coverage records and %s partial coverage records.\",\n",
    "#             time_series_df.shape,\n",
    "#             len(full_coverage_df),\n",
    "#             len(partial_coverage_df))\n",
    "\n",
    "# if drop_outside_date_range:\n",
    "#     # Remove rows outside the date range for both dataframes\n",
    "#     full_coverage_df = (full_coverage_df[(full_coverage_df['date'] >= start_date) &\n",
    "#                                             (full_coverage_df['date'] <= end_date)])\n",
    "#     partial_coverage_df = (partial_coverage_df[(partial_coverage_df['date'] >= start_date) &\n",
    "#                                                 (partial_coverage_df['date'] <= end_date)])\n",
    "\n",
    "#     # Log the number of remaining records\n",
    "#     total_remaining = len(full_coverage_df) + len(partial_coverage_df)\n",
    "#     logger.info(\"After removing records outside the date range, %s records remain.\",\n",
    "#                 total_remaining)\n",
    "\n",
    "# # return full_coverage_df, partial_coverage_df\n",
    "\n",
    "time_series_df.shape\n",
    "full_coverage_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data_range = time_series_df.groupby(id_column, observed=True)['date'].agg(['min', 'max'])\n",
    "series_data_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cohort_metrics_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If no cohort members were identified, continue\n",
    "if len(cohort_wallets) == 0:\n",
    "    logger.info(\"No wallets identified as members of cohort '%s'\", cohort_name)\n",
    "    continue\n",
    "\n",
    "# Generate cohort buysell_metrics\n",
    "cohort_metrics_df = cwm.generate_buysell_metrics_df(profits_df,\n",
    "                                                    config['training_data']['training_period_end'],\n",
    "                                                    cohort_wallets)\n",
    "\n",
    "# Generate cohort indicator metrics\n",
    "cohort_metrics_df = ind.generate_time_series_indicators(cohort_metrics_df,\n",
    "                                                        metrics_config['wallet_cohorts'][cohort_name],\n",
    "                                                        'coin_id')\n",
    "\n",
    "# Split df to only training_period\n",
    "cohort_metrics_df,_ = cwm.split_dataframe_by_coverage(cohort_metrics_df,\n",
    "                                                    config['training_data']['training_period_start'],\n",
    "                                                    config['training_data']['modeling_period_end'],\n",
    "                                                    id_column='coin_id',\n",
    "                                                    drop_outside_date_range=True)\n",
    "\n",
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Add time window modeling period start\n",
    "# cohort_metrics_df.loc[:,'time_window'] = config['training_data']['modeling_period_start']\n",
    "\n",
    "cohort_metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market data: generate window-specific flattened metrics\n",
    "flattened_market_data_df, flattened_market_data_filepath = fe.generate_window_time_series_features(\n",
    "    market_data_df,\n",
    "    config,\n",
    "    metrics_config,\n",
    "    modeling_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metrics_config['total_bought']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config['wallet_cohorts']['whales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.df_mem(profits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IN WINDOW FUNCTIONS\n",
    "\n",
    "market_data_df: just filter to window\n",
    "macro_trends_df: just filter to window\n",
    "\n",
    "profits_df\n",
    "1. identify all dates needed\n",
    "    all cohort lookback window starts\n",
    "    training_period_start\n",
    "    training_period_end\n",
    "    modeling_period_start\n",
    "    modeling_period_end\n",
    "2. impute them\n",
    "3. filter df to only dates between earliest and latest dates\n",
    "\n",
    "\n",
    "4. wallet cohorts and buysell metrics\n",
    "5. indicators\n",
    "6. filter to window\n",
    "\"\"\"\n",
    "\n",
    "# def build_time_window_model_input(n, window, config, metrics_config, modeling_config):\n",
    "#     \"\"\"\n",
    "#     Generates training data for each of the config.training_data.additional_windows.\n",
    "\n",
    "#     Params:\n",
    "#         n (int): The lookback number of the time window (e.g 0,1,2)\n",
    "#         window (Dict): The config override dict with the window's modeling_period_start\n",
    "#         config: config.yaml\n",
    "#         metrics_config: metrics_config.yaml\n",
    "#         modeling_config: modeling_config.yaml\n",
    "\n",
    "#     Returns:\n",
    "#         model_data (Dict): Dictionary containing all of the modeling features and variables:\n",
    "#             X_train, X_test (DataFrame): Model training features\n",
    "#             y_train, y_test (pd.Series): Model target variables\n",
    "#             returns_test (DataFrame): The actual returns of each coin_id in each time_window.\n",
    "#                 - coin_id: Index (str)\n",
    "#                 - time_window: Index (int)\n",
    "#                 - returns: value column (float)\n",
    "#     \"\"\"\n",
    "\n",
    "# Prepare the full configuration by applying overrides from the current trial config\n",
    "config, metrics_config, modeling_config = prepare_configs(modeling_config['modeling']['config_folder'], window)\n",
    "\n",
    "# Define window start and end dates\n",
    "start_date = config['training_data']['training_period_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "\n",
    "# Rebuild market data\n",
    "market_data_df = dr.retrieve_market_data()\n",
    "market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "\n",
    "# Retrieve macro trends data\n",
    "macro_trends_df = dr.retrieve_macro_trends_data()\n",
    "macro_trends_df = cwm.generate_macro_trends_features(macro_trends_df, config)\n",
    "\n",
    "# Rebuild profits_df\n",
    "if 'profits_df' not in locals():\n",
    "    profits_df = None\n",
    "profits_df = rebuild_profits_df_if_necessary(config, prices_df, profits_df)\n",
    "\n",
    "# Build the configured model input data for the nth window\n",
    "X_train, X_test, y_train, y_test, returns_test = build_configured_model_input(\n",
    "                                    profits_df,\n",
    "                                    market_data_df,\n",
    "                                    macro_trends_df,\n",
    "                                    config,\n",
    "                                    metrics_config,\n",
    "                                    modeling_config)\n",
    "\n",
    "# Add time window indices to dfs with coin_ids\n",
    "X_train['time_window'] = n\n",
    "X_train.set_index('time_window', append=True, inplace=True)\n",
    "X_test['time_window'] = n\n",
    "X_test.set_index('time_window', append=True, inplace=True)\n",
    "returns_test['time_window'] = n\n",
    "returns_test.set_index('time_window', append=True, inplace=True)\n",
    "\n",
    "model_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'returns_test': returns_test\n",
    "}\n",
    "\n",
    "# return model_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "\n",
    "# Initialize empty lists to hold concatenated data\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "returns_test_list = []\n",
    "\n",
    "for n, window in enumerate(time_windows):\n",
    "\n",
    "    model_data = mif.build_time_window_model_input(n, window, config, metrics_config, modeling_config)\n",
    "\n",
    "    # Append the current window's data to the lists\n",
    "    X_train_list.append(model_data['X_train'])\n",
    "    X_test_list.append(model_data['X_test'])\n",
    "    y_train_list.append(model_data['y_train'])\n",
    "    y_test_list.append(model_data['y_test'])\n",
    "    returns_test_list.append(model_data['returns_test'])\n",
    "\n",
    "\n",
    "# Concatenate all the data for each part\n",
    "X_train = pd.concat(X_train_list, axis=0)\n",
    "X_test = pd.concat(X_test_list, axis=0)\n",
    "y_train = pd.concat(y_train_list, axis=0)\n",
    "y_test = pd.concat(y_test_list, axis=0)\n",
    "returns_test = pd.concat(returns_test_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 3.4 Train the model using the current configuration and log the results\n",
    "model, model_id = m.train_model(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    modeling_config)\n",
    "\n",
    "# 3.5 Evaluate and save the model performance on the test set to a CSV\n",
    "metrics_dict, y_pred, y_pred_prob = m.evaluate_model(model, X_test, y_test, model_id, returns_test, modeling_config)\n",
    "\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "features = X_train.columns  # Feature names\n",
    "\n",
    "# Create a DataFrame with feature names and importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for module in modules:\n",
    "    importlib.reload(module)\n",
    "\n",
    "\n",
    "# Select y_pred_prob from the classifier, or y_pred from a regressor\n",
    "predictions = y_pred_prob or y_pred\n",
    "returns = returns_test['returns']\n",
    "winsorization_cutoff = modeling_config[\"evaluation\"][\"winsorization_cutoff\"]\n",
    "\n",
    "\n",
    "ia.generate_profitability_curves(predictions, returns, winsorization_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Window Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking profits_df downcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = config['training_data']['earliest_window_start']\n",
    "end_date = config['training_data']['modeling_period_end']\n",
    "minimum_wallet_inflows = config['data_cleaning']['minimum_wallet_inflows']\n",
    "\n",
    "# SQL query to retrieve profits data\n",
    "query_sql = f\"\"\"\n",
    "    -- STEP 1: retrieve profits data and apply USD inflows filter\n",
    "    -------------------------------------------------------------\n",
    "    with profits_base as (\n",
    "        select coin_id\n",
    "        ,date\n",
    "        ,wallet_address\n",
    "        ,profits_cumulative\n",
    "        ,usd_balance\n",
    "        ,usd_net_transfers\n",
    "        ,usd_inflows\n",
    "        ,usd_inflows_cumulative\n",
    "        from core.coin_wallet_profits\n",
    "        where date <= '{end_date}'\n",
    "    ),\n",
    "\n",
    "    usd_inflows_filter as (\n",
    "        select coin_id\n",
    "        ,wallet_address\n",
    "        ,max(usd_inflows_cumulative) as total_usd_inflows\n",
    "        from profits_base\n",
    "        -- we don't need to include coin-wallet pairs that have no transactions between\n",
    "        -- the start and end dates\n",
    "        group by 1,2\n",
    "    ),\n",
    "\n",
    "    profits_base_filtered as (\n",
    "        select pb.*\n",
    "        from profits_base pb\n",
    "        join usd_inflows_filter f on f.coin_id = pb.coin_id\n",
    "            and f.wallet_address = pb.wallet_address\n",
    "        where f.total_usd_inflows >= {minimum_wallet_inflows}\n",
    "    ),\n",
    "\n",
    "\n",
    "    -- STEP 2: create new records for all coin-wallet pairs as of the training_period_start\n",
    "    ---------------------------------------------------------------------------------------\n",
    "    -- compute the starting profits and balances as of the training_period_start\n",
    "    training_start_existing_rows as (\n",
    "        -- identify coin-wallet pairs that already have a balance as of the period end\n",
    "        select *\n",
    "        from profits_base_filtered\n",
    "        where date = '{start_date}'\n",
    "    ),\n",
    "    training_start_needs_rows as (\n",
    "        -- for coin-wallet pairs that don't have existing records, identify the row closest to the period end date\n",
    "        select t.*\n",
    "        ,cmd_previous.price as price_previous\n",
    "        ,cmd_training.price as price_current\n",
    "        ,row_number() over (partition by t.coin_id,t.wallet_address order by t.date desc) as rn\n",
    "        from profits_base_filtered t\n",
    "        left join training_start_existing_rows e on e.coin_id = t.coin_id\n",
    "            and e.wallet_address = t.wallet_address\n",
    "\n",
    "        -- obtain the last price used to compute the balance and profits data\n",
    "        join core.coin_market_data cmd_previous on cmd_previous.coin_id = t.coin_id and cmd_previous.date = t.date\n",
    "\n",
    "        -- obtain the training_period_start price so we can update the calculations\n",
    "        join core.coin_market_data cmd_training on cmd_training.coin_id = t.coin_id and cmd_training.date = '{start_date}'\n",
    "        where t.date < '{start_date}'\n",
    "        and e.coin_id is null\n",
    "    ),\n",
    "    training_start_new_rows as (\n",
    "        -- create a new row for the period end date by carrying the balance from the closest existing record\n",
    "        select t.coin_id\n",
    "        ,cast('{start_date}' as datetime) as date\n",
    "        ,t.wallet_address\n",
    "        -- profits_cumulative is the previous profits_cumulative + the change in profits up to the start_date\n",
    "        ,((t.price_current / t.price_previous) - 1) * t.usd_balance + t.profits_cumulative as profits_cumulative\n",
    "        -- usd_balance is previous balance * (1 + % change in price)\n",
    "        ,(t.price_current / t.price_previous) * t.usd_balance as usd_balance\n",
    "        -- there were no transfers\n",
    "        ,0 as usd_net_transfers\n",
    "        -- there were no inflows\n",
    "        ,0 as usd_inflows\n",
    "        -- no change since there were no inflows\n",
    "        ,usd_inflows_cumulative as usd_inflows_cumulative\n",
    "\n",
    "        from training_start_needs_rows t\n",
    "        where rn=1\n",
    "\n",
    "    ),\n",
    "\n",
    "    -- STEP 3: merge all records together\n",
    "    -------------------------------------\n",
    "    profits_merged as (\n",
    "        select * from profits_base_filtered\n",
    "        -- transfers prior to the training period are summarized in training_start_new_rows\n",
    "        where date >= '{start_date}'\n",
    "\n",
    "        union all\n",
    "\n",
    "        select * from training_start_new_rows\n",
    "    )\n",
    "\n",
    "    select coin_id\n",
    "    ,date\n",
    "\n",
    "    -- replace the memory-intensive address strings with integers\n",
    "    ,DENSE_RANK() OVER (ORDER BY wallet_address) as wallet_address\n",
    "\n",
    "    ,profits_cumulative\n",
    "    ,usd_balance\n",
    "    ,usd_net_transfers\n",
    "    ,usd_inflows\n",
    "    -- set a floor of $0.01 to avoid divide by 0 errors caused by rounding\n",
    "    ,greatest(0.01,usd_inflows_cumulative) as usd_inflows_cumulative\n",
    "    from profits_merged\n",
    "\"\"\"\n",
    "\n",
    "# Run the SQL query using dgc's run_sql method\n",
    "profits_df = dgc().run_sql(query_sql)\n",
    "\n",
    "logger.info('Converting columns to memory-optimized formats...')\n",
    "\n",
    "# Convert coin_id to categorical and date to date\n",
    "profits_df['coin_id'] = profits_df['coin_id'].astype('category')\n",
    "profits_df['date'] = pd.to_datetime(profits_df['date'])\n",
    "\n",
    "# Add total_return column\n",
    "profits_df['total_return'] = (profits_df['profits_cumulative']\n",
    "                                / profits_df['usd_inflows_cumulative'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_df = safe_downcast(profits_df, 'wallet_address', 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.can_cast(market_data_df['market_cap'].dtype, 'int32', casting='safe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df = safe_downcast(market_data_df, 'volume', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Data resequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "n = 0\n",
    "window = time_windows[n]\n",
    "\n",
    "# Prepare the full configuration by applying overrides from the current trial config\n",
    "config, metrics_config, modeling_config = mif.prepare_configs(modeling_config['modeling']['config_folder'], window)\n",
    "\n",
    "# Generate time_windows config overrides that will modify each window's config settings\n",
    "time_windows = mif.generate_time_windows(config)\n",
    "n = 0\n",
    "window = time_windows[n]\n",
    "\n",
    "\n",
    "# market_data_df, _ = cwm.split_dataframe_by_coverage(market_data_df, start_date, end_date, id_column='coin_id')\n",
    "# prices_df = market_data_df[['coin_id','date','price']].copy()\n",
    "market_data_df_full = market_data_df.copy()\n",
    "market_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "market_data_df = market_data_df_full.copy()\n",
    "print(market_data_df.columns)\n",
    "market_data_df = ind.generate_time_series_indicators('market_data', market_data_df, metrics_config)\n",
    "print(market_data_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(time_series_df.index, pd.RangeIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "market_data_df = market_data_df_full.copy()\n",
    "value_column = 'price'\n",
    "value_column_indicators_config = metrics_config['time_series']['market_data'][value_column]['indicators']\n",
    "id_column = 'coin_id'\n",
    "market_data_df = ind.generate_column_time_series_indicators(\n",
    "    market_data_df,\n",
    "    value_column,\n",
    "    value_column_indicators_config,\n",
    "    id_column\n",
    ")\n",
    "\n",
    "market_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = None\n",
    "if not id_column:\n",
    "    print('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = time_series_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "# time_series_df = market_data_df[['date','coin_id','price']].copy()\n",
    "time_series_df = market_data_df_full.copy()\n",
    "config = config\n",
    "value_column_indicators_config = metrics_config['time_series']['market_data']['price']['indicators']\n",
    "value_column = 'price'\n",
    "id_column='coin_id'\n",
    "\n",
    "time_series_df = time_series_df.set_index(['coin_id','date'])\n",
    "\n",
    "# Data Quality Checks and Formatting\n",
    "if value_column not in time_series_df.columns:\n",
    "    raise KeyError(f\"Input DataFrame does not include column '{value_column}'.\")\n",
    "\n",
    "if time_series_df[value_column].isnull().any():\n",
    "    raise ValueError(f\"The '{value_column}' column contains null values, which are not allowed.\")\n",
    "\n",
    "# Indicator Calculations\n",
    "# ----------------------\n",
    "# If there is an id_column, group on it\n",
    "if id_column:\n",
    "    groupby_column = id_column\n",
    "# If there isn't, create a dummy_column for grouping and remove it later\n",
    "else:\n",
    "    time_series_df['dummy_group'] = 1\n",
    "    groupby_column = 'dummy_group'\n",
    "\n",
    "# For each indicator, loop through all options and add the appropriate column\n",
    "for indicator, indicator_config in value_column_indicators_config.items():\n",
    "    if indicator == 'sma':\n",
    "        windows = indicator_config['parameters']['window']\n",
    "        for w in windows:\n",
    "            ind_series = time_series_df.groupby(level=groupby_column, observed=True)[value_column].transform(\n",
    "                lambda x: ind.calculate_sma(x, w))\n",
    "            time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    elif indicator == 'ema':\n",
    "        windows = indicator_config['parameters']['window']\n",
    "        for w in windows:\n",
    "            ind_series = time_series_df.groupby(level=groupby_column, observed=True)[value_column].transform(\n",
    "                lambda x: ind.calculate_ema(x, w))\n",
    "            time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    # elif indicator == 'rsi':\n",
    "    #     windows = indicator_config['parameters']['window']\n",
    "    #     for w in windows:\n",
    "    #         ind_series = time_series_df.groupby(level=groupby_column, observed=True)['price'].transform(\n",
    "    #             lambda x: calculate_rsi(x, w))\n",
    "    #         time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    # elif indicator == 'bollinger_bands_upper':\n",
    "    #     windows = indicator_config['parameters']['window']\n",
    "    #     num_std = indicator_config['parameters'].get('num_std', None)\n",
    "    #     for w in windows:\n",
    "    #         ind_series = time_series_df.groupby(level=groupby_column, observed=True)['price'].transform(\n",
    "    #             lambda x: calculate_bollinger_bands(x, 'upper', w, num_std))\n",
    "    #         time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "    # elif indicator == 'bollinger_bands_lower':\n",
    "    #     windows = indicator_config['parameters']['window']\n",
    "    #     num_std = indicator_config['parameters'].get('num_std', None)\n",
    "    #     for w in windows:\n",
    "    #         ind_series = time_series_df.groupby(level=groupby_column, observed=True)['price'].transform(\n",
    "    #             lambda x: calculate_bollinger_bands(x, 'lower', w, num_std))\n",
    "    #         time_series_df[f\"{value_column}_{indicator}_{w}\"] = ind_series\n",
    "\n",
    "# Remove the dummy column if it was created\n",
    "if groupby_column == 'dummy_group':\n",
    "    time_series_df = time_series_df.drop('dummy_group', axis=1)\n",
    "\n",
    "logger.info(\"Generated indicators for column '%s' :%s\",\n",
    "            value_column,\n",
    "            list(value_column_indicators_config.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(value_column_indicators_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"%s\",value_column_indicators_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.xs('9d6619f4-b44b-4ff4-9f68-1f563f57e060',level='coin_id').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = indicator_config['parameters'].get('num_std', None)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_df.groupby(level='coin_id', observed=True)['price'].transform("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indicators implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "\n",
    "df = market_data_df.copy()\n",
    "df = market_data_df.set_index(['coin_id','date'])\n",
    "\n",
    "\n",
    "# Add Relative Strength Index (RSI)\n",
    "df['rsi'] = df.groupby(level='coin_id', observed=True)['price'].transform(\n",
    "    lambda x: ind.calculate_rsi(x, 14))\n",
    "# Add Money Flow Index (MFI)\n",
    "df = ind.add_mfi_column(df)\n",
    "\n",
    "# Calculate MACD with EMAs\n",
    "df['ema_12'] = df.groupby(level='coin_id', observed=True)['price'].transform(lambda x: ind.calculate_ema(x, 12))\n",
    "df['ema_26'] = df.groupby(level='coin_id', observed=True)['price'].transform(lambda x: ind.calculate_ema(x, 26))\n",
    "df = ind.add_crossover_column(df, 'ema_12', 'ema_26', drop_col1=True, drop_col2=True)\n",
    "\n",
    "# Add Bollinger Bands\n",
    "df = ind.add_bollinger_bands(df, include_middle=False)\n",
    "# Add crossover for price and upper band\n",
    "df = ind.add_crossover_column(df, 'price', 'bollinger_band_upper', drop_col1=False, drop_col2=True)\n",
    "# Add crossover for price and lower band\n",
    "df = ind.add_crossover_column(df, 'price', 'bollinger_band_lower', drop_col1=False, drop_col2=True)\n",
    "\n",
    "# Calculate OBV\n",
    "df['obv_price_volume'] = ind.generalized_obv(df['price'],df['volume'])\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Define a function to calculate MFI within each group, similar to the crossovers function\n",
    "def apply_mfi(group):\n",
    "    # Reset index to avoid issues with the multi-index during group operations\n",
    "    group = group.reset_index()\n",
    "    group['mfi'] = ind.calculate_mfi(group['price'], group['volume'])\n",
    "\n",
    "    # Set index back to the original multi-index\n",
    "    return group.set_index(['coin_id', 'date'])\n",
    "\n",
    "# Apply the function within each 'coin_id' group\n",
    "df = df.groupby('coin_id', observed=True, group_keys=False).apply(apply_mfi)\n",
    "\n",
    "# Display the updated DataFrame with the MFI column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "df2 = df[['ema_12','ema_26']].copy()\n",
    "\n",
    "df2 = ind.add_crossover_column(df2, 'ema_12', 'ema_26', drop_col1=True, drop_col2=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_crossovers(series1, series2):\n",
    "    \"\"\"\n",
    "    Identify crossovers between two time series.\n",
    "\n",
    "    This function calculates the points where series1 crosses over series2.\n",
    "    It handles NaN values by converting them to 0.\n",
    "\n",
    "    Parameters:\n",
    "    series1 (array-like): The first time series\n",
    "    series2 (array-like): The second time series\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of the same length as the input series, where:\n",
    "        0 indicates no crossover\n",
    "        1 indicates an upward crossover (series1 crosses above series2)\n",
    "        -1 indicates a downward crossover (series1 crosses below series2)\n",
    "    \"\"\"\n",
    "    diff = series1 - series2\n",
    "\n",
    "    # Handle NaN values\n",
    "    diff = np.nan_to_num(diff, nan=0.0)\n",
    "\n",
    "    # Initialize crossovers array\n",
    "    crossovers = np.zeros(len(series1))\n",
    "\n",
    "    # Identify crossovers\n",
    "    signs = np.sign(diff)\n",
    "    sign_changes = signs[1:] != signs[:-1]\n",
    "    crossover_indices = np.where(sign_changes)[0] + 1\n",
    "\n",
    "    # Assign 1 for upward crossovers, -1 for downward crossovers\n",
    "    crossovers[crossover_indices] = np.where(signs[crossover_indices] > 0, 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ema_12','ema_26']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `df` is your DataFrame with multi-index (coin_id, date) and ema_12, ema_26 columns\n",
    "\n",
    "# Define a function that applies identify_crossovers to a group\n",
    "def apply_crossovers(group):\n",
    "    group['crossovers'] = identify_crossovers(group['ema_12'], group['ema_26'])\n",
    "    return group\n",
    "\n",
    "# Apply the function within each 'coin_id' group\n",
    "df = df.groupby('coin_id', group_keys=False).apply(apply_crossovers)\n",
    "\n",
    "# Display the resulting DataFrame with the new 'crossovers' column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame with the new 'crossovers' column\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# @pytest.fixture\n",
    "def rolling_metrics_config():\n",
    "    \"\"\"Fixture providing a complex metrics configuration with rolling metrics.\"\"\"\n",
    "    return {\n",
    "        \"wallet_cohorts\": {\n",
    "            \"whales\": {\n",
    "                \"total_volume\": {\n",
    "                    \"aggregations\": {\n",
    "                        \"last\": {\n",
    "                            \"scaling\": \"log\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"rolling\": {\n",
    "                        \"aggregations\": {\n",
    "                            \"mean\": {\n",
    "                                \"scaling\": \"log\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"window_duration\": 10,\n",
    "                        \"lookback_periods\": 3\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "rolling_metrics_config=rolling_metrics_config()\n",
    "# @pytest.fixture\n",
    "def dummy_rolling_dataframe():\n",
    "    \"\"\"\n",
    "    Fixture providing a dummy dataframe with a MultiIndex and sample data for rolling metrics.\n",
    "    \"\"\"\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            pd.to_datetime(['2023-01-01', '2023-01-02']),\n",
    "            ['bitcoin', 'ethereum']\n",
    "        ],\n",
    "        names=['time_window', 'coin_id']\n",
    "    )\n",
    "    data = {\n",
    "        'wallet_cohorts_whales_total_volume_last': [1000, 2000, 3000, 4000],\n",
    "        'wallet_cohorts_whales_total_volume_mean_10d_period_1': [500, 600, 700, 800],\n",
    "        'wallet_cohorts_whales_total_volume_mean_10d_period_2': [400, 500, 600, 700],\n",
    "        'wallet_cohorts_whales_total_volume_mean_10d_period_3': [300, 400, 500, 600],\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=index)\n",
    "    return df\n",
    "dummy_rolling_dataframe=dummy_rolling_dataframe()\n",
    "\n",
    "# @pytest.mark.unit\n",
    "# def test_scaling_processor_with_rolling_metrics(rolling_metrics_config, dummy_rolling_dataframe):\n",
    "\"\"\"\n",
    "Test the ScalingProcessor class for correct column mapping and scaling application with rolling metrics.\n",
    "\"\"\"\n",
    "# Instantiate the ScalingProcessor with the provided rolling_metrics_config\n",
    "processor = prp.ScalingProcessor(rolling_metrics_config)\n",
    "\n",
    "# Expected column_scaling_map based on the rolling_metrics_config\n",
    "expected_column_scaling_map = {\n",
    "    'wallet_cohorts_whales_total_volume_last': 'log',\n",
    "    'wallet_cohorts_whales_total_volume_mean_10d_period_1': 'log',\n",
    "    'wallet_cohorts_whales_total_volume_mean_10d_period_2': 'log',\n",
    "    'wallet_cohorts_whales_total_volume_mean_10d_period_3': 'log',\n",
    "}\n",
    "\n",
    "# Assert that the column_scaling_map is as expected\n",
    "assert processor.column_scaling_map == expected_column_scaling_map, (\n",
    "    \"Column scaling map does not match expected mapping.\"\n",
    ")\n",
    "\n",
    "# Apply scaling to the dummy_rolling_dataframe (as training data)\n",
    "scaled_df = processor.apply_scaling(dummy_rolling_dataframe, is_train=True)\n",
    "\n",
    "# Prepare expected scaled values for each column\n",
    "# For each column, scaling is 'log', so we apply np.log1p to the original values\n",
    "\n",
    "# Logical steps for 'wallet_cohorts_whales_total_volume_last':\n",
    "# - Original values: [1000, 2000, 3000, 4000]\n",
    "# - Apply np.log1p to each value to get the expected scaled values\n",
    "original_values_last = dummy_rolling_dataframe['wallet_cohorts_whales_total_volume_last'].values\n",
    "expected_values_last = np.log1p(original_values_last)\n",
    "\n",
    "# Logical steps for 'wallet_cohorts_whales_total_volume_mean_10d_period_1':\n",
    "# - Original values: [500, 600, 700, 800]\n",
    "# - Apply np.log1p to each value\n",
    "original_values_mean1 = dummy_rolling_dataframe[\n",
    "    'wallet_cohorts_whales_total_volume_mean_10d_period_1'\n",
    "].values\n",
    "expected_values_mean1 = np.log1p(original_values_mean1)\n",
    "\n",
    "# Logical steps for 'wallet_cohorts_whales_total_volume_mean_10d_period_2':\n",
    "# - Original values: [400, 500, 600, 700]\n",
    "# - Apply np.log1p to each value\n",
    "original_values_mean2 = dummy_rolling_dataframe[\n",
    "    'wallet_cohorts_whales_total_volume_mean_10d_period_2'\n",
    "].values\n",
    "expected_values_mean2 = np.log1p(original_values_mean2)\n",
    "\n",
    "# Logical steps for 'wallet_cohorts_whales_total_volume_mean_10d_period_3':\n",
    "# - Original values: [300, 400, 500, 600]\n",
    "# - Apply np.log1p to each value\n",
    "original_values_mean3 = dummy_rolling_dataframe[\n",
    "    'wallet_cohorts_whales_total_volume_mean_10d_period_3'\n",
    "].values\n",
    "expected_values_mean3 = np.log1p(original_values_mean3)\n",
    "\n",
    "# Now, compare the scaled values in scaled_df to the expected values calculated above\n",
    "\n",
    "# Compare 'wallet_cohorts_whales_total_volume_last' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['wallet_cohorts_whales_total_volume_last'].values,\n",
    "    expected_values_last,\n",
    "    atol=1e-4,\n",
    "    err_msg=(\n",
    "        \"Scaled values for 'wallet_cohorts_whales_total_volume_last' do not match \"\n",
    "        \"expected log-transformed values.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare 'wallet_cohorts_whales_total_volume_mean_10d_period_1' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['wallet_cohorts_whales_total_volume_mean_10d_period_1'].values,\n",
    "    expected_values_mean1,\n",
    "    atol=1e-4,\n",
    "    err_msg=(\n",
    "        \"Scaled values for 'wallet_cohorts_whales_total_volume_mean_10d_period_1' \"\n",
    "        \"do not match expected log-transformed values.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare 'wallet_cohorts_whales_total_volume_mean_10d_period_2' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['wallet_cohorts_whales_total_volume_mean_10d_period_2'].values,\n",
    "    expected_values_mean2,\n",
    "    atol=1e-4,\n",
    "    err_msg=(\n",
    "        \"Scaled values for 'wallet_cohorts_whales_total_volume_mean_10d_period_2' \"\n",
    "        \"do not match expected log-transformed values.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compare 'wallet_cohorts_whales_total_volume_mean_10d_period_3' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['wallet_cohorts_whales_total_volume_mean_10d_period_3'].values,\n",
    "    expected_values_mean3,\n",
    "    atol=1e-4,\n",
    "    err_msg=(\n",
    "        \"Scaled values for 'wallet_cohorts_whales_total_volume_mean_10d_period_3' \"\n",
    "        \"do not match expected log-transformed values.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.column_scaling_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_cohorts_whales_total_volume_mean_10d_period_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "def scaling_1_metrics_config():\n",
    "    \"\"\"\n",
    "    Fixture providing a sample metrics configuration dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'time_series': {\n",
    "            'market_data': {\n",
    "                'price': {\n",
    "                    'aggregations': {\n",
    "                        'std': {\n",
    "                            'scaling': 'none'\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'volume': {\n",
    "                    'aggregations': {\n",
    "                        'sum': {\n",
    "                            'scaling': 'standard'\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'market_cap': {\n",
    "                    'aggregations': {\n",
    "                        'last': {\n",
    "                            'scaling': 'log'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "scaling_1_metrics_config=scaling_1_metrics_config()\n",
    "def scaling_1_dummy_dataframe():\n",
    "    \"\"\"\n",
    "    Fixture providing a dummy dataframe with a MultiIndex and sample data.\n",
    "    \"\"\"\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            pd.to_datetime(['2023-01-01', '2023-01-02']),\n",
    "            ['bitcoin', 'ethereum']\n",
    "        ],\n",
    "        names=['time_window', 'coin_id']\n",
    "    )\n",
    "    data = {\n",
    "        'time_series_market_data_price_std': [1.0, 2.0, 3.0, 4.0],\n",
    "        'time_series_market_data_volume_sum': [100, 200, 300, 400],\n",
    "        'time_series_market_data_market_cap_last': [1000, 2000, 3000, 4000]\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=index)\n",
    "    return df\n",
    "scaling_1_dummy_dataframe=scaling_1_dummy_dataframe()\n",
    "# @pytest.mark.unit\n",
    "# def test_scaling_processor(scaling_1_metrics_config, scaling_1_dummy_dataframe):\n",
    "\"\"\"\n",
    "Test the ScalingProcessor class for correct column mapping and scaling application.\n",
    "\"\"\"\n",
    "# Instantiate the ScalingProcessor with the provided metrics_config\n",
    "processor = prp.ScalingProcessor(scaling_1_metrics_config)\n",
    "\n",
    "# Expected column_scaling_map based on the metrics_config\n",
    "expected_column_scaling_map = {\n",
    "    'time_series_market_data_price_std': 'none',\n",
    "    'time_series_market_data_volume_sum': 'standard',\n",
    "    'time_series_market_data_market_cap_last': 'log'\n",
    "}\n",
    "\n",
    "# Assert that the column_scaling_map is as expected\n",
    "assert processor.column_scaling_map == expected_column_scaling_map, (\n",
    "    \"Column scaling map does not match expected mapping.\"\n",
    ")\n",
    "\n",
    "# Apply scaling to the dummy_dataframe (as training data)\n",
    "scaled_df = processor.apply_scaling(scaling_1_dummy_dataframe, is_train=True)\n",
    "\n",
    "# Prepare expected scaled values for each column\n",
    "\n",
    "# For 'time_series_market_data_price_std', scaling is 'none',\n",
    "# so values should remain the same as in the original dataframe.\n",
    "expected_price_std = scaling_1_dummy_dataframe['time_series_market_data_price_std'].values\n",
    "\n",
    "# For 'time_series_market_data_volume_sum', scaling is 'standard'.\n",
    "# This means we need to standardize the values by subtracting the mean and dividing by the std deviation.\n",
    "volume_values = scaling_1_dummy_dataframe['time_series_market_data_volume_sum'].values.reshape(-1, 1)\n",
    "# Calculate mean and standard deviation of the volume values\n",
    "volume_mean = volume_values.mean()\n",
    "volume_std = volume_values.std()\n",
    "# Standardize the volume values\n",
    "expected_volume_sum = (volume_values - volume_mean) / volume_std\n",
    "expected_volume_sum = expected_volume_sum.flatten()\n",
    "\n",
    "# For 'time_series_market_data_market_cap_last', scaling is 'log'.\n",
    "# We apply the natural logarithm to the values (using np.log1p to handle zero values safely).\n",
    "market_cap_values = scaling_1_dummy_dataframe['time_series_market_data_market_cap_last'].values\n",
    "expected_market_cap_last = np.log1p(market_cap_values)\n",
    "\n",
    "# Now, we compare the scaled values in scaled_df to the expected values calculated above.\n",
    "\n",
    "# Compare 'time_series_market_data_price_std' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['time_series_market_data_price_std'].values,\n",
    "    expected_price_std,\n",
    "    atol=1e-4,\n",
    "    err_msg=\"Scaled values for 'price_std' do not match expected values.\"\n",
    ")\n",
    "\n",
    "# Compare 'time_series_market_data_volume_sum' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['time_series_market_data_volume_sum'].values,\n",
    "    expected_volume_sum,\n",
    "    atol=1e-4,\n",
    "    err_msg=\"Scaled values for 'volume_sum' do not match expected standardized values.\"\n",
    ")\n",
    "\n",
    "# Compare 'time_series_market_data_market_cap_last' values\n",
    "np.testing.assert_allclose(\n",
    "    scaled_df['time_series_market_data_market_cap_last'].values,\n",
    "    expected_market_cap_last,\n",
    "    atol=1e-4,\n",
    "    err_msg=\"Scaled values for 'market_cap_last' do not match expected log-transformed values.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.column_scaling_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt.promote_indicators_to_metrics(metrics_config['macro_trends'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_metrics_config():\n",
    "    \"\"\"Complex metrics_config structure with nested aggregations\"\"\"\n",
    "    return {\n",
    "        \"wallet_cohorts\": {\n",
    "            \"whales\": {\n",
    "                \"total_volume\": {\n",
    "                    \"aggregations\": {\n",
    "                        \"last\": {\n",
    "                            \"scaling\": \"log\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"rolling\": {\n",
    "                        \"aggregations\": {\n",
    "                            \"mean\": {\n",
    "                                \"scaling\": \"log\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"window_duration\": 10,\n",
    "                        \"lookback_periods\": 3\n",
    "                    },\n",
    "                    \"indicators\": {\n",
    "                        \"ema\": {\n",
    "                            \"parameters\": {\n",
    "                                \"window\": [7]\n",
    "                            },\n",
    "                            \"aggregations\": {\n",
    "                                \"last\": {\n",
    "                                    \"scaling\": \"none\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"rolling\": {\n",
    "                                \"aggregations\": {\n",
    "                                    \"last\": {\n",
    "                                        \"scaling\": \"standard\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"window_duration\": 7,\n",
    "                                \"lookback_periods\": 3\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "complex_metrics_config=complex_metrics_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "processor = prp.ScalingProcessor(complex_metrics_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_config = complex_metrics_config['wallet_cohorts']['whales']['total_volume']['rolling']\n",
    "rolling_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prefix = 'total_volume'\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "if 'aggregations' in rolling_config:\n",
    "    for agg_type, agg_config in rolling_config['aggregations'].items():\n",
    "        if isinstance(agg_config, dict) and 'scaling' in agg_config:\n",
    "            mapping[f\"{new_prefix}_rolling_{agg_type}\"] = agg_config['scaling']\n",
    "if 'comparisons' in rolling_config:\n",
    "    for comp_type, comp_config in rolling_config['comparisons'].items():\n",
    "        if isinstance(comp_config, dict) and 'scaling' in comp_config:\n",
    "            mapping[f\"{new_prefix}_rolling_{comp_type}\"] = comp_config['scaling']\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[importlib.reload(module) for module in modules]  # Reload all modules\n",
    "config, metrics_config, modeling_config, experiments_config = u.load_all_configs('../config')  # Reload all configs\n",
    "\n",
    "# Create a ScalingProcessor instance with the complex configuration\n",
    "processor = prp.ScalingProcessor(complex_metrics_config)\n",
    "\n",
    "# Define the expected keys based on the provided information\n",
    "expected_keys = [\n",
    "       'whales_total_volume_last',\n",
    "       'whales_total_volume_mean_10d_period_1',\n",
    "       'whales_total_volume_mean_10d_period_2',\n",
    "       'whales_total_volume_mean_10d_period_3',\n",
    "       'whales_total_volume_ema_7_last',\n",
    "       'whales_total_volume_ema_7_last_7d_period_1',\n",
    "       'whales_total_volume_ema_7_last_7d_period_2',\n",
    "       'whales_total_volume_ema_7_last_7d_period_3'\n",
    "]\n",
    "\n",
    "# Verify that the column_scaling_map contains all expected keys\n",
    "for key in expected_keys:\n",
    "       # For each expected key, we check if it exists in the column_scaling_map\n",
    "       # The assertion will fail if any key is missing\n",
    "       assert key in processor.column_scaling_map, f\"Expected key '{key}' not found in column_scaling_map\"\n",
    "\n",
    "# Verify that the number of keys in column_scaling_map matches the expected count\n",
    "# This ensures that there are no unexpected additional keys\n",
    "assert len(processor.column_scaling_map) == len(expected_keys), (\n",
    "       f\"Expected {len(expected_keys)} keys, but found {len(processor.column_scaling_map)} keys \"\n",
    "       f\"in column_scaling_map\"\n",
    ")\n",
    "\n",
    "# Print the actual keys for debugging purposes\n",
    "print(\"Actual keys in column_scaling_map:\", list(processor.column_scaling_map.keys()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = prp.ScalingProcessor(complex_metrics_config)\n",
    "\n",
    "processor.column_scaling_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.column_scaling_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.column_scaling_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df[\"metric1\"]\n",
    "\n",
    "0    -1.41421356237\n",
    "1   -0.707106781187\n",
    "2                 0\n",
    "3    0.707106781187\n",
    "4     1.41421356237\n",
    "Name: metric1, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in config.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['price_bollinger_bands_upper_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'price': [100, 110, 105, 200, 220, 210]\n",
    "})\n",
    "\n",
    "upper_band = ind.calculate_bollinger_bands(sample_data['price'], 'upper', window=2, num_std=2)\n",
    "print(upper_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['price_rsi_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result['price_rsi_2'], expected_rsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(result_df.loc[result_df['coin_id'] == 'coin1', 'price_sma_2'].values) == list(expected_sma_2_coin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(expected_sma_2_coin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_rsi.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreams_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
